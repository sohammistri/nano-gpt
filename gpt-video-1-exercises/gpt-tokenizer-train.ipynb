{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer based LM\n",
    "\n",
    "Train the LM instead of character level, we will now train a token level LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import tiktoken\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/tiny-shakespeare/input.txt\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load gpt-2 tokenizer\n",
    "gpt2_tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "gpt2_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = gpt2_tokenizer.encode(data)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2997381850454848"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1115394 / 338025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270420, 67605)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare train and test data\n",
    "split_idx = int(0.80 * len(tokens))\n",
    "train_tokens = tokens[:split_idx]\n",
    "val_tokens = tokens[split_idx:]\n",
    "len(train_tokens), len(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data == gpt2_tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, emb_dim, block_size, n_heads, head_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # 1st LayerNorm\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # first Linear to get from emb_dim --> 3 * n_heads*head_dim, to get k,q,v, then proj back to emb_dim\n",
    "        self.c_proj = nn.Linear(emb_dim, 3 * n_heads * head_dim, bias=False)\n",
    "        self.proj = nn.Linear(n_heads * head_dim, emb_dim)\n",
    "\n",
    "        # 2nd LayerNorm\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # finally thinking layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # finally register the tril matrix\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the shape\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Layer norm\n",
    "        ln_x = self.ln1(x)\n",
    "\n",
    "        # Project and extract k,q,v\n",
    "        c = self.c_proj(ln_x) # (B,T,C)  --> (B,T,3*nh*H)\n",
    "        c = c.view(B, T, self.n_heads, 3 * self.head_dim) # (B,T,nh,3*H)\n",
    "        k, q, v = torch.split(c, self.head_dim, dim=-1) # each of shape B,T,nh,H\n",
    "        k, q, v = k.transpose(-3, -2), q.transpose(-3, -2), v.transpose(-3, -2) # B, nh, T, H\n",
    "\n",
    "        # Get the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.50) # (B,nh,T,H) @ (B,nh,H,T) -> (B,nh,T,T)\n",
    "        wei = wei.masked_fill(self.mask[:, :, :T, :T] == 0, -float(\"inf\"))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        wei = self.dropout1(wei)\n",
    "\n",
    "        # Apply to v\n",
    "        act = wei @ v # (B,nh,T,T) @ (B,nh,T,H) -> (B,nh,T,H)\n",
    "        act = act.transpose(-3, -2) # B,T,nh,H\n",
    "        act = act.contiguous().view(B, T, self.n_heads * self.head_dim)\n",
    "\n",
    "        # Transform to emb_dim and skip connection\n",
    "        act = self.proj(act) # (B, T,C)\n",
    "        act = x + act\n",
    "\n",
    "        # Think and skip connections\n",
    "        ln_act = self.ln2(act)\n",
    "        out = self.ffn(ln_act) # (B,T,C)\n",
    "        out = x + out # x shape (B,T,C)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, emb_dim, n_layers, n_heads, head_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # helper variables\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding lookup table\n",
    "        self.token_embbeding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\n",
    "        # MHA head\n",
    "        self.MHA = nn.Sequential(*[MHA(emb_dim, block_size, n_heads, head_dim, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        # Layernorm\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # final linear layer\n",
    "        self.lm_layer = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(f\"Number of parameters: {sum([p.numel() for p in self.parameters()])}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x shape (B, T)\n",
    "        B, T = x.shape\n",
    "\n",
    "        token_emb = self.token_embbeding_table(x)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(0, T).to(self.device))\n",
    "        emb = token_emb + pos_emb\n",
    "\n",
    "        emb = self.MHA(emb)\n",
    "        emb = self.ln(emb)\n",
    "        logits = self.lm_layer(emb) # (B, T, V)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, max_tokens=1000):\n",
    "        with torch.no_grad():\n",
    "            cur_window, idx_list = torch.LongTensor([[0]]).to(self.device), [0] # (1, 1)\n",
    "\n",
    "            for i in range(max_tokens):\n",
    "                cur_window = cur_window[:, -self.block_size:] # (1, B)\n",
    "                logits, _ = self.forward(cur_window) # (1,B,V)\n",
    "                probs = torch.softmax(logits, dim=-1).squeeze(dim=0) # (B,V)\n",
    "                idx = torch.multinomial(probs, num_samples=1, replacement=True)[-1].item()\n",
    "                cur_window = torch.concat([cur_window, torch.LongTensor([[idx]]).view(1, 1).to(self.device)], dim=-1)\n",
    "                idx_list.append(idx)\n",
    "\n",
    "            generated_text = gpt2_tokenizer.decode(idx_list)\n",
    "\n",
    "            return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tokens, block_size, batch_size):\n",
    "    batch = torch.randint(0, len(tokens)-block_size, (batch_size,)) # B dimension array of random indices\n",
    "    Xb = torch.stack([torch.LongTensor(tokens[i:i+block_size]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    yb = torch.stack([torch.LongTensor(tokens[i+1:i+block_size+1]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(tokens, block_size, batch_size, model, device):\n",
    "    loss_values = []\n",
    "    for _ in range(100):\n",
    "        Xb, yb = get_batch(tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        _, loss = model(Xb, yb)\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    mean_loss = torch.FloatTensor(loss_values).mean().item()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_tokens, val_tokens, model, optimizer, device, block_size, batch_size, n_iters, eval_interval):\n",
    "    train_lossi, val_lossi = [], []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        model.train()\n",
    "        Xb, yb = get_batch(train_tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        # forward\n",
    "        _, loss = model(Xb, yb)\n",
    "\n",
    "        # set grads to zero\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # do backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i % eval_interval == 0) or (i == n_iters - 1):\n",
    "            model.eval()\n",
    "            train_loss = compute_loss(train_tokens, block_size, batch_size, model, device)\n",
    "            val_loss = compute_loss(val_tokens, block_size, batch_size, model, device)\n",
    "\n",
    "            train_lossi.append(train_loss)\n",
    "            val_lossi.append(val_loss)\n",
    "\n",
    "            print(f\"Step {i}/{n_iters} --> Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    return train_lossi, val_lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "n_iters = 5000\n",
    "eval_interval = n_iters//10\n",
    "lr = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "emb_dim = 32\n",
    "n_heads = 4\n",
    "head_dim = emb_dim // n_heads\n",
    "n_layers = 1\n",
    "dropout = 0.2\n",
    "vocab_size = gpt2_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3279633\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPT(emb_dim=emb_dim, vocab_size=vocab_size, block_size=block_size, n_heads=n_heads,\\\n",
    "                 n_layers=n_layers, head_dim=head_dim, device=device, dropout=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000 --> Train: 10.8293 | Val: 10.8278\n",
      "Step 500/5000 --> Train: 6.3177 | Val: 6.5101\n",
      "Step 1000/5000 --> Train: 5.5742 | Val: 5.9234\n",
      "Step 1500/5000 --> Train: 5.1121 | Val: 5.5991\n",
      "Step 2000/5000 --> Train: 4.7973 | Val: 5.4119\n",
      "Step 2500/5000 --> Train: 4.5589 | Val: 5.3498\n",
      "Step 3000/5000 --> Train: 4.3892 | Val: 5.2783\n",
      "Step 3500/5000 --> Train: 4.3030 | Val: 5.2720\n",
      "Step 4000/5000 --> Train: 4.2083 | Val: 5.2964\n",
      "Step 4500/5000 --> Train: 4.1260 | Val: 5.2672\n",
      "Step 4999/5000 --> Train: 4.0519 | Val: 5.2912\n"
     ]
    }
   ],
   "source": [
    "train_lossi, val_lossi = train(train_tokens=train_tokens, val_tokens=val_tokens, model=model, optimizer=optimizer,\\\n",
    "      device=device, block_size=block_size, batch_size=batch_size, n_iters=n_iters, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.82490511970208"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected first loss:\n",
    "import math\n",
    "math.log(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "! O, give, so excuse with some silk, by thy leader;\n",
       "For God! be wine,\n",
       "Or so ill.\n",
       "Of the fisher 'pulled from that the fortune'OPSA:\n",
       "Yet prismuath.\n",
       "\n",
       "First Gentleman:\n",
       "Your brother's likely thou keep but smooth'd, consider'd: you, by my feel their means,\n",
       "\n",
       "Tope their found the crown with yonder grief,\n",
       "That hate it scroll!'\n",
       "I rail her, he manners home to appeurt Laura to sleep as;\n",
       "And all the people cannot be was four'd for him:\n",
       "Pray my lord.\n",
       "Therefore revenge a neighbour right.\n",
       "\n",
       "BUCKINGHAM:\n",
       "Unless you might wear the babe, has not off.\n",
       "\n",
       "First T clamew violent.\n",
       "\n",
       "This day is good from Rome was?\n",
       "I voice the first die; and as part of you,\n",
       "Meant decay. Youued the counsel\n",
       "When my shame honour shall do twenty thousand-t,\n",
       " push to hear them gave thee.\n",
       "\n",
       "WARWICK:\n",
       "A soldier, and soon a king's frown,\n",
       "If't had there in the poor sword of many-morrow,\n",
       "Or go of the great law. We will--\n",
       "\n",
       "CORIOLANUS:\n",
       "I neither good to lay it draws pick'd and their Barninail to all both:\n",
       "Ay,\n",
       "And struck best at form; as revenge him seeming were young, let; I I may have prevail'd\n",
       "Whether the king as dull?\n",
       "\n",
       "KING RICHARD: but the eye, repent,\n",
       "Where gracious night, that land that oath could wish me here.\n",
       "After it is for the hand''s side?\n",
       "Toe'st means to have quite all it toid eachest!\n",
       "\n",
       "LEONTES:\n",
       "The poor man is above chks Castle.\n",
       "\n",
       "BUCKINGHAM:\n",
       "And go: go, at my heart naked; he is.\n",
       "\n",
       "KING RICHARDOUCALUS:\n",
       "No unman Leontes speak.\n",
       "\n",
       "CORIOLANUS:\n",
       "But in them AUMNurse:\n",
       "Able upon't\n",
       "And, our well: herenone, then is heavens\n",
       "GE:\n",
       "And Clarence will you coming withouthest night: I brother, by their worship from your guest\n",
       "Provost:\n",
       "And fright it as nays but to Romeo! who's suit of his weapon\n",
       "Eness them.\n",
       "\n",
       "LADY CAPULET:\n",
       "What, that is I'll none done the fair;\n",
       "Thy flower, do I will hold his life:\n",
       "Thou follow you, I would--\n",
       "\n",
       "ROMEO:\n",
       "Come, to have home: when the happy Worth is submission.\n",
       "\n",
       "KING RICHARD II:\n",
       "He makes lived there: Ie thy flattland is--\n",
       "\n",
       "GLOUCALUS:\n",
       "That lies ch herb of this.\n",
       "\n",
       "Pray a feeling, he was wrong?\n",
       "\n",
       "NIA:\n",
       "But thou, first with their Party, play indeed;\n",
       "But by our horse of roses, on it: to me\n",
       "If it not this fair my fellows:\n",
       "Here hath some ground that be\n",
       "and confretch are in blood\n",
       "s, after Warwick's name:\n",
       "And smell to England was black assured I see\n",
       "ook more more, I'll give you all a pupil every lesser.\n",
       "\n",
       "Second Lord:\n",
       "Ogrowing and the house of\n",
       "Some tumultuous articulate of York. throne:\n",
       "How know, how he is noble as\n",
       "Comm'd the care to Elils.\n",
       "\n",
       "\n",
       "CLIFFORD:\n",
       "ld hide him?\n",
       "\n",
       "GLOUCAMILLO:\n",
       "I therefore, she'reeling on this army spent\n",
       "Fare thee Buckingham.\n",
       "Now, their provostCIUS:\n",
       "And she was; as you that I plant love do quickly please your grace that? prison, rather\n",
       "The moon of every hand of one in the nextbury,\n",
       "That ever general live with us i' the sweet butcher of this great trick\n",
       "And their influences, God-scient: I might would reconcil!\n",
       "I beg in MOWY:\n",
       "Never and look months inize with a world?\n",
       "\n",
       "CATESBY:\n",
       "Parest they would be the target to my lord, and that cried anon to crave England.\n",
       "\n",
       "Yre I may devise to be\n",
       "Thouite Mercutoth we can, and their death?\n",
       "But Juliet, and teach thee the hell,\n",
       "Which would have at't in this one,\n",
       "Thy condition doth seem from thy worthy brother?\n",
       "Well, he\n",
       "Not both:\n",
       "For she was my father; therefore, Warwick?\n",
       "Because he vengeance hath received:ILLman:\n",
       "Will to protest, Suffolk, they that our right's happy fellest"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(model.generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
