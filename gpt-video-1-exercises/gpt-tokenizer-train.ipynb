{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer based LM\n",
    "\n",
    "Train the LM instead of character level, we will now train a token level LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown\n",
    "import tiktoken\n",
    "import wandb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/tiny-shakespeare/input.txt\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load gpt-2 tokenizer\n",
    "gpt2_tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "gpt2_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338025"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = gpt2_tokenizer.encode(data)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2997381850454848"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1115394 / 338025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270420, 67605)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare train and test data\n",
    "split_idx = int(0.80 * len(tokens))\n",
    "train_tokens = tokens[:split_idx]\n",
    "val_tokens = tokens[split_idx:]\n",
    "len(train_tokens), len(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data == gpt2_tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, emb_dim, block_size, n_heads, head_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # 1st LayerNorm\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # first Linear to get from emb_dim --> 3 * n_heads*head_dim, to get k,q,v, then proj back to emb_dim\n",
    "        self.c_proj = nn.Linear(emb_dim, 3 * n_heads * head_dim, bias=False)\n",
    "        self.proj = nn.Linear(n_heads * head_dim, emb_dim)\n",
    "\n",
    "        # 2nd LayerNorm\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # finally thinking layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # finally register the tril matrix\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the shape\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Layer norm\n",
    "        ln_x = self.ln1(x)\n",
    "\n",
    "        # Project and extract k,q,v\n",
    "        c = self.c_proj(ln_x) # (B,T,C)  --> (B,T,3*nh*H)\n",
    "        c = c.view(B, T, self.n_heads, 3 * self.head_dim) # (B,T,nh,3*H)\n",
    "        k, q, v = torch.split(c, self.head_dim, dim=-1) # each of shape B,T,nh,H\n",
    "        k, q, v = k.transpose(-3, -2), q.transpose(-3, -2), v.transpose(-3, -2) # B, nh, T, H\n",
    "\n",
    "        # Get the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.50) # (B,nh,T,H) @ (B,nh,H,T) -> (B,nh,T,T)\n",
    "        wei = wei.masked_fill(self.mask[:, :, :T, :T] == 0, -float(\"inf\"))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        wei = self.dropout1(wei)\n",
    "\n",
    "        # Apply to v\n",
    "        act = wei @ v # (B,nh,T,T) @ (B,nh,T,H) -> (B,nh,T,H)\n",
    "        act = act.transpose(-3, -2) # B,T,nh,H\n",
    "        act = act.contiguous().view(B, T, self.n_heads * self.head_dim)\n",
    "\n",
    "        # Transform to emb_dim and skip connection\n",
    "        act = self.proj(act) # (B, T,C)\n",
    "        act = x + act\n",
    "\n",
    "        # Think and skip connections\n",
    "        ln_act = self.ln2(act)\n",
    "        out = self.ffn(ln_act) # (B,T,C)\n",
    "        out = x + out # x shape (B,T,C)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, emb_dim, n_layers, n_heads, head_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # helper variables\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding lookup table\n",
    "        self.token_embbeding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\n",
    "        # MHA head\n",
    "        self.MHA = nn.Sequential(*[MHA(emb_dim, block_size, n_heads, head_dim, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        # Layernorm\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # final linear layer\n",
    "        self.lm_layer = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(f\"Number of parameters: {sum([p.numel() for p in self.parameters()])}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x shape (B, T)\n",
    "        B, T = x.shape\n",
    "\n",
    "        token_emb = self.token_embbeding_table(x)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(0, T).to(self.device))\n",
    "        emb = token_emb + pos_emb\n",
    "\n",
    "        emb = self.MHA(emb)\n",
    "        emb = self.ln(emb)\n",
    "        logits = self.lm_layer(emb) # (B, T, V)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, max_tokens=1000):\n",
    "        with torch.no_grad():\n",
    "            cur_window, idx_list = torch.LongTensor([[0]]).to(self.device), [0] # (1, 1)\n",
    "\n",
    "            for i in range(max_tokens):\n",
    "                cur_window = cur_window[:, -self.block_size:] # (1, B)\n",
    "                logits, _ = self.forward(cur_window) # (1,B,V)\n",
    "                probs = torch.softmax(logits, dim=-1).squeeze(dim=0) # (B,V)\n",
    "                idx = torch.multinomial(probs, num_samples=1, replacement=True)[-1].item()\n",
    "                cur_window = torch.concat([cur_window, torch.LongTensor([[idx]]).view(1, 1).to(self.device)], dim=-1)\n",
    "                idx_list.append(idx)\n",
    "\n",
    "            generated_text = gpt2_tokenizer.decode(idx_list)\n",
    "\n",
    "            return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tokens, block_size, batch_size):\n",
    "    batch = torch.randint(0, len(tokens)-block_size, (batch_size,)) # B dimension array of random indices\n",
    "    Xb = torch.stack([torch.LongTensor(tokens[i:i+block_size]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    yb = torch.stack([torch.LongTensor(tokens[i+1:i+block_size+1]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(tokens, block_size, batch_size, model, device):\n",
    "    loss_values = []\n",
    "    for _ in range(100):\n",
    "        Xb, yb = get_batch(tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        _, loss = model(Xb, yb)\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    mean_loss = torch.FloatTensor(loss_values).mean().item()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_tokens, val_tokens, model, optimizer, device, block_size, batch_size, n_iters, eval_interval):\n",
    "    train_lossi, val_lossi = [], []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        model.train()\n",
    "        Xb, yb = get_batch(train_tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        # forward\n",
    "        _, loss = model(Xb, yb)\n",
    "\n",
    "        # set grads to zero\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # do backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i % eval_interval == 0) or (i == n_iters - 1):\n",
    "            model.eval()\n",
    "            train_loss = compute_loss(train_tokens, block_size, batch_size, model, device)\n",
    "            val_loss = compute_loss(val_tokens, block_size, batch_size, model, device)\n",
    "\n",
    "            train_lossi.append(train_loss)\n",
    "            val_lossi.append(val_loss)\n",
    "\n",
    "             # log metrics to wandb\n",
    "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "        # break\n",
    "\n",
    "    return train_lossi, val_lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "n_iters = 5000\n",
    "eval_interval = n_iters//10\n",
    "lr = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "emb_dim = 32\n",
    "n_heads = 4\n",
    "head_dim = emb_dim // n_heads\n",
    "n_layers = 1\n",
    "dropout = 0.2\n",
    "vocab_size = gpt2_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoham07-mistri12\u001b[0m (\u001b[33msoham-mistri-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf429da856f44ff9cb1b8ab9303d89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011141871766368341, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sohammistri/Documents/nano-gpt/gpt-video-1-exercises/wandb/run-20250112_114725-h2scmiif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif' target=\"_blank\">earthy-lake-1</a></strong> to <a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small' target=\"_blank\">https://wandb.ai/soham-mistri-personal/nano-gpt-token-small</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif' target=\"_blank\">https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x149a35df0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wandb init\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"nano-gpt-token-small\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"batch_size\": batch_size,\n",
    "    \"block_size\": block_size,\n",
    "    \"n_iters\": n_iters,\n",
    "    \"learning_rate\": lr,    \n",
    "    \"emb_dim\": 32,\n",
    "    \"n_heads\": 4,\n",
    "    \"head_dim\": emb_dim // n_heads,\n",
    "    \"n_layers\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"vocab_size\": gpt2_tokenizer.n_vocab\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3279633\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPT(emb_dim=emb_dim, vocab_size=vocab_size, block_size=block_size, n_heads=n_heads,\\\n",
    "                 n_layers=n_layers, head_dim=head_dim, device=device, dropout=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lossi, val_lossi = train(train_tokens=train_tokens, val_tokens=val_tokens, model=model, optimizer=optimizer,\\\n",
    "      device=device, block_size=block_size, batch_size=batch_size, n_iters=n_iters, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.82490511970208"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected first loss:\n",
    "import math\n",
    "math.log(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "! her hate your eyes task the mother's ghostly,\n",
       "To know, a weather!\n",
       "\n",
       "GLOUCESTER:\n",
       "Then then at a greaterDevl:\n",
       "When it dukaunt news is in the blood in the train:\n",
       "WICK:\n",
       "So now he but ask'd you to his Lord Angelo;\n",
       "The city are amended.\n",
       "\n",
       "GLOUCY:\n",
       "What all the field;man ay, and woo'd him our father\n",
       "Unay order Agman to the same trunk, when I'll make some our unlawful.\n",
       "\n",
       "Yoe'st thou would stale even all;\n",
       "I thought of yet it be thrive:\n",
       "First Watchman:\n",
       "What word I will seek a poor day's blood food,\n",
       "And glanced hunger lingering simply our knowledge: you\n",
       "To be flood.\n",
       "\n",
       "COMINIUS:\n",
       "I shall not have broinating tinstalled wouldp to this envious lock'd.\n",
       "\n",
       "QUEEN is at't not from senators\n",
       "Lord comes the man! Come, I use to laugh, let's ladies\n",
       "MOPSA:\n",
       " slandering to love, I\n",
       "By favours from so Gaunt, Pompey, that he will be climb,\n",
       "And not bless such gracious lie dead!\n",
       "\n",
       "ROMEO:\n",
       "An sovereign, my lord, my sovereign plays your liege, then I suming than wind'd, and, man is my friends:\n",
       "Ay to puts valiant, and at mine neighbours, with heaven speaks\n",
       "I never have very three-day;\n",
       "\n",
       "To know the sun, in the lantern,\n",
       "She had hath done to this:\n",
       "Come, if him will; we could you bears,\n",
       "Of unilies of the most noble.\n",
       "Thou, i' the oath, nor would not the king.\n",
       "\n",
       "ANTIGONUS:\n",
       "That thou art visibleurer is he wuscle, whilst I am indeed to the aines.\n",
       "\n",
       "Dok masend me to use that they ope the oldest and!--\n",
       "When you princes that and desperate one doth your speech: therefore speak.\n",
       "\n",
       "KING RICHARD III:\n",
       "It shall know thy shining much; no swither all, dancesey my noble for going\n",
       "To fly theurden'd and ask,\n",
       "If you do find a thousand sword\n",
       "pacedTRESS OF YORK:\n",
       "Pately equal much husband against her within the pierce fiCIUS:\n",
       "Thy banish'd in the Romans,\n",
       "As ourque to the bloody soul,\n",
       "Were at sorrow. O, that had he is the wind:\n",
       "How so is most Hastings, strike, and the pleasure of a friendly name, and more than it takes of Richard's very limbs: this is a king\n",
       "With nothing admit stampeded, cousin! shortly should\n",
       "We keep me like protest's Rosley.\n",
       "\n",
       "LEONTES:\n",
       "Then I loves the bowues I am stay\n",
       "If been man of your esolsce:\n",
       "Lady:\n",
       "O:\n",
       "Why call it in thy love, no more indeed, sir;\n",
       "What is the impat'd or strike. You,\n",
       "Enough as whose caps on, exceeds the manner.\n",
       "\n",
       "First Senator:\n",
       "Ripp'd, and neglected achieved there\n",
       "But I came:\n",
       "For fame are at now;ick, thou art serves it else,\n",
       " conclude friends and death\n",
       "A danger and your faith, on grief would he hathillo to the w Adrian's crown\n",
       "MAMILLO:\n",
       "Call his witness and murder less p sol Citizen:\n",
       "Therefore your queen to thee, thatily stands,\n",
       "Thechingthankful than you have all too hungry?\n",
       "\n",
       "DUKEYea, you bait import by the weary head.\n",
       "\n",
       "QUEENRY VI:\n",
       "Why, my lords; thou allegiance maystable.\n",
       "\n",
       "B goose, which ne'er hadst revive my tyranny;\n",
       "The friend, what that\n",
       " Tingbroke when\n",
       "Which they are; pleasing expedience?\n",
       "Your monarchPeace is made:\n",
       "Wash and so drew, if my Romeo's woe, somethingicious trowhood,\n",
       "Vereth to justice will lose the king.\n",
       "man! that this is nothing, and serve on a my gracious lord\n",
       "Awade me be vade it will tell him hold me: I cry you now.\n",
       "\n",
       "Clown:\n",
       "going to be reven me on behind;\n",
       "Have blessed authority married action\n",
       "ines in spectators; you haveNurse:\n",
       "That I gasby that lies, my frost and odd gh Forrest, and my lord's love, great Servingmen,\n",
       "To this inevitable DF, if the blessings in thy sword, how told doth there now high's troth\n",
       " banished against the head,\n",
       "We cannot not we must bear ask you,\n",
       "The sweet knee, if the praises fail:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(model.generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>4.05224</td></tr><tr><td>val_loss</td><td>5.28553</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-lake-1</strong> at: <a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif' target=\"_blank\">https://wandb.ai/soham-mistri-personal/nano-gpt-token-small/runs/h2scmiif</a><br> View project at: <a href='https://wandb.ai/soham-mistri-personal/nano-gpt-token-small' target=\"_blank\">https://wandb.ai/soham-mistri-personal/nano-gpt-token-small</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250112_114725-h2scmiif/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
