wandb_project: "nano-gpt-token-tiny-shakespeare-finetune-large"
batch_size: 256
block_size: 256
emb_dim: 384
n_heads: 6
head_dim: 64
n_layers: 6
dropout: 0.2
fixed_lr: false
n_iters: 4000
warmup_iters: 200
lr_decay_iters: 4000
learning_rate: 2.0e-5  # max lr
min_lr: 2.0e-6  # min lr
tokenizer_model: "gpt-2"
split_ratio: 0.8
checkpoint_dir: "./checkpoint-tiny-shakespeare-finetune/"
always_save_checkpoint: false
dataset: "tiny_shakespeare"
train_on_full: false
data_path: "../data/tiny-shakespeare/input.txt"
continue_train: false
finetune: true
finetune_ckpt: "best-1B-pretrain-ckpt-2.pt"

