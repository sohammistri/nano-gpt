{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Head and MultiHead layer in a single block.\n",
    "\n",
    "We should have a single layer. The idea is to use `num_heads` as a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/tiny-shakespeare/input.txt\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "First Citizen:\n",
       "Before we proceed any further, hear me speak.\n",
       "\n",
       "All:\n",
       "Speak, speak.\n",
       "\n",
       "First Citizen:\n",
       "You are all resolved rather to die than to famish?\n",
       "\n",
       "All:\n",
       "Resolved. resolved.\n",
       "\n",
       "First Citizen:\n",
       "First, you know Caius Marcius is chief enemy to the people.\n",
       "\n",
       "All:\n",
       "We know't, we know't.\n",
       "\n",
       "First Citizen:\n",
       "Let us kill him, and we'll have corn at our own price.\n",
       "Is't a verdict?\n",
       "\n",
       "All:\n",
       "No more talking on't; let it be done: away, away!\n",
       "\n",
       "Second Citizen:\n",
       "One word, good citizens.\n",
       "\n",
       "First Citizen:\n",
       "We are accounted poor citizens, the patricians good.\n",
       "What authority surfeits on would relieve us: if they\n",
       "would yield us but the superfluity, while it were\n",
       "wholesome, we might guess they relieved us humanely;\n",
       "but they think we are too dear: the leanness that\n",
       "afflicts us, the object of our misery, is as an\n",
       "inventory to particularise their abundance; our\n",
       "sufferance is a gain to them Let us revenge this with\n",
       "our pikes, ere we become rakes: for the gods know I\n",
       "speak this in hunger for bread, not in thirst for revenge.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(data[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "\"\".join(char for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi, itos = {}, {}\n",
    "\n",
    "for i, char in enumerate(chars):\n",
    "    stoi[char] = i\n",
    "    itos[i] = char\n",
    "\n",
    "encode = lambda text: [stoi[c] for c in text]\n",
    "decode = lambda idx_list: \"\".join(itos[i] for i in idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892315, 223079)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare train and test data\n",
    "split_idx = int(0.80 * len(data))\n",
    "tokens = encode(data)\n",
    "train_tokens = tokens[:split_idx]\n",
    "val_tokens = tokens[split_idx:]\n",
    "len(train_tokens), len(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, emb_dim, block_size, n_heads, head_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # 1st LayerNorm\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # first Linear to get from emb_dim --> 3 * n_heads*head_dim, to get k,q,v, then proj back to emb_dim\n",
    "        self.c_proj = nn.Linear(emb_dim, 3 * n_heads * head_dim, bias=False)\n",
    "        self.proj = nn.Linear(n_heads * head_dim, emb_dim)\n",
    "\n",
    "        # 2nd LayerNorm\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # finally thinking layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # finally register the tril matrix\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the shape\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Layer norm\n",
    "        ln_x = self.ln1(x)\n",
    "\n",
    "        # Project and extract k,q,v\n",
    "        c = self.c_proj(ln_x) # (B,T,C)  --> (B,T,3*nh*H)\n",
    "        c = c.view(B, T, self.n_heads, 3 * self.head_dim) # (B,T,nh,3*H)\n",
    "        k, q, v = torch.split(c, self.head_dim, dim=-1) # each of shape B,T,nh,H\n",
    "        k, q, v = k.transpose(-3, -2), q.transpose(-3, -2), v.transpose(-3, -2) # B, nh, T, H\n",
    "\n",
    "        # Get the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.50) # (B,nh,T,H) @ (B,nh,H,T) -> (B,nh,T,T)\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, -float(\"inf\"))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        wei = self.dropout1(wei)\n",
    "\n",
    "        # Apply to v\n",
    "        act = wei @ v # (B,nh,T,T) @ (B,nh,T,H) -> (B,nh,T,H)\n",
    "        act = act.transpose(-3, -2) # B,T,nh,H\n",
    "        act = act.reshape(B, T, self.n_heads * self.head_dim)\n",
    "\n",
    "        # Transform to emb_dim and skip connection\n",
    "        act = self.proj(act) # (B, T,C)\n",
    "        act = x + act\n",
    "\n",
    "        # Think and skip connections\n",
    "        ln_act = self.ln2(act)\n",
    "        out = self.ffn(ln_act) # (B,T,C)\n",
    "        out = x + out # x shape (B,T,C)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, emb_dim, n_layers, n_heads, head_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # helper variables\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding lookup table\n",
    "        self.token_embbeding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\n",
    "        # MHA head\n",
    "        self.MHA = nn.Sequential(*[MHA(emb_dim, block_size, n_heads, head_dim, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        # Layernorm\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # final linear layer\n",
    "        self.lm_layer = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(f\"Number of parameters: {sum([p.numel() for p in self.parameters()])}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x shape (B, T)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        token_emb = self.token_embbeding_table(x)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(0, T).to(self.device))\n",
    "        emb = token_emb + pos_emb\n",
    "\n",
    "        emb = self.MHA(emb)\n",
    "        emb = self.ln(emb)\n",
    "        logits = self.lm_layer(emb) # (B, T, V)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, max_tokens=1000):\n",
    "        with torch.no_grad():\n",
    "            cur_window, idx_list = torch.LongTensor([[0]]).to(self.device), [0] # (1, 1)\n",
    "\n",
    "            for i in range(max_tokens):\n",
    "                cur_window = cur_window[:, -self.block_size:] # (1, B)\n",
    "                logits, _ = self.forward(cur_window) # (1,B,V)\n",
    "                probs = torch.softmax(logits, dim=-1).squeeze(dim=0) # (B,V)\n",
    "                idx = torch.multinomial(probs, num_samples=1, replacement=True)[-1].item()\n",
    "                cur_window = torch.concat([cur_window, torch.LongTensor([[idx]]).view(1, 1).to(self.device)], dim=-1)\n",
    "                idx_list.append(idx)\n",
    "\n",
    "            generated_text = decode(idx_list)\n",
    "\n",
    "            return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tokens, block_size, batch_size):\n",
    "    batch = torch.randint(0, len(tokens)-block_size, (batch_size,)) # B dimension array of random indices\n",
    "    Xb = torch.stack([torch.LongTensor(tokens[i:i+block_size]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    yb = torch.stack([torch.LongTensor(tokens[i+1:i+block_size+1]) for i in batch], dim=0) # Create (B, T) dimension array\n",
    "    return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(tokens, block_size, batch_size, model, device):\n",
    "    loss_values = []\n",
    "    for _ in range(100):\n",
    "        Xb, yb = get_batch(tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        _, loss = model(Xb, yb)\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    mean_loss = torch.FloatTensor(loss_values).mean().item()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_tokens, val_tokens, model, optimizer, device, block_size, batch_size, n_iters, eval_interval):\n",
    "    train_lossi, val_lossi = [], []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        model.train()\n",
    "        Xb, yb = get_batch(train_tokens, block_size, batch_size)\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        # forward\n",
    "        _, loss = model(Xb, yb)\n",
    "\n",
    "        # set grads to zero\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # do backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i % eval_interval == 0) or (i == n_iters - 1):\n",
    "            model.eval()\n",
    "            train_loss = compute_loss(train_tokens, block_size, batch_size, model, device)\n",
    "            val_loss = compute_loss(val_tokens, block_size, batch_size, model, device)\n",
    "\n",
    "            train_lossi.append(train_loss)\n",
    "            val_lossi.append(val_loss)\n",
    "\n",
    "            print(f\"Step {i}/{n_iters} --> Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    return train_lossi, val_lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "n_iters = 5000\n",
    "eval_interval = n_iters//10\n",
    "lr = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "emb_dim = 32\n",
    "n_heads = 4\n",
    "head_dim = emb_dim // n_heads\n",
    "n_layers = 1\n",
    "dropout = 0.2\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 17153\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPT(emb_dim=emb_dim, vocab_size=vocab_size, block_size=block_size, n_heads=n_heads,\\\n",
    "                 n_layers=n_layers, head_dim=head_dim, device=device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000 --> Train: 4.1585 | Val: 4.1585\n",
      "Step 500/5000 --> Train: 2.5426 | Val: 2.5566\n",
      "Step 1000/5000 --> Train: 2.3062 | Val: 2.3372\n",
      "Step 1500/5000 --> Train: 2.2060 | Val: 2.2385\n",
      "Step 2000/5000 --> Train: 2.1434 | Val: 2.2008\n",
      "Step 2500/5000 --> Train: 2.0995 | Val: 2.1563\n",
      "Step 3000/5000 --> Train: 2.0762 | Val: 2.1338\n",
      "Step 3500/5000 --> Train: 2.0497 | Val: 2.1164\n",
      "Step 4000/5000 --> Train: 2.0393 | Val: 2.1098\n",
      "Step 4500/5000 --> Train: 2.0223 | Val: 2.0954\n",
      "Step 4999/5000 --> Train: 1.9987 | Val: 2.0755\n"
     ]
    }
   ],
   "source": [
    "train_lossi, val_lossi = train(train_tokens=train_tokens, val_tokens=val_tokens, model=model, optimizer=optimizer,\\\n",
    "      device=device, block_size=block_size, batch_size=batch_size, n_iters=n_iters, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NanoGPT(\n",
       "  (token_embbeding_table): Embedding(65, 32)\n",
       "  (position_embedding_table): Embedding(8, 32)\n",
       "  (MHA): Sequential(\n",
       "    (0): MHA(\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (c_proj): Linear(in_features=32, out_features=96, bias=False)\n",
       "      (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_layer): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "MEO:\n",
       "Theer and\n",
       "Thichfere that the one modd on,\n",
       "But rear twill sity of roman have thampter, hild\n",
       "Men of bureds to notine,\n",
       "And wouldeordingsir my rel p, comuseds of'er shalion have.\n",
       "\n",
       "ICK:\n",
       "Nobloy lar me sonce,\n",
       "And loblover my lod it a well sty shie it shad knis to\n",
       "Your , thins, of or seent: shink, wherech over\n",
       "Frow may.\n",
       "\n",
       "LETERWIOP:\n",
       "He our\n",
       "banknom a to the marie!\n",
       "On stall oldy hosed thy to nower:\n",
       "To the dow! in in cespeeath fire it fady affounple thaslo shourder'd blodds, wentle's alle your there?\n",
       "\n",
       "CIUSSARn ferven his Qso loves humplef it trove gaarde Whater dony.\n",
       "Rame't,\n",
       "Tubrod, this with a will'd stram andcom: what Murght me nant the a do thorse brothou to mart\n",
       "Wath and hathnire vere the; do by sigued to thoum oun of'end.\n",
       "\n",
       "When lagiveVer beove vaw\n",
       "Yenter\n",
       "Shat awam sirtime scrows\n",
       "Sirth\n",
       "Bitspliding this hold nided.\n",
       "Achorloofer one\n",
       "Thing thore hath dey\n",
       "But with sone in thatrung be a rean's sharignasule refortoods, and flatip inst, junding hereatined whould me cond, ight be.\n",
       "But my you vour "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(model.generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
