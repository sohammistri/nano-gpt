{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUEzC2wrB1B"
      },
      "source": [
        "# MinBPE exercise\n",
        "\n",
        "Implement the tokenizer we describe in the video and exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1bVmC1KZrB1D"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "import regex as re\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "klw3ZEsk0gYr"
      },
      "outputs": [],
      "source": [
        "# some basic tests\n",
        "test_strings = [\n",
        "    \"\", # empty string\n",
        "    \"?\", # single character\n",
        "    \"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\", # fun small string\n",
        "    \"\"\"The llama (/Ààl…ëÀêm…ô/; Spanish pronunciation: [Àà éama] or [Àà ùama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\n",
        "\n",
        "Llamas are social animals and live with others as a herd. Their wool is soft and contains only a small amount of lanolin.[2] Llamas can learn simple tasks after a few repetitions. When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5‚Äì8 miles).[3] The name llama (also historically spelled \"lama\" or \"glama\") was adopted by European settlers from native Peruvians.[4]\"\"\",\n",
        "\"FILE:test/taylorswift.txt\"\n",
        "]\n",
        "\n",
        "def unpack(text):\n",
        "    # we do this because `pytest -v .` prints the arguments to console, and we don't\n",
        "    # want to print the entire contents of the file, it creates a mess. So here we go.\n",
        "    if text.startswith(\"FILE:\"):\n",
        "        taylorswift_file = text[5:]\n",
        "        contents = open(taylorswift_file, \"r\", encoding=\"utf-8\").read()\n",
        "        return contents\n",
        "    else:\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZYbpolSrB1F",
        "outputId": "1344961b-5de4-4ba9-91ac-00bcb3ae9799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read content\n",
        "# source: https://www.kaggle.com/datasets/ukveteran/big-text\n",
        "with open(\"train/big.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# lets train on 1st million words in this\n",
        "text = text[:1000000]\n",
        "\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "7qMVa3wcrB1G",
        "outputId": "cd2e7536-f248-48b2-f90f-8b75c3fc6ac9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
              "by Sir Arthur Conan Doyle\n",
              "(#15 in our series by Sir Arthur Conan Doyle)\n",
              "\n",
              "Copyright laws are changing all over the world. Be sure to check the\n",
              "copyright laws for your country before downloading or redistributing\n",
              "this or any other Project Gutenberg eBook.\n",
              "\n",
              "This header should be the first thing seen when viewing this Project\n",
              "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
              "header without written permission.\n",
              "\n",
              "Please read the \"legal small print,\" and other information about the\n",
              "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
              "important information about your specific rights and restrictions in\n",
              "how the file may be used.  You can also find out about how to make a\n",
              "donation to Project Gutenberg, and how to get involved.\n",
              "\n",
              "\n",
              "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
              "\n",
              "**eBooks Readable By Both Humans and By Computers, Since 1971**\n",
              "\n",
              "*****These eBooks Were Prepared By Thousan"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(text[:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "RdY91f-h4ZK0"
      },
      "outputs": [],
      "source": [
        "def get_stats(tokens):\n",
        "    pair_freq_stats = {}\n",
        "    for c1, c2 in zip(tokens, tokens[1:]):\n",
        "        pair_freq_stats[(c1, c2)] = pair_freq_stats.get((c1, c2), 0) + 1\n",
        "    return pair_freq_stats\n",
        "\n",
        "def replace_pair(tokens, pair, pair_idx):\n",
        "    new_tokens, i = [], 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        if (i < len(tokens) - 1) and ((tokens[i], tokens[i + 1]) == pair):\n",
        "            new_tokens.append(pair_idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "\n",
        "    return new_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqsCpVl2rB1E"
      },
      "source": [
        "## BasicTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "1TRSAyAzrB1E"
      },
      "outputs": [],
      "source": [
        "class BasicTokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.vocab_size = 256\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(self.vocab_size)}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        ## Train the BPE tokenizer.\n",
        "        assert vocab_size > self.vocab_size # to ensure we have a larger vocab size\n",
        "        num_merges = vocab_size - self.vocab_size\n",
        "\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        old_token_length = len(tokens)\n",
        "\n",
        "        if verbose:\n",
        "            start = \"Start\"\n",
        "            print(f\"{start:.20s} | No. of tokens: {len(tokens):6d} | Vocab size: {self.vocab_size:5d}\")\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            # first get the stats of the bigrams\n",
        "            pair_stats = get_stats(tokens)\n",
        "            # get the max freq bigram\n",
        "            max_freq_pair = max(pair_stats, key=lambda k: pair_stats.get(k, -float(\"inf\")))\n",
        "            # now lets create new tokens by replacing this pair with this\n",
        "            tokens = replace_pair(tokens, max_freq_pair, self.vocab_size)\n",
        "            # update the running variables\n",
        "            self.merges[max_freq_pair] = self.vocab_size\n",
        "            self.vocab[self.vocab_size] = self.vocab[max_freq_pair[0]] + self.vocab[max_freq_pair[1]]\n",
        "            self.vocab_size += 1\n",
        "\n",
        "            if verbose:\n",
        "                if ((i + 1) % verbose_iters == 0) or (i == num_merges - 1):\n",
        "                    print((f\"Iteration {(i + 1):4d} | No. of tokens: {len(tokens):6d} | Merged pair: {str(max_freq_pair):10s} --> {self.merges[max_freq_pair]:5d}\"))\n",
        "\n",
        "        if verbose:\n",
        "            compression = old_token_length / len(tokens)\n",
        "            print(f\"Compression : {compression:.2f} X\")\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "        while len(tokens)>=2:\n",
        "            # first get the stats of the bigrams\n",
        "            pair_stats = get_stats(tokens)\n",
        "            # now check if there is a pair which is merged as per our tokenizer\n",
        "            merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "            # check if there is actually a match\n",
        "            if self.merges.get(merge_pair) is None:\n",
        "                break\n",
        "\n",
        "            # now replace with the merges token\n",
        "            tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text = b\"\".join(self.vocab[id] for id in ids)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6_wk9UBrB1G",
        "outputId": "b7b5e24a-fd11-4af3-b29b-3119d7476346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check encoded str length\n",
        "len(list(text.encode(\"utf-8\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UgbfAa-rB1G",
        "outputId": "2aa0c6b4-6cc4-4c8b-8ee0-399966edc8bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration    1 | No. of tokens: 970416 | Merged pair: (101, 32)  -->   256\n",
            "Iteration    2 | No. of tokens: 948950 | Merged pair: (116, 104) -->   257\n",
            "Iteration    3 | No. of tokens: 932251 | Merged pair: (100, 32)  -->   258\n",
            "Iteration    4 | No. of tokens: 916862 | Merged pair: (115, 32)  -->   259\n",
            "Iteration    5 | No. of tokens: 902588 | Merged pair: (116, 32)  -->   260\n",
            "Iteration    6 | No. of tokens: 888972 | Merged pair: (105, 110) -->   261\n",
            "Iteration    7 | No. of tokens: 875719 | Merged pair: (101, 114) -->   262\n",
            "Iteration    8 | No. of tokens: 863648 | Merged pair: (97, 110)  -->   263\n",
            "Iteration    9 | No. of tokens: 852577 | Merged pair: (44, 32)   -->   264\n",
            "Iteration   10 | No. of tokens: 842084 | Merged pair: (257, 256) -->   265\n",
            "Iteration   11 | No. of tokens: 832073 | Merged pair: (111, 110) -->   266\n",
            "Iteration   12 | No. of tokens: 823657 | Merged pair: (121, 32)  -->   267\n",
            "Iteration   13 | No. of tokens: 815311 | Merged pair: (101, 110) -->   268\n",
            "Iteration   14 | No. of tokens: 807594 | Merged pair: (111, 117) -->   269\n",
            "Iteration   15 | No. of tokens: 800768 | Merged pair: (111, 32)  -->   270\n",
            "Iteration   16 | No. of tokens: 794055 | Merged pair: (102, 32)  -->   271\n",
            "Iteration   17 | No. of tokens: 787461 | Merged pair: (111, 114) -->   272\n",
            "Iteration   18 | No. of tokens: 781025 | Merged pair: (46, 32)   -->   273\n",
            "Iteration   19 | No. of tokens: 774995 | Merged pair: (101, 258) -->   274\n",
            "Iteration   20 | No. of tokens: 769057 | Merged pair: (111, 271) -->   275\n",
            "Iteration   21 | No. of tokens: 763233 | Merged pair: (97, 114)  -->   276\n",
            "Iteration   22 | No. of tokens: 760437 | Merged pair: (32, 32)   -->   277\n",
            "Iteration   23 | No. of tokens: 755223 | Merged pair: (114, 101) -->   278\n",
            "Iteration   24 | No. of tokens: 750106 | Merged pair: (263, 258) -->   279\n",
            "Iteration   25 | No. of tokens: 745244 | Merged pair: (116, 105) -->   280\n",
            "Iteration   26 | No. of tokens: 740522 | Merged pair: (116, 270) -->   281\n",
            "Iteration   27 | No. of tokens: 735960 | Merged pair: (261, 103) -->   282\n",
            "Iteration   28 | No. of tokens: 731486 | Merged pair: (97, 108)  -->   283\n",
            "Iteration   29 | No. of tokens: 727438 | Merged pair: (104, 105) -->   284\n",
            "Iteration   30 | No. of tokens: 723473 | Merged pair: (115, 116) -->   285\n",
            "Iteration   31 | No. of tokens: 719545 | Merged pair: (97, 32)   -->   286\n",
            "Iteration   32 | No. of tokens: 715630 | Merged pair: (104, 97)  -->   287\n",
            "Iteration   33 | No. of tokens: 712077 | Merged pair: (10, 10)   -->   288\n",
            "Iteration   34 | No. of tokens: 708424 | Merged pair: (32, 265)  -->   289\n",
            "Iteration   35 | No. of tokens: 705024 | Merged pair: (97, 259)  -->   290\n",
            "Iteration   36 | No. of tokens: 701696 | Merged pair: (97, 260)  -->   291\n",
            "Iteration   37 | No. of tokens: 698458 | Merged pair: (262, 32)  -->   292\n",
            "Iteration   38 | No. of tokens: 695244 | Merged pair: (101, 115) -->   293\n",
            "Iteration   39 | No. of tokens: 692037 | Merged pair: (111, 109) -->   294\n",
            "Iteration   40 | No. of tokens: 688967 | Merged pair: (282, 32)  -->   295\n",
            "Iteration   41 | No. of tokens: 685988 | Merged pair: (73, 32)   -->   296\n",
            "Iteration   42 | No. of tokens: 683140 | Merged pair: (99, 104)  -->   297\n",
            "Iteration   43 | No. of tokens: 680393 | Merged pair: (111, 108) -->   298\n",
            "Iteration   44 | No. of tokens: 677664 | Merged pair: (261, 32)  -->   299\n",
            "Compression : 1.48 X\n"
          ]
        }
      ],
      "source": [
        "# now lets train for some small iterations\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "basic_tokenizer.train(text=text, vocab_size=300, verbose=True, verbose_iters=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i0zHjwhrB1H",
        "outputId": "d63220c9-3fd5-4576-e127-58aa885a06ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (116, 104): 257,\n",
              " (100, 32): 258,\n",
              " (115, 32): 259,\n",
              " (116, 32): 260,\n",
              " (105, 110): 261,\n",
              " (101, 114): 262,\n",
              " (97, 110): 263,\n",
              " (44, 32): 264,\n",
              " (257, 256): 265,\n",
              " (111, 110): 266,\n",
              " (121, 32): 267,\n",
              " (101, 110): 268,\n",
              " (111, 117): 269,\n",
              " (111, 32): 270,\n",
              " (102, 32): 271,\n",
              " (111, 114): 272,\n",
              " (46, 32): 273,\n",
              " (101, 258): 274,\n",
              " (111, 271): 275,\n",
              " (97, 114): 276,\n",
              " (32, 32): 277,\n",
              " (114, 101): 278,\n",
              " (263, 258): 279,\n",
              " (116, 105): 280,\n",
              " (116, 270): 281,\n",
              " (261, 103): 282,\n",
              " (97, 108): 283,\n",
              " (104, 105): 284,\n",
              " (115, 116): 285,\n",
              " (97, 32): 286,\n",
              " (104, 97): 287,\n",
              " (10, 10): 288,\n",
              " (32, 265): 289,\n",
              " (97, 259): 290,\n",
              " (97, 260): 291,\n",
              " (262, 32): 292,\n",
              " (101, 115): 293,\n",
              " (111, 109): 294,\n",
              " (282, 32): 295,\n",
              " (73, 32): 296,\n",
              " (99, 104): 297,\n",
              " (111, 108): 298,\n",
              " (261, 32): 299}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_tokenizer.merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5qYvf8rB1H",
        "outputId": "a2e64132-8b94-48a6-d73d-608a49401da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration  100 | No. of tokens: 576783 | Merged pair: (117, 112) -->   355\n",
            "Iteration  200 | No. of tokens: 495579 | Merged pair: (266, 103) -->   455\n",
            "Iteration  300 | No. of tokens: 452955 | Merged pair: (109, 105) -->   555\n",
            "Iteration  400 | No. of tokens: 424971 | Merged pair: (119, 423) -->   655\n",
            "Iteration  500 | No. of tokens: 403227 | Merged pair: (99, 326)  -->   755\n",
            "Iteration  600 | No. of tokens: 386082 | Merged pair: (282, 264) -->   855\n",
            "Iteration  700 | No. of tokens: 372312 | Merged pair: (668, 405) -->   955\n",
            "Iteration  800 | No. of tokens: 360795 | Merged pair: (573, 265) -->  1055\n",
            "Iteration  900 | No. of tokens: 350883 | Merged pair: (84, 73)   -->  1155\n",
            "Iteration 1000 | No. of tokens: 342119 | Merged pair: (103, 346) -->  1255\n",
            "Compression : 2.92 X\n"
          ]
        }
      ],
      "source": [
        "# now lets get 1k compressions\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "basic_tokenizer.train(text=text, vocab_size=1256, verbose=True, verbose_iters=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "IJT5JwwKrB1H"
      },
      "outputs": [],
      "source": [
        "# lets visualise the merges\n",
        "with open(\"basic_merges.txt\", \"w\") as file:\n",
        "    sorted_merges = sorted(list(basic_tokenizer.merges.items()), key=lambda x: x[1])\n",
        "    for k, v in sorted_merges:\n",
        "        file.write(f\"[{(basic_tokenizer.vocab[k[0]]).decode('utf-8', errors='replace')}][{(basic_tokenizer.vocab[k[1]]).decode('utf-8', errors='replace')}]  ---->   {v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "UvX-VrHtrB1H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test string: 0 Passed! :)\n",
            "Test string: 1 Passed! :)\n",
            "Test string: 2 Passed! :)\n",
            "Test string: 3 Passed! :)\n",
            "Test string: 4 Passed! :)\n"
          ]
        }
      ],
      "source": [
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert test_text == basic_tokenizer.decode(basic_tokenizer.encode(test_text))\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")\n",
        "    # assert test_text == basic_tokenizer.decode(basic_tokenizer.encode(test_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "    num_merges = 0\n",
        "    while True:\n",
        "        # first get the stats of the bigrams\n",
        "        pair_stats = get_stats(tokens)\n",
        "        # now check if there is a pair which is merged as per our tokenizer\n",
        "        merge_pair = min(pair_stats, key=lambda k: basic_tokenizer.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "        # check if there is actually a match\n",
        "        if basic_tokenizer.merges.get(merge_pair) is None:\n",
        "            break\n",
        "        num_merges += 1\n",
        "        # now replace with the merges token\n",
        "        tokens = replace_pair(tokens, merge_pair, basic_tokenizer.merges[merge_pair])\n",
        "\n",
        "    print(num_merges)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "883\n"
          ]
        }
      ],
      "source": [
        "encode(unpack(test_strings[4]));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hngDL7xN3YjD"
      },
      "source": [
        "## RegexTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "yuLWEPZfrB1I"
      },
      "outputs": [],
      "source": [
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_PATTERN_REGEX = re.compile(GPT4_SPLIT_PATTERN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "dxvJB8Df3WFU"
      },
      "outputs": [],
      "source": [
        "class RegexTokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.vocab_size = 256\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(self.vocab_size)}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        ## Train the BPE tokenizer.\n",
        "        assert vocab_size > self.vocab_size # to ensure we have a larger vocab size\n",
        "        num_merges = vocab_size - self.vocab_size\n",
        "\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "        old_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "\n",
        "        if verbose:\n",
        "            start = \"Start\"\n",
        "            print(f\"{start:.20s} | No. of tokens: {old_token_length:6d} | Vocab size: {self.vocab_size:5d}\")\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            # itrate over each token list\n",
        "            pair_stats_full = {}\n",
        "            for tokens in tokens_list:\n",
        "                # first get the stats of the bigrams\n",
        "                pair_stats = get_stats(tokens)\n",
        "                # combine\n",
        "                for k, v in pair_stats.items():\n",
        "                    pair_stats_full[k] = pair_stats_full.get(k, 0) + v\n",
        "\n",
        "            # get the max freq bigram\n",
        "            max_freq_pair = max(pair_stats_full, key=lambda k: pair_stats_full.get(k, -float(\"inf\")))\n",
        "            # now lets create new tokens by replacing this pair with this\n",
        "            new_tokens_list = []\n",
        "            for tokens in tokens_list:\n",
        "                new_tokens_list.append(replace_pair(tokens, max_freq_pair, self.vocab_size))\n",
        "            # update the running variables\n",
        "            self.merges[max_freq_pair] = self.vocab_size\n",
        "            self.vocab[self.vocab_size] = self.vocab[max_freq_pair[0]] + self.vocab[max_freq_pair[1]]\n",
        "            self.vocab_size += 1\n",
        "            tokens_list = list(new_tokens_list)\n",
        "\n",
        "            if verbose:\n",
        "                if ((i + 1) % verbose_iters == 0) or (i == num_merges - 1):\n",
        "                    new_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "                    print((f\"Iteration {(i + 1):4d} | No. of tokens: {new_token_length:6d} | Merged pair: {str(max_freq_pair):10s} --> {self.merges[max_freq_pair]:5d}\"))\n",
        "\n",
        "        if verbose:\n",
        "            new_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "            compression = old_token_length / new_token_length\n",
        "            print(f\"Compression : {compression:.2f} X\")\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "\n",
        "        encoded_tokens_list = []\n",
        "\n",
        "        num_merges = 0\n",
        "\n",
        "        for tokens in tqdm(tokens_list):\n",
        "            while len(tokens) >= 2:\n",
        "                # first get the stats of the bigrams\n",
        "                pair_stats = get_stats(tokens)\n",
        "                # now check if there is a pair which is merged as per our tokenizer\n",
        "                merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "                # check if there is actually a match\n",
        "                if self.merges.get(merge_pair) is None:\n",
        "                    break\n",
        "                \n",
        "                num_merges += 1\n",
        "                # now replace with the merges token\n",
        "                tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "\n",
        "            encoded_tokens_list.append(tokens)\n",
        "\n",
        "        final_tokens = [item for sublist in encoded_tokens_list for item in sublist]\n",
        "        print(num_merges)\n",
        "        return final_tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text = b\"\".join(self.vocab[id] for id in ids)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OsC10xS3WC2",
        "outputId": "09fb08b0-6cfa-4da2-9554-eeefe70c6064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration   10 | No. of tokens: 861074 | Merged pair: (101, 114) -->   265\n",
            "Iteration   20 | No. of tokens: 790967 | Merged pair: (32, 99)   -->   275\n",
            "Iteration   30 | No. of tokens: 740730 | Merged pair: (259, 103) -->   285\n",
            "Iteration   40 | No. of tokens: 700386 | Merged pair: (105, 99)  -->   295\n",
            "Iteration   44 | No. of tokens: 687143 | Merged pair: (32, 104)  -->   299\n",
            "Compression : 1.46 X\n"
          ]
        }
      ],
      "source": [
        "# now lets train for some small iterations\n",
        "regex_tokenizer = RegexTokenizer()\n",
        "regex_tokenizer.train(text=text, vocab_size=300, verbose=True, verbose_iters=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Yc6RdLDawF",
        "outputId": "6c09a70b-f7b7-4d22-857b-740bebc8c93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration  100 | No. of tokens: 580314 | Merged pair: (97, 109)  -->   355\n",
            "Iteration  200 | No. of tokens: 501016 | Merged pair: (32, 118)  -->   455\n",
            "Iteration  300 | No. of tokens: 458709 | Merged pair: (453, 110) -->   555\n",
            "Iteration  400 | No. of tokens: 430368 | Merged pair: (459, 110) -->   655\n",
            "Iteration  500 | No. of tokens: 410025 | Merged pair: (300, 408) -->   755\n",
            "Iteration  600 | No. of tokens: 394498 | Merged pair: (103, 103) -->   855\n",
            "Iteration  700 | No. of tokens: 382035 | Merged pair: (290, 107) -->   955\n",
            "Iteration  800 | No. of tokens: 371558 | Merged pair: (341, 285) -->  1055\n",
            "Iteration  900 | No. of tokens: 362520 | Merged pair: (735, 1080) -->  1155\n",
            "Iteration 1000 | No. of tokens: 354597 | Merged pair: (109, 98)  -->  1255\n",
            "Compression : 2.82 X\n"
          ]
        }
      ],
      "source": [
        "# now lets train for 1k iterations\n",
        "regex_tokenizer = RegexTokenizer()\n",
        "regex_tokenizer.train(text=text, vocab_size=1256, verbose=True, verbose_iters=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "1aGXlDT5DmlM"
      },
      "outputs": [],
      "source": [
        "# lets visualise the merges\n",
        "with open(\"regex_merges.txt\", \"w\") as file:\n",
        "    sorted_merges = sorted(list(regex_tokenizer.merges.items()), key=lambda x: x[1])\n",
        "    for k, v in sorted_merges:\n",
        "        file.write(f\"[{(regex_tokenizer.vocab[k[0]]).decode('utf-8', errors='replace')}][{(regex_tokenizer.vocab[k[1]]).decode('utf-8', errors='replace')}]  ---->   {v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "6HcQBfdJ3WAB",
        "outputId": "e0d713ea-18f3-43f5-a885-6af5ebef3aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Test string: 0 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 27235.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Test string: 1 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 84260.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "Test string: 2 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:00<00:00, 116415.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "328\n",
            "Test string: 3 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46195/46195 [00:00<00:00, 293444.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86282\n",
            "Test string: 4 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert test_text == regex_tokenizer.decode(regex_tokenizer.encode(test_text))\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46195/46195 [00:00<00:00, 303990.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "99286"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(regex_tokenizer.encode(unpack(test_strings[4])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyh2Ed_x3V6W",
        "outputId": "ff3c45c8-5cf6-4657-c19d-c140cacc755c"
      },
      "source": [
        "## GPT-4 tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "vwmKWKT03V3h"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\")\n",
        "text = enc.decode(ids) # get the same text back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "0ogWro143V0T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[15339,\n",
              " 1917,\n",
              " 12340,\n",
              " 30,\n",
              " 320,\n",
              " 31495,\n",
              " 230,\n",
              " 75265,\n",
              " 243,\n",
              " 92245,\n",
              " 16715,\n",
              " 28509,\n",
              " 4513,\n",
              " 57037]"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{b'!': 0,\n",
              " b'\"': 1,\n",
              " b'#': 2,\n",
              " b'$': 3,\n",
              " b'%': 4,\n",
              " b'&': 5,\n",
              " b\"'\": 6,\n",
              " b'(': 7,\n",
              " b')': 8,\n",
              " b'*': 9,\n",
              " b'+': 10,\n",
              " b',': 11,\n",
              " b'-': 12,\n",
              " b'.': 13,\n",
              " b'/': 14,\n",
              " b'0': 15,\n",
              " b'1': 16,\n",
              " b'2': 17,\n",
              " b'3': 18,\n",
              " b'4': 19,\n",
              " b'5': 20,\n",
              " b'6': 21,\n",
              " b'7': 22,\n",
              " b'8': 23,\n",
              " b'9': 24,\n",
              " b':': 25,\n",
              " b';': 26,\n",
              " b'<': 27,\n",
              " b'=': 28,\n",
              " b'>': 29,\n",
              " b'?': 30,\n",
              " b'@': 31,\n",
              " b'A': 32,\n",
              " b'B': 33,\n",
              " b'C': 34,\n",
              " b'D': 35,\n",
              " b'E': 36,\n",
              " b'F': 37,\n",
              " b'G': 38,\n",
              " b'H': 39,\n",
              " b'I': 40,\n",
              " b'J': 41,\n",
              " b'K': 42,\n",
              " b'L': 43,\n",
              " b'M': 44,\n",
              " b'N': 45,\n",
              " b'O': 46,\n",
              " b'P': 47,\n",
              " b'Q': 48,\n",
              " b'R': 49,\n",
              " b'S': 50,\n",
              " b'T': 51,\n",
              " b'U': 52,\n",
              " b'V': 53,\n",
              " b'W': 54,\n",
              " b'X': 55,\n",
              " b'Y': 56,\n",
              " b'Z': 57,\n",
              " b'[': 58,\n",
              " b'\\\\': 59,\n",
              " b']': 60,\n",
              " b'^': 61,\n",
              " b'_': 62,\n",
              " b'`': 63,\n",
              " b'a': 64,\n",
              " b'b': 65,\n",
              " b'c': 66,\n",
              " b'd': 67,\n",
              " b'e': 68,\n",
              " b'f': 69,\n",
              " b'g': 70,\n",
              " b'h': 71,\n",
              " b'i': 72,\n",
              " b'j': 73,\n",
              " b'k': 74,\n",
              " b'l': 75,\n",
              " b'm': 76,\n",
              " b'n': 77,\n",
              " b'o': 78,\n",
              " b'p': 79,\n",
              " b'q': 80,\n",
              " b'r': 81,\n",
              " b's': 82,\n",
              " b't': 83,\n",
              " b'u': 84,\n",
              " b'v': 85,\n",
              " b'w': 86,\n",
              " b'x': 87,\n",
              " b'y': 88,\n",
              " b'z': 89,\n",
              " b'{': 90,\n",
              " b'|': 91,\n",
              " b'}': 92,\n",
              " b'~': 93,\n",
              " b'\\xa1': 94,\n",
              " b'\\xa2': 95,\n",
              " b'\\xa3': 96,\n",
              " b'\\xa4': 97,\n",
              " b'\\xa5': 98,\n",
              " b'\\xa6': 99,\n",
              " b'\\xa7': 100,\n",
              " b'\\xa8': 101,\n",
              " b'\\xa9': 102,\n",
              " b'\\xaa': 103,\n",
              " b'\\xab': 104,\n",
              " b'\\xac': 105,\n",
              " b'\\xae': 106,\n",
              " b'\\xaf': 107,\n",
              " b'\\xb0': 108,\n",
              " b'\\xb1': 109,\n",
              " b'\\xb2': 110,\n",
              " b'\\xb3': 111,\n",
              " b'\\xb4': 112,\n",
              " b'\\xb5': 113,\n",
              " b'\\xb6': 114,\n",
              " b'\\xb7': 115,\n",
              " b'\\xb8': 116,\n",
              " b'\\xb9': 117,\n",
              " b'\\xba': 118,\n",
              " b'\\xbb': 119,\n",
              " b'\\xbc': 120,\n",
              " b'\\xbd': 121,\n",
              " b'\\xbe': 122,\n",
              " b'\\xbf': 123,\n",
              " b'\\xc0': 124,\n",
              " b'\\xc1': 125,\n",
              " b'\\xc2': 126,\n",
              " b'\\xc3': 127,\n",
              " b'\\xc4': 128,\n",
              " b'\\xc5': 129,\n",
              " b'\\xc6': 130,\n",
              " b'\\xc7': 131,\n",
              " b'\\xc8': 132,\n",
              " b'\\xc9': 133,\n",
              " b'\\xca': 134,\n",
              " b'\\xcb': 135,\n",
              " b'\\xcc': 136,\n",
              " b'\\xcd': 137,\n",
              " b'\\xce': 138,\n",
              " b'\\xcf': 139,\n",
              " b'\\xd0': 140,\n",
              " b'\\xd1': 141,\n",
              " b'\\xd2': 142,\n",
              " b'\\xd3': 143,\n",
              " b'\\xd4': 144,\n",
              " b'\\xd5': 145,\n",
              " b'\\xd6': 146,\n",
              " b'\\xd7': 147,\n",
              " b'\\xd8': 148,\n",
              " b'\\xd9': 149,\n",
              " b'\\xda': 150,\n",
              " b'\\xdb': 151,\n",
              " b'\\xdc': 152,\n",
              " b'\\xdd': 153,\n",
              " b'\\xde': 154,\n",
              " b'\\xdf': 155,\n",
              " b'\\xe0': 156,\n",
              " b'\\xe1': 157,\n",
              " b'\\xe2': 158,\n",
              " b'\\xe3': 159,\n",
              " b'\\xe4': 160,\n",
              " b'\\xe5': 161,\n",
              " b'\\xe6': 162,\n",
              " b'\\xe7': 163,\n",
              " b'\\xe8': 164,\n",
              " b'\\xe9': 165,\n",
              " b'\\xea': 166,\n",
              " b'\\xeb': 167,\n",
              " b'\\xec': 168,\n",
              " b'\\xed': 169,\n",
              " b'\\xee': 170,\n",
              " b'\\xef': 171,\n",
              " b'\\xf0': 172,\n",
              " b'\\xf1': 173,\n",
              " b'\\xf2': 174,\n",
              " b'\\xf3': 175,\n",
              " b'\\xf4': 176,\n",
              " b'\\xf5': 177,\n",
              " b'\\xf6': 178,\n",
              " b'\\xf7': 179,\n",
              " b'\\xf8': 180,\n",
              " b'\\xf9': 181,\n",
              " b'\\xfa': 182,\n",
              " b'\\xfb': 183,\n",
              " b'\\xfc': 184,\n",
              " b'\\xfd': 185,\n",
              " b'\\xfe': 186,\n",
              " b'\\xff': 187,\n",
              " b'\\x00': 188,\n",
              " b'\\x01': 189,\n",
              " b'\\x02': 190,\n",
              " b'\\x03': 191,\n",
              " b'\\x04': 192,\n",
              " b'\\x05': 193,\n",
              " b'\\x06': 194,\n",
              " b'\\x07': 195,\n",
              " b'\\x08': 196,\n",
              " b'\\t': 197,\n",
              " b'\\n': 198,\n",
              " b'\\x0b': 199,\n",
              " b'\\x0c': 200,\n",
              " b'\\r': 201,\n",
              " b'\\x0e': 202,\n",
              " b'\\x0f': 203,\n",
              " b'\\x10': 204,\n",
              " b'\\x11': 205,\n",
              " b'\\x12': 206,\n",
              " b'\\x13': 207,\n",
              " b'\\x14': 208,\n",
              " b'\\x15': 209,\n",
              " b'\\x16': 210,\n",
              " b'\\x17': 211,\n",
              " b'\\x18': 212,\n",
              " b'\\x19': 213,\n",
              " b'\\x1a': 214,\n",
              " b'\\x1b': 215,\n",
              " b'\\x1c': 216,\n",
              " b'\\x1d': 217,\n",
              " b'\\x1e': 218,\n",
              " b'\\x1f': 219,\n",
              " b' ': 220,\n",
              " b'\\x7f': 221,\n",
              " b'\\x80': 222,\n",
              " b'\\x81': 223,\n",
              " b'\\x82': 224,\n",
              " b'\\x83': 225,\n",
              " b'\\x84': 226,\n",
              " b'\\x85': 227,\n",
              " b'\\x86': 228,\n",
              " b'\\x87': 229,\n",
              " b'\\x88': 230,\n",
              " b'\\x89': 231,\n",
              " b'\\x8a': 232,\n",
              " b'\\x8b': 233,\n",
              " b'\\x8c': 234,\n",
              " b'\\x8d': 235,\n",
              " b'\\x8e': 236,\n",
              " b'\\x8f': 237,\n",
              " b'\\x90': 238,\n",
              " b'\\x91': 239,\n",
              " b'\\x92': 240,\n",
              " b'\\x93': 241,\n",
              " b'\\x94': 242,\n",
              " b'\\x95': 243,\n",
              " b'\\x96': 244,\n",
              " b'\\x97': 245,\n",
              " b'\\x98': 246,\n",
              " b'\\x99': 247,\n",
              " b'\\x9a': 248,\n",
              " b'\\x9b': 249,\n",
              " b'\\x9c': 250,\n",
              " b'\\x9d': 251,\n",
              " b'\\x9e': 252,\n",
              " b'\\x9f': 253,\n",
              " b'\\xa0': 254,\n",
              " b'\\xad': 255,\n",
              " b'  ': 256,\n",
              " b'    ': 257,\n",
              " b'in': 258,\n",
              " b' t': 259,\n",
              " b'        ': 260,\n",
              " b'er': 261,\n",
              " b'   ': 262,\n",
              " b'on': 263,\n",
              " b' a': 264,\n",
              " b're': 265,\n",
              " b'at': 266,\n",
              " b'st': 267,\n",
              " b'en': 268,\n",
              " b'or': 269,\n",
              " b' th': 270,\n",
              " b'\\n\\n': 271,\n",
              " b' c': 272,\n",
              " b'le': 273,\n",
              " b' s': 274,\n",
              " b'it': 275,\n",
              " b'an': 276,\n",
              " b'ar': 277,\n",
              " b'al': 278,\n",
              " b' the': 279,\n",
              " b';\\n': 280,\n",
              " b' p': 281,\n",
              " b' f': 282,\n",
              " b'ou': 283,\n",
              " b' =': 284,\n",
              " b'is': 285,\n",
              " b'       ': 286,\n",
              " b'ing': 287,\n",
              " b'es': 288,\n",
              " b' w': 289,\n",
              " b'ion': 290,\n",
              " b'ed': 291,\n",
              " b'ic': 292,\n",
              " b' b': 293,\n",
              " b' d': 294,\n",
              " b'et': 295,\n",
              " b' m': 296,\n",
              " b' o': 297,\n",
              " b'\\t\\t': 298,\n",
              " b'ro': 299,\n",
              " b'as': 300,\n",
              " b'el': 301,\n",
              " b'ct': 302,\n",
              " b'nd': 303,\n",
              " b' in': 304,\n",
              " b' h': 305,\n",
              " b'ent': 306,\n",
              " b'id': 307,\n",
              " b' n': 308,\n",
              " b'am': 309,\n",
              " b'           ': 310,\n",
              " b' to': 311,\n",
              " b' re': 312,\n",
              " b'--': 313,\n",
              " b' {': 314,\n",
              " b' of': 315,\n",
              " b'om': 316,\n",
              " b');\\n': 317,\n",
              " b'im': 318,\n",
              " b'\\r\\n': 319,\n",
              " b' (': 320,\n",
              " b'il': 321,\n",
              " b'//': 322,\n",
              " b' and': 323,\n",
              " b'ur': 324,\n",
              " b'se': 325,\n",
              " b' l': 326,\n",
              " b'ex': 327,\n",
              " b' S': 328,\n",
              " b'ad': 329,\n",
              " b' \"': 330,\n",
              " b'ch': 331,\n",
              " b'ut': 332,\n",
              " b'if': 333,\n",
              " b'**': 334,\n",
              " b' }': 335,\n",
              " b'em': 336,\n",
              " b'ol': 337,\n",
              " b'                ': 338,\n",
              " b'th': 339,\n",
              " b')\\n': 340,\n",
              " b' {\\n': 341,\n",
              " b' g': 342,\n",
              " b'ig': 343,\n",
              " b'iv': 344,\n",
              " b',\\n': 345,\n",
              " b'ce': 346,\n",
              " b'od': 347,\n",
              " b' v': 348,\n",
              " b'ate': 349,\n",
              " b' T': 350,\n",
              " b'ag': 351,\n",
              " b'ay': 352,\n",
              " b' *': 353,\n",
              " b'ot': 354,\n",
              " b'us': 355,\n",
              " b' C': 356,\n",
              " b' st': 357,\n",
              " b' I': 358,\n",
              " b'un': 359,\n",
              " b'ul': 360,\n",
              " b'ue': 361,\n",
              " b' A': 362,\n",
              " b'ow': 363,\n",
              " b\" '\": 364,\n",
              " b'ew': 365,\n",
              " b' <': 366,\n",
              " b'ation': 367,\n",
              " b'()': 368,\n",
              " b' for': 369,\n",
              " b'ab': 370,\n",
              " b'ort': 371,\n",
              " b'um': 372,\n",
              " b'ame': 373,\n",
              " b' is': 374,\n",
              " b'pe': 375,\n",
              " b'tr': 376,\n",
              " b'ck': 377,\n",
              " b'\\xe2\\x80': 378,\n",
              " b' y': 379,\n",
              " b'ist': 380,\n",
              " b'----': 381,\n",
              " b'.\\n\\n': 382,\n",
              " b'he': 383,\n",
              " b' e': 384,\n",
              " b'lo': 385,\n",
              " b' M': 386,\n",
              " b' be': 387,\n",
              " b'ers': 388,\n",
              " b' on': 389,\n",
              " b' con': 390,\n",
              " b'ap': 391,\n",
              " b'ub': 392,\n",
              " b' P': 393,\n",
              " b'               ': 394,\n",
              " b'ass': 395,\n",
              " b'int': 396,\n",
              " b'>\\n': 397,\n",
              " b'ly': 398,\n",
              " b'urn': 399,\n",
              " b' $': 400,\n",
              " b';\\n\\n': 401,\n",
              " b'av': 402,\n",
              " b'port': 403,\n",
              " b'ir': 404,\n",
              " b'->': 405,\n",
              " b'nt': 406,\n",
              " b'ction': 407,\n",
              " b'end': 408,\n",
              " b' de': 409,\n",
              " b'00': 410,\n",
              " b'ith': 411,\n",
              " b'out': 412,\n",
              " b'turn': 413,\n",
              " b'our': 414,\n",
              " b'     ': 415,\n",
              " b'lic': 416,\n",
              " b'res': 417,\n",
              " b'pt': 418,\n",
              " b'==': 419,\n",
              " b' this': 420,\n",
              " b' wh': 421,\n",
              " b' if': 422,\n",
              " b' D': 423,\n",
              " b'ver': 424,\n",
              " b'age': 425,\n",
              " b' B': 426,\n",
              " b'ht': 427,\n",
              " b'ext': 428,\n",
              " b'=\"': 429,\n",
              " b' that': 430,\n",
              " b'****': 431,\n",
              " b' R': 432,\n",
              " b' it': 433,\n",
              " b'ess': 434,\n",
              " b' F': 435,\n",
              " b' r': 436,\n",
              " b'os': 437,\n",
              " b'and': 438,\n",
              " b' as': 439,\n",
              " b'ect': 440,\n",
              " b'ke': 441,\n",
              " b'rom': 442,\n",
              " b' //': 443,\n",
              " b'con': 444,\n",
              " b' L': 445,\n",
              " b'(\"': 446,\n",
              " b'qu': 447,\n",
              " b'lass': 448,\n",
              " b' with': 449,\n",
              " b'iz': 450,\n",
              " b'de': 451,\n",
              " b' N': 452,\n",
              " b' al': 453,\n",
              " b'op': 454,\n",
              " b'up': 455,\n",
              " b'get': 456,\n",
              " b' }\\n': 457,\n",
              " b'ile': 458,\n",
              " b' an': 459,\n",
              " b'ata': 460,\n",
              " b'ore': 461,\n",
              " b'ri': 462,\n",
              " b' pro': 463,\n",
              " b';\\r\\n': 464,\n",
              " b'\\t\\t\\t\\t': 465,\n",
              " b'ter': 466,\n",
              " b'ain': 467,\n",
              " b' W': 468,\n",
              " b' E': 469,\n",
              " b' com': 470,\n",
              " b' return': 471,\n",
              " b'art': 472,\n",
              " b' H': 473,\n",
              " b'ack': 474,\n",
              " b'import': 475,\n",
              " b'ublic': 476,\n",
              " b' or': 477,\n",
              " b'est': 478,\n",
              " b'ment': 479,\n",
              " b' G': 480,\n",
              " b'able': 481,\n",
              " b' -': 482,\n",
              " b'ine': 483,\n",
              " b'ill': 484,\n",
              " b'ind': 485,\n",
              " b'ere': 486,\n",
              " b'::': 487,\n",
              " b'ity': 488,\n",
              " b' +': 489,\n",
              " b' tr': 490,\n",
              " b'elf': 491,\n",
              " b'ight': 492,\n",
              " b\"('\": 493,\n",
              " b'orm': 494,\n",
              " b'ult': 495,\n",
              " b'str': 496,\n",
              " b'..': 497,\n",
              " b'\",': 498,\n",
              " b' you': 499,\n",
              " b'ype': 500,\n",
              " b'pl': 501,\n",
              " b' new': 502,\n",
              " b' j': 503,\n",
              " b'                   ': 504,\n",
              " b' from': 505,\n",
              " b' ex': 506,\n",
              " b' O': 507,\n",
              " b'20': 508,\n",
              " b'ld': 509,\n",
              " b' [': 510,\n",
              " b'oc': 511,\n",
              " b':\\n': 512,\n",
              " b' se': 513,\n",
              " b' le': 514,\n",
              " b'--------': 515,\n",
              " b'.s': 516,\n",
              " b'{\\n': 517,\n",
              " b\"',\": 518,\n",
              " b'ant': 519,\n",
              " b' at': 520,\n",
              " b'ase': 521,\n",
              " b'.c': 522,\n",
              " b' ch': 523,\n",
              " b'</': 524,\n",
              " b'ave': 525,\n",
              " b'ang': 526,\n",
              " b' are': 527,\n",
              " b' int': 528,\n",
              " b'\\xe2\\x80\\x99': 529,\n",
              " b'_t': 530,\n",
              " b'ert': 531,\n",
              " b'ial': 532,\n",
              " b'act': 533,\n",
              " b'}\\n': 534,\n",
              " b'ive': 535,\n",
              " b'ode': 536,\n",
              " b'ost': 537,\n",
              " b' class': 538,\n",
              " b' not': 539,\n",
              " b'og': 540,\n",
              " b'ord': 541,\n",
              " b'alue': 542,\n",
              " b'all': 543,\n",
              " b'ff': 544,\n",
              " b'();\\n': 545,\n",
              " b'ont': 546,\n",
              " b'ime': 547,\n",
              " b'are': 548,\n",
              " b' U': 549,\n",
              " b' pr': 550,\n",
              " b' :': 551,\n",
              " b'ies': 552,\n",
              " b'ize': 553,\n",
              " b'ure': 554,\n",
              " b' by': 555,\n",
              " b'ire': 556,\n",
              " b' }\\n\\n': 557,\n",
              " b'.p': 558,\n",
              " b' sh': 559,\n",
              " b'ice': 560,\n",
              " b'ast': 561,\n",
              " b'ption': 562,\n",
              " b'tring': 563,\n",
              " b'ok': 564,\n",
              " b'__': 565,\n",
              " b'cl': 566,\n",
              " b'##': 567,\n",
              " b' he': 568,\n",
              " b'ard': 569,\n",
              " b').': 570,\n",
              " b' @': 571,\n",
              " b'iew': 572,\n",
              " b'\\t\\t\\t': 573,\n",
              " b' was': 574,\n",
              " b'ip': 575,\n",
              " b'this': 576,\n",
              " b' u': 577,\n",
              " b' The': 578,\n",
              " b'ide': 579,\n",
              " b'ace': 580,\n",
              " b'ib': 581,\n",
              " b'ac': 582,\n",
              " b'rou': 583,\n",
              " b' we': 584,\n",
              " b'ject': 585,\n",
              " b' public': 586,\n",
              " b'ak': 587,\n",
              " b've': 588,\n",
              " b'ath': 589,\n",
              " b'oid': 590,\n",
              " b' =>': 591,\n",
              " b'ust': 592,\n",
              " b'que': 593,\n",
              " b' res': 594,\n",
              " b'))': 595,\n",
              " b\"'s\": 596,\n",
              " b' k': 597,\n",
              " b'ans': 598,\n",
              " b'yst': 599,\n",
              " b'unction': 600,\n",
              " b'********': 601,\n",
              " b' i': 602,\n",
              " b' us': 603,\n",
              " b'pp': 604,\n",
              " b'10': 605,\n",
              " b'one': 606,\n",
              " b'ail': 607,\n",
              " b'====': 608,\n",
              " b'name': 609,\n",
              " b' str': 610,\n",
              " b' /': 611,\n",
              " b' &': 612,\n",
              " b'ach': 613,\n",
              " b'div': 614,\n",
              " b'ystem': 615,\n",
              " b'ell': 616,\n",
              " b' have': 617,\n",
              " b'err': 618,\n",
              " b'ould': 619,\n",
              " b'ull': 620,\n",
              " b'pon': 621,\n",
              " b' J': 622,\n",
              " b'_p': 623,\n",
              " b' ==': 624,\n",
              " b'ign': 625,\n",
              " b'St': 626,\n",
              " b'.\\n': 627,\n",
              " b' pl': 628,\n",
              " b');\\n\\n': 629,\n",
              " b'form': 630,\n",
              " b'put': 631,\n",
              " b'ount': 632,\n",
              " b'}\\n\\n': 633,\n",
              " b'dd': 634,\n",
              " b'ite': 635,\n",
              " b' get': 636,\n",
              " b'rr': 637,\n",
              " b'ome': 638,\n",
              " b' \\xe2\\x80': 639,\n",
              " b'aram': 640,\n",
              " b'cc': 641,\n",
              " b' */': 642,\n",
              " b'ER': 643,\n",
              " b'In': 644,\n",
              " b'les': 645,\n",
              " b'_s': 646,\n",
              " b'ong': 647,\n",
              " b'ie': 648,\n",
              " b' can': 649,\n",
              " b' V': 650,\n",
              " b'erv': 651,\n",
              " b'pr': 652,\n",
              " b' un': 653,\n",
              " b'row': 654,\n",
              " b'ber': 655,\n",
              " b' do': 656,\n",
              " b'll': 657,\n",
              " b' el': 658,\n",
              " b' self': 659,\n",
              " b'ated': 660,\n",
              " b'ary': 661,\n",
              " b' .': 662,\n",
              " b\"']\": 663,\n",
              " b'ud': 664,\n",
              " b' en': 665,\n",
              " b' Th': 666,\n",
              " b'                       ': 667,\n",
              " b'te': 668,\n",
              " b'_c': 669,\n",
              " b'uct': 670,\n",
              " b' ab': 671,\n",
              " b'ork': 672,\n",
              " b'.get': 673,\n",
              " b' #': 674,\n",
              " b'aw': 675,\n",
              " b'ress': 676,\n",
              " b'ob': 677,\n",
              " b'Name': 678,\n",
              " b'201': 679,\n",
              " b'app': 680,\n",
              " b\"['\": 681,\n",
              " b' all': 682,\n",
              " b'ory': 683,\n",
              " b'ition': 684,\n",
              " b'ance': 685,\n",
              " b'ear': 686,\n",
              " b' cont': 687,\n",
              " b'vent': 688,\n",
              " b'ia': 689,\n",
              " b' will': 690,\n",
              " b'IN': 691,\n",
              " b'         ': 692,\n",
              " b'return': 693,\n",
              " b' </': 694,\n",
              " b'data': 695,\n",
              " b')\\n\\n': 696,\n",
              " b'Re': 697,\n",
              " b'ple': 698,\n",
              " b'ild': 699,\n",
              " b'ther': 700,\n",
              " b' your': 701,\n",
              " b'\"\\n': 702,\n",
              " b'($': 703,\n",
              " b' out': 704,\n",
              " b'),': 705,\n",
              " b' has': 706,\n",
              " b'String': 707,\n",
              " b'so': 708,\n",
              " b' up': 709,\n",
              " b'ax': 710,\n",
              " b' def': 711,\n",
              " b' bo': 712,\n",
              " b'ge': 713,\n",
              " b'alse': 714,\n",
              " b'ON': 715,\n",
              " b'per': 716,\n",
              " b'12': 717,\n",
              " b'ich': 718,\n",
              " b' but': 719,\n",
              " b' \\n': 720,\n",
              " b' _': 721,\n",
              " b'_m': 722,\n",
              " b'add': 723,\n",
              " b'quest': 724,\n",
              " b'odel': 725,\n",
              " b'self': 726,\n",
              " b'ery': 727,\n",
              " b'ft': 728,\n",
              " b'ens': 729,\n",
              " b'////': 730,\n",
              " b'ake': 731,\n",
              " b'.C': 732,\n",
              " b' go': 733,\n",
              " b' function': 734,\n",
              " b' K': 735,\n",
              " b'ivate': 736,\n",
              " b' im': 737,\n",
              " b' const': 738,\n",
              " b'.t': 739,\n",
              " b' */\\n': 740,\n",
              " b');\\r\\n': 741,\n",
              " b' void': 742,\n",
              " b' set': 743,\n",
              " b' System': 744,\n",
              " b'cri': 745,\n",
              " b'()\\n': 746,\n",
              " b'li': 747,\n",
              " b'\\tif': 748,\n",
              " b'.m': 749,\n",
              " b'ally': 750,\n",
              " b'set': 751,\n",
              " b'ep': 752,\n",
              " b'\\xe2\\x80\\x99s': 753,\n",
              " b'bo': 754,\n",
              " b'def': 755,\n",
              " b\"',\\n\": 756,\n",
              " b' me': 757,\n",
              " b' !': 758,\n",
              " b'atch': 759,\n",
              " b'\">': 760,\n",
              " b'\",\\n': 761,\n",
              " b'ec': 762,\n",
              " b' In': 763,\n",
              " b'ph': 764,\n",
              " b' |': 765,\n",
              " b'_f': 766,\n",
              " b' var': 767,\n",
              " b'ence': 768,\n",
              " b'Id': 769,\n",
              " b'ree': 770,\n",
              " b'ink': 771,\n",
              " b'lect': 772,\n",
              " b'ug': 773,\n",
              " b'eth': 774,\n",
              " b' else': 775,\n",
              " b'----------------': 776,\n",
              " b'19': 777,\n",
              " b'cont': 778,\n",
              " b' so': 779,\n",
              " b'atic': 780,\n",
              " b' lo': 781,\n",
              " b'pro': 782,\n",
              " b'ton': 783,\n",
              " b'ss': 784,\n",
              " b'own': 785,\n",
              " b'abel': 786,\n",
              " b'oint': 787,\n",
              " b'ous': 788,\n",
              " b'eld': 789,\n",
              " b'ST': 790,\n",
              " b'The': 791,\n",
              " b'                                ': 792,\n",
              " b'RE': 793,\n",
              " b'\":': 794,\n",
              " b'olor': 795,\n",
              " b'tp': 796,\n",
              " b'eg': 797,\n",
              " b'key': 798,\n",
              " b'ude': 799,\n",
              " b' St': 800,\n",
              " b'ound': 801,\n",
              " b' ar': 802,\n",
              " b'\");\\n': 803,\n",
              " b'ener': 804,\n",
              " b'ser': 805,\n",
              " b'11': 806,\n",
              " b'bject': 807,\n",
              " b'essage': 808,\n",
              " b'fer': 809,\n",
              " b' more': 810,\n",
              " b'ations': 811,\n",
              " b'ents': 812,\n",
              " b' his': 813,\n",
              " b' they': 814,\n",
              " b'.S': 815,\n",
              " b' Y': 816,\n",
              " b'use': 817,\n",
              " b'ne': 818,\n",
              " b'ish': 819,\n",
              " b'old': 820,\n",
              " b'_d': 821,\n",
              " b'io': 822,\n",
              " b'ield': 823,\n",
              " b' per': 824,\n",
              " b'Cont': 825,\n",
              " b'ings': 826,\n",
              " b'####': 827,\n",
              " b' data': 828,\n",
              " b' sa': 829,\n",
              " b'ef': 830,\n",
              " b'fo': 831,\n",
              " b' one': 832,\n",
              " b'eng': 833,\n",
              " b' dis': 834,\n",
              " b'AT': 835,\n",
              " b' name': 836,\n",
              " b' true': 837,\n",
              " b'val': 838,\n",
              " b'led': 839,\n",
              " b'.f': 840,\n",
              " b' ne': 841,\n",
              " b' end': 842,\n",
              " b'32': 843,\n",
              " b'.T': 844,\n",
              " b'16': 845,\n",
              " b'cre': 846,\n",
              " b'ark': 847,\n",
              " b'log': 848,\n",
              " b'Ex': 849,\n",
              " b'error': 850,\n",
              " b'_id': 851,\n",
              " b'urre': 852,\n",
              " b'ange': 853,\n",
              " b' null': 854,\n",
              " b'rray': 855,\n",
              " b' my': 856,\n",
              " b'pan': 857,\n",
              " b'ict': 858,\n",
              " b'ator': 859,\n",
              " b'View': 860,\n",
              " b'List': 861,\n",
              " b'\\treturn': 862,\n",
              " b'\\xe2\\x80\\x9d': 863,\n",
              " b' pre': 864,\n",
              " b' x': 865,\n",
              " b'clude': 866,\n",
              " b'arg': 867,\n",
              " b'15': 868,\n",
              " b'ov': 869,\n",
              " b'.h': 870,\n",
              " b' >': 871,\n",
              " b' their': 872,\n",
              " b\"')\": 873,\n",
              " b'irst': 874,\n",
              " b'ick': 875,\n",
              " b'gh': 876,\n",
              " b'LE': 877,\n",
              " b'OR': 878,\n",
              " b' private': 879,\n",
              " b'tem': 880,\n",
              " b'\\r\\n\\r\\n': 881,\n",
              " b'user': 882,\n",
              " b' )': 883,\n",
              " b'com': 884,\n",
              " b'.A': 885,\n",
              " b'\";\\n': 886,\n",
              " b' id': 887,\n",
              " b'read': 888,\n",
              " b' who': 889,\n",
              " b'_b': 890,\n",
              " b'\">\\n': 891,\n",
              " b' time': 892,\n",
              " b' man': 893,\n",
              " b'ry': 894,\n",
              " b'========': 895,\n",
              " b'roup': 896,\n",
              " b'rop': 897,\n",
              " b'public': 898,\n",
              " b'vel': 899,\n",
              " b'umber': 900,\n",
              " b'ble': 901,\n",
              " b' which': 902,\n",
              " b'****************': 903,\n",
              " b' any': 904,\n",
              " b' false': 905,\n",
              " b'we': 906,\n",
              " b' value': 907,\n",
              " b' li': 908,\n",
              " b'\")': 909,\n",
              " b'nder': 910,\n",
              " b'gr': 911,\n",
              " b' no': 912,\n",
              " b'param': 913,\n",
              " b'25': 914,\n",
              " b'fig': 915,\n",
              " b'.com': 916,\n",
              " b' app': 917,\n",
              " b'_l': 918,\n",
              " b'ions': 919,\n",
              " b'.D': 920,\n",
              " b' Ch': 921,\n",
              " b' about': 922,\n",
              " b' add': 923,\n",
              " b' su': 924,\n",
              " b' string': 925,\n",
              " b'ID': 926,\n",
              " b' over': 927,\n",
              " b'string': 928,\n",
              " b'.l': 929,\n",
              " b'ource': 930,\n",
              " b'000': 931,\n",
              " b'_C': 932,\n",
              " b']\\n': 933,\n",
              " b' qu': 934,\n",
              " b' String': 935,\n",
              " b'ca': 936,\n",
              " b'SE': 937,\n",
              " b' ro': 938,\n",
              " b'sh': 939,\n",
              " b'ual': 940,\n",
              " b'Type': 941,\n",
              " b'son': 942,\n",
              " b'new': 943,\n",
              " b'ern': 944,\n",
              " b' ag': 945,\n",
              " b'AR': 946,\n",
              " b'];\\n': 947,\n",
              " b'].': 948,\n",
              " b' ?': 949,\n",
              " b'ical': 950,\n",
              " b' des': 951,\n",
              " b'uth': 952,\n",
              " b'ix': 953,\n",
              " b'ays': 954,\n",
              " b' type': 955,\n",
              " b\"'t\": 956,\n",
              " b'ault': 957,\n",
              " b' inter': 958,\n",
              " b'var': 959,\n",
              " b'.b': 960,\n",
              " b' part': 961,\n",
              " b'.d': 962,\n",
              " b'urrent': 963,\n",
              " b'IT': 964,\n",
              " b'EN': 965,\n",
              " b'30': 966,\n",
              " b'enc': 967,\n",
              " b'(f': 968,\n",
              " b'ra': 969,\n",
              " b'value': 970,\n",
              " b'cho': 971,\n",
              " b'18': 972,\n",
              " b'utton': 973,\n",
              " b'ose': 974,\n",
              " b'14': 975,\n",
              " b' !=': 976,\n",
              " b'ater': 977,\n",
              " b'\\xc3\\xa9': 978,\n",
              " b'reate': 979,\n",
              " b'oll': 980,\n",
              " b'pos': 981,\n",
              " b'yle': 982,\n",
              " b'ng': 983,\n",
              " b'AL': 984,\n",
              " b'using': 985,\n",
              " b'ames': 986,\n",
              " b' {\\r\\n': 987,\n",
              " b'ates': 988,\n",
              " b'ely': 989,\n",
              " b' work': 990,\n",
              " b' em': 991,\n",
              " b'inal': 992,\n",
              " b' sp': 993,\n",
              " b' when': 994,\n",
              " b'.set': 995,\n",
              " b'      ': 996,\n",
              " b'):\\n': 997,\n",
              " b'to': 998,\n",
              " b'quire': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc._mergeable_ranks # similar to vocab except we map string to id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100256"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(enc._mergeable_ranks) #100k merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100256"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {v:k for k, v in enc._mergeable_ranks.items()}\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b' Conveyor'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[100255]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now lets implement the GPT4Tokenizer \n",
        "without handling special tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT4Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_path = \"cl100k_base\"\n",
        "        self.enc = tiktoken.get_encoding(self.tokenizer_path)\n",
        "        self._create_vocab_and_merges()\n",
        "        # now here is another tricky part.\n",
        "        # for some reason, the tokens corresponding to individual bytes\n",
        "        # are permuted in a different order. This is completely non-sensical\n",
        "        # and probably historical, but therefore we have to deal with it here.\n",
        "        \n",
        "        self.byte_shuffle = {i: self.enc._mergeable_ranks[bytes([i])] for i in range(256)} # does map actual byte 0 to the one in this dict\n",
        "        self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n",
        "\n",
        "    def _get_split_tokens(self, mergeable_ranks, token, max_rank):\n",
        "        # Idea: Get the most optimal split of the token into exactly two pre-existing tokens\n",
        "        parts = [bytes([t]) for t in token]\n",
        "        while True:\n",
        "            min_id, min_rank = None, None # min_id -> the point to split the token, min_rank -> to find the most optimal split\n",
        "            for i, pair in enumerate(zip(parts, parts[1:])):\n",
        "                rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "\n",
        "                if (rank is not None) and (min_rank is None or rank < min_rank):\n",
        "                    min_id = i\n",
        "                    min_rank = rank\n",
        "\n",
        "            if (min_rank is None) or (min_rank >= max_rank): # no split was obtained\n",
        "                break\n",
        "            # else update the tokens\n",
        "            assert min_id is not None\n",
        "            parts = parts[:min_id] + [parts[min_id] + parts[min_id + 1]] + parts[min_id + 2:]\n",
        "\n",
        "        return parts\n",
        "\n",
        "    def _create_vocab_and_merges(self):\n",
        "        self.vocab = {v:k for k, v in self.enc._mergeable_ranks.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        self.merges = {}\n",
        "        for token, idx in self.enc._mergeable_ranks.items():\n",
        "            if len(token) < 2:\n",
        "                continue\n",
        "            parts = self._get_split_tokens(self.enc._mergeable_ranks, token, idx)\n",
        "            assert len(parts) == 2\n",
        "            self.merges[(self.enc._mergeable_ranks[parts[0]], self.enc._mergeable_ranks[parts[1]])] = idx\n",
        "\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def encode(self, text):\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "        # now shuffle the byte tokens\n",
        "        shuffled_tokens_list = [[self.byte_shuffle[b] for b in tokens] for tokens in tokens_list]\n",
        "        encoded_tokens_list = []\n",
        "\n",
        "        num_merges = 0\n",
        "\n",
        "        for tokens in shuffled_tokens_list:\n",
        "            while len(tokens) >= 2:\n",
        "                # first get the stats of the bigrams\n",
        "                pair_stats = get_stats(tokens)\n",
        "                # now check if there is a pair which is merged as per our tokenizer\n",
        "                merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "                # check if there is actually a match\n",
        "                if merge_pair not in self.merges:\n",
        "                    break\n",
        "\n",
        "                # now replace with the merges token\n",
        "                num_merges += 1\n",
        "                tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "\n",
        "            encoded_tokens_list.append(tokens)\n",
        "\n",
        "        print(f\"Number of merges: {num_merges}\")\n",
        "        final_tokens = [item for sublist in encoded_tokens_list for item in sublist]\n",
        "\n",
        "        return final_tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text = b\"\".join(self.vocab[id] for id in ids)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt4_tokenizer = GPT4Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test against vocab and the function he had created directly\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "vocab = {v:k for k, v in enc._mergeable_ranks.items()}\n",
        "\n",
        "# Ref: https://github.com/openai/tiktoken/issues/60#issuecomment-1499977960\n",
        "\n",
        "def bpe(mergeable_ranks, token, max_rank = None):\n",
        "    parts = [bytes([b]) for b in token]\n",
        "    while True:\n",
        "        min_idx = None\n",
        "        min_rank = None\n",
        "        for i, pair in enumerate(zip(parts, parts[1:])):\n",
        "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "            if rank is not None and (min_rank is None or rank < min_rank): # in case we get a hit on a pair and min rank is not set or min rank is greater\n",
        "                min_idx = i\n",
        "                min_rank = rank\n",
        "        if min_rank is None or (max_rank is not None and min_rank >= max_rank): # did not get merge\n",
        "            break\n",
        "        assert min_idx is not None\n",
        "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
        "    return parts\n",
        "\n",
        "merges = {}\n",
        "\n",
        "for token, idx in enc._mergeable_ranks.items():\n",
        "    if len(token) < 2:\n",
        "        continue\n",
        "    parts = bpe(enc._mergeable_ranks, token, idx)\n",
        "    assert len(parts) == 2\n",
        "    merges[(enc._mergeable_ranks[parts[0]], enc._mergeable_ranks[parts[1]])] = idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert vocab == gpt4_tokenizer.vocab\n",
        "assert merges == gpt4_tokenizer.merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets visualise the merges\n",
        "with open(\"gpt4_merges.txt\", \"w\") as file:\n",
        "    sorted_merges = sorted(list(gpt4_tokenizer.merges.items()), key=lambda x: x[1])\n",
        "    for k, v in sorted_merges:\n",
        "        file.write(f\"[{(gpt4_tokenizer.vocab[k[0]]).decode('utf-8', errors='replace')}][{(gpt4_tokenizer.vocab[k[1]]).decode('utf-8', errors='replace')}]  ---->   {v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 0\n",
            "Test string: 0 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29330.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 0\n",
            "Test string: 1 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 37635.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 32\n",
            "Test string: 2 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:00<00:00, 121612.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 445\n",
            "Test string: 3 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46195/46195 [00:00<00:00, 251278.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 136282\n",
            "Test string: 4 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert test_text == gpt4_tokenizer.decode(gpt4_tokenizer.encode(test_text))\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 0\n",
            "Test string: 0 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 26214.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 0\n",
            "Test string: 1 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 18513.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 32\n",
            "Test string: 2 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:00<00:00, 100362.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 445\n",
            "Test string: 3 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46195/46195 [00:00<00:00, 214573.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 136282\n",
            "Test string: 4 Passed! :)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# compare our encoding with the internal encode function\n",
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert enc.encode(test_text) == gpt4_tokenizer.encode(test_text)\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ability to handle special tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 15\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[b'<', b'|', b'endo', b'ft', b'ext', b'|', b'>', b'hello', b' world']"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lets see the default one which we have now\n",
        "test_text = \"<|endoftext|>hello world\"\n",
        "[gpt4_tokenizer.vocab[i] for i in gpt4_tokenizer.encode(test_text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges: 15\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[27, 91, 8862, 728, 428, 91, 29, 15339, 1917]"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt4_tokenizer.encode(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[100257, 15339, 1917]"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <|endoftext|> is the special token\n",
        "ENDOFTEXT = \"<|endoftext|>\"\n",
        "FIM_PREFIX = \"<|fim_prefix|>\"\n",
        "FIM_MIDDLE = \"<|fim_middle|>\"\n",
        "FIM_SUFFIX = \"<|fim_suffix|>\"\n",
        "ENDOFPROMPT = \"<|endofprompt|>\"\n",
        "SPECIAL_TOKENS = [ENDOFTEXT, FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, ENDOFPROMPT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello <|endoftext|> world!'"
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello ', '<|endoftext|>', ' world!']"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "special_pattern = \"(\" + \"|\".join(re.escape(k) for k in SPECIAL_TOKENS) + \")\"\n",
        "special_chunks = re.split(special_pattern, text)\n",
        "special_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now lets handle this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT4Tokenizer:\n",
        "    def __init__(self, special_token_map={}):\n",
        "        self.tokenizer_path = \"cl100k_base\"\n",
        "        self.enc = tiktoken.get_encoding(self.tokenizer_path)\n",
        "        self._create_vocab_and_merges()\n",
        "        self.special_token_map = special_token_map\n",
        "        self.inv_special_token_map = {v:k for k, v in self.special_token_map.items()}\n",
        "        self.byte_shuffle = {i: self.enc._mergeable_ranks[bytes([i])] for i in range(256)} # does map actual byte 0 to the one in this dict\n",
        "        self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n",
        "\n",
        "    def _get_split_tokens(self, mergeable_ranks, token, max_rank):\n",
        "        # Idea: Get the most optimal split of the token into exactly two pre-existing tokens\n",
        "        parts = [bytes([t]) for t in token]\n",
        "        while True:\n",
        "            min_id, min_rank = None, None # min_id -> the point to split the token, min_rank -> to find the most optimal split\n",
        "            for i, pair in enumerate(zip(parts, parts[1:])):\n",
        "                rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "\n",
        "                if (rank is not None) and (min_rank is None or rank < min_rank):\n",
        "                    min_id = i\n",
        "                    min_rank = rank\n",
        "\n",
        "            if (min_rank is None) or (min_rank >= max_rank): # no split was obtained\n",
        "                break\n",
        "            # else update the tokens\n",
        "            assert min_id is not None\n",
        "            parts = parts[:min_id] + [parts[min_id] + parts[min_id + 1]] + parts[min_id + 2:]\n",
        "\n",
        "        return parts\n",
        "\n",
        "    def _create_vocab_and_merges(self):\n",
        "        self.vocab = {v:k for k, v in self.enc._mergeable_ranks.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        self.merges = {}\n",
        "        for token, idx in self.enc._mergeable_ranks.items():\n",
        "            if len(token) < 2:\n",
        "                continue\n",
        "            parts = self._get_split_tokens(self.enc._mergeable_ranks, token, idx)\n",
        "            assert len(parts) == 2\n",
        "            self.merges[(self.enc._mergeable_ranks[parts[0]], self.enc._mergeable_ranks[parts[1]])] = idx\n",
        "\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def encode_ordinary(self, text):\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "        # now shuffle the byte tokens\n",
        "        shuffled_tokens_list = [[self.byte_shuffle[b] for b in tokens] for tokens in tokens_list]\n",
        "        encoded_tokens_list = []\n",
        "\n",
        "        num_merges = 0\n",
        "        for tokens in shuffled_tokens_list:\n",
        "            while len(tokens) >= 2:\n",
        "                # first get the stats of the bigrams\n",
        "                pair_stats = get_stats(tokens)\n",
        "                # now check if there is a pair which is merged as per our tokenizer\n",
        "                merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "                # check if there is actually a match\n",
        "                if merge_pair not in self.merges:\n",
        "                    break\n",
        "\n",
        "                # now replace with the merges token\n",
        "                tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "                num_merges += 1\n",
        "            encoded_tokens_list.append(tokens)\n",
        "\n",
        "        final_tokens = [item for sublist in encoded_tokens_list for item in sublist]\n",
        "\n",
        "        return final_tokens, num_merges\n",
        "\n",
        "    def encode(self, text, allowed_special=\"none\"):\n",
        "        # first identify if special tokens should be handles or not\n",
        "        # if none, encode ordinary :)\n",
        "        num_merges = 0\n",
        "        if allowed_special == \"none\":\n",
        "            tokenized_text, num_merges = self.encode_ordinary(text)\n",
        "            print(f\"Number of merges = {num_merges}\")\n",
        "            return tokenized_text\n",
        "        elif allowed_special != \"all\":\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        # else first handle special token\n",
        "        SPECIAL_PATTERN = \"(\" + \"|\".join(re.escape(k) for k in self.special_token_map.keys()) + \")\"\n",
        "        special_split_text = re.split(SPECIAL_PATTERN, text)\n",
        "\n",
        "        # next tokenize these individual non special ones and add them to split_text list\n",
        "        final_tokens = []\n",
        "        for t in special_split_text:\n",
        "            if t in self.special_token_map:\n",
        "                final_tokens.append(self.special_token_map[t])\n",
        "            else:\n",
        "                tokenized_text, num_merges_tmp = self.encode_ordinary(t)\n",
        "                final_tokens.extend(tokenized_text)\n",
        "                num_merges += num_merges_tmp\n",
        "\n",
        "        print(f\"Number of merges = {num_merges}\")\n",
        "        return final_tokens\n",
        "        \n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text_split = []\n",
        "        for id in ids:\n",
        "            if id in self.vocab:\n",
        "                enc_text_split.append(self.vocab[id])\n",
        "            elif id in self.inv_special_token_map:\n",
        "                enc_text_split.append(self.inv_special_token_map[id].encode(\"utf-8\"))\n",
        "        enc_text = b\"\".join(t for t in enc_text_split)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt4_tokenizer_special = GPT4Tokenizer(special_token_map={\n",
        "    ENDOFTEXT: 100257,\n",
        "    FIM_PREFIX: 100258,\n",
        "    FIM_MIDDLE: 100259,\n",
        "    FIM_SUFFIX: 100260,\n",
        "    ENDOFPROMPT: 100276,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges = 9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello <|endoftext|> world!'"
            ]
          },
          "execution_count": 347,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt4_tokenizer_special.decode(gpt4_tokenizer_special.encode(\"Hello <|endoftext|> world!\", allowed_special=\"all\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|endoftext|>Hello world this is one document\\n<|endoftext|>And this is another document\\n<|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM\\n<|endoftext|>Last document!!! üëã<|endofprompt|>'"
            ]
          },
          "execution_count": 348,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "specials_string = \"\"\"\n",
        "<|endoftext|>Hello world this is one document\n",
        "<|endoftext|>And this is another document\n",
        "<|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM\n",
        "<|endoftext|>Last document!!! üëã<|endofprompt|>\n",
        "\"\"\".strip()\n",
        "specials_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges = 85\n"
          ]
        }
      ],
      "source": [
        "assert specials_string == gpt4_tokenizer_special.decode(gpt4_tokenizer_special.encode(specials_string, allowed_special=\"all\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges = 85\n"
          ]
        }
      ],
      "source": [
        "# test tiktoke equality\n",
        "assert enc.encode(specials_string, allowed_special=\"all\") == gpt4_tokenizer_special.encode(specials_string, allowed_special=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merges = 0\n",
            "Test string: 0 Passed! :)\n",
            "Number of merges = 0\n",
            "Test string: 1 Passed! :)\n",
            "Number of merges = 32\n",
            "Test string: 2 Passed! :)\n",
            "Number of merges = 445\n",
            "Test string: 3 Passed! :)\n",
            "Number of merges = 136282\n",
            "Test string: 4 Passed! :)\n"
          ]
        }
      ],
      "source": [
        "# test easy string as well :)\n",
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert test_text == gpt4_tokenizer_special.decode(gpt4_tokenizer_special.encode(test_text, allowed_special=\"none\"))\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
