{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUEzC2wrB1B"
      },
      "source": [
        "# MinBPE exercise\n",
        "\n",
        "Implement the tokenizer we describe in the video and exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1bVmC1KZrB1D"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "import regex as re\n",
        "from multiprocessing import Pool\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some basic tests\n",
        "test_strings = [\n",
        "    \"\", # empty string\n",
        "    \"?\", # single character\n",
        "    \"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\", # fun small string\n",
        "    \"\"\"The llama (/Ààl…ëÀêm…ô/; Spanish pronunciation: [Àà éama] or [Àà ùama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\n",
        "\n",
        "Llamas are social animals and live with others as a herd. Their wool is soft and contains only a small amount of lanolin.[2] Llamas can learn simple tasks after a few repetitions. When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5‚Äì8 miles).[3] The name llama (also historically spelled \"lama\" or \"glama\") was adopted by European settlers from native Peruvians.[4]\"\"\",\n",
        "\"FILE:taylorswift.txt\"\n",
        "]\n",
        "\n",
        "def unpack(text):\n",
        "    # we do this because `pytest -v .` prints the arguments to console, and we don't\n",
        "    # want to print the entire contents of the file, it creates a mess. So here we go.\n",
        "    if text.startswith(\"FILE:\"):\n",
        "        taylorswift_file = text[5:]\n",
        "        contents = open(taylorswift_file, \"r\", encoding=\"utf-8\").read()\n",
        "        return contents\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "klw3ZEsk0gYr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZYbpolSrB1F",
        "outputId": "1344961b-5de4-4ba9-91ac-00bcb3ae9799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# read content\n",
        "# source: https://www.kaggle.com/datasets/ukveteran/big-text\n",
        "with open(\"big.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# lets train on 1st million words in this\n",
        "text = text[:1000000]\n",
        "\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "7qMVa3wcrB1G",
        "outputId": "cd2e7536-f248-48b2-f90f-8b75c3fc6ac9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\nby Sir Arthur Conan Doyle\n(#15 in our series by Sir Arthur Conan Doyle)\n\nCopyright laws are changing all over the world. Be sure to check the\ncopyright laws for your country before downloading or redistributing\nthis or any other Project Gutenberg eBook.\n\nThis header should be the first thing seen when viewing this Project\nGutenberg file.  Please do not remove it.  Do not change or edit the\nheader without written permission.\n\nPlease read the \"legal small print,\" and other information about the\neBook and Project Gutenberg at the bottom of this file.  Included is\nimportant information about your specific rights and restrictions in\nhow the file may be used.  You can also find out about how to make a\ndonation to Project Gutenberg, and how to get involved.\n\n\n**Welcome To The World of Free Plain Vanilla Electronic Texts**\n\n**eBooks Readable By Both Humans and By Computers, Since 1971**\n\n*****These eBooks Were Prepared By Thousan"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(text[:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(tokens):\n",
        "    pair_freq_stats = {}\n",
        "    for c1, c2 in zip(tokens, tokens[1:]):\n",
        "        pair_freq_stats[(c1, c2)] = pair_freq_stats.get((c1, c2), 0) + 1\n",
        "    return pair_freq_stats\n",
        "\n",
        "def replace_pair(tokens, pair, pair_idx):\n",
        "    new_tokens, i = [], 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        if (i < len(tokens) - 1) and ((tokens[i], tokens[i + 1]) == pair):\n",
        "            new_tokens.append(pair_idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "\n",
        "    return new_tokens"
      ],
      "metadata": {
        "id": "RdY91f-h4ZK0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqsCpVl2rB1E"
      },
      "source": [
        "## BasicTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1TRSAyAzrB1E"
      },
      "outputs": [],
      "source": [
        "class BasicTokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.vocab_size = 256\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(self.vocab_size)}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        ## Train the BPE tokenizer.\n",
        "        assert vocab_size > self.vocab_size # to ensure we have a larger vocab size\n",
        "        num_merges = vocab_size - self.vocab_size\n",
        "\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        old_token_length = len(tokens)\n",
        "\n",
        "        if verbose:\n",
        "            start = \"Start\"\n",
        "            print(f\"{start:.20s} | No. of tokens: {len(tokens):6d} | Vocab size: {self.vocab_size:5d}\")\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            # first get the stats of the bigrams\n",
        "            pair_stats = get_stats(tokens)\n",
        "            # get the max freq bigram\n",
        "            max_freq_pair = max(pair_stats, key=lambda k: pair_stats.get(k, -float(\"inf\")))\n",
        "            # now lets create new tokens by replacing this pair with this\n",
        "            tokens = replace_pair(tokens, max_freq_pair, self.vocab_size)\n",
        "            # update the running variables\n",
        "            self.merges[max_freq_pair] = self.vocab_size\n",
        "            self.vocab[self.vocab_size] = self.vocab[max_freq_pair[0]] + self.vocab[max_freq_pair[1]]\n",
        "            self.vocab_size += 1\n",
        "\n",
        "            if verbose:\n",
        "                if ((i + 1) % verbose_iters == 0) or (i == num_merges - 1):\n",
        "                    print((f\"Iteration {(i + 1):4d} | No. of tokens: {len(tokens):6d} | Merged pair: {str(max_freq_pair):10s} --> {self.merges[max_freq_pair]:5d}\"))\n",
        "\n",
        "        if verbose:\n",
        "            compression = old_token_length / len(tokens)\n",
        "            print(f\"Compression : {compression:.2f} X\")\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "        while len(tokens)>=2:\n",
        "            # first get the stats of the bigrams\n",
        "            pair_stats = get_stats(text)\n",
        "            # now check if there is a pair which is merged as per our tokenizer\n",
        "            merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "            # check if there is actually a match\n",
        "            if self.merges.get(merge_pair) is None:\n",
        "                break\n",
        "\n",
        "            # now replace with the merges token\n",
        "            tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text = b\"\".join(self.vocab[id] for id in ids)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6_wk9UBrB1G",
        "outputId": "b7b5e24a-fd11-4af3-b29b-3119d7476346"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# check encoded str length\n",
        "len(list(text.encode(\"utf-8\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UgbfAa-rB1G",
        "outputId": "2aa0c6b4-6cc4-4c8b-8ee0-399966edc8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration    1 | No. of tokens: 970416 | Merged pair: (101, 32)  -->   256\n",
            "Iteration    2 | No. of tokens: 948950 | Merged pair: (116, 104) -->   257\n",
            "Iteration    3 | No. of tokens: 932251 | Merged pair: (100, 32)  -->   258\n",
            "Iteration    4 | No. of tokens: 916862 | Merged pair: (115, 32)  -->   259\n",
            "Iteration    5 | No. of tokens: 902588 | Merged pair: (116, 32)  -->   260\n",
            "Iteration    6 | No. of tokens: 888972 | Merged pair: (105, 110) -->   261\n",
            "Iteration    7 | No. of tokens: 875719 | Merged pair: (101, 114) -->   262\n",
            "Iteration    8 | No. of tokens: 863648 | Merged pair: (97, 110)  -->   263\n",
            "Iteration    9 | No. of tokens: 852577 | Merged pair: (44, 32)   -->   264\n",
            "Iteration   10 | No. of tokens: 842084 | Merged pair: (257, 256) -->   265\n",
            "Iteration   11 | No. of tokens: 832073 | Merged pair: (111, 110) -->   266\n",
            "Iteration   12 | No. of tokens: 823657 | Merged pair: (121, 32)  -->   267\n",
            "Iteration   13 | No. of tokens: 815311 | Merged pair: (101, 110) -->   268\n",
            "Iteration   14 | No. of tokens: 807594 | Merged pair: (111, 117) -->   269\n",
            "Iteration   15 | No. of tokens: 800768 | Merged pair: (111, 32)  -->   270\n",
            "Iteration   16 | No. of tokens: 794055 | Merged pair: (102, 32)  -->   271\n",
            "Iteration   17 | No. of tokens: 787461 | Merged pair: (111, 114) -->   272\n",
            "Iteration   18 | No. of tokens: 781025 | Merged pair: (46, 32)   -->   273\n",
            "Iteration   19 | No. of tokens: 774995 | Merged pair: (101, 258) -->   274\n",
            "Iteration   20 | No. of tokens: 769057 | Merged pair: (111, 271) -->   275\n",
            "Iteration   21 | No. of tokens: 763233 | Merged pair: (97, 114)  -->   276\n",
            "Iteration   22 | No. of tokens: 760437 | Merged pair: (32, 32)   -->   277\n",
            "Iteration   23 | No. of tokens: 755223 | Merged pair: (114, 101) -->   278\n",
            "Iteration   24 | No. of tokens: 750106 | Merged pair: (263, 258) -->   279\n",
            "Iteration   25 | No. of tokens: 745244 | Merged pair: (116, 105) -->   280\n",
            "Iteration   26 | No. of tokens: 740522 | Merged pair: (116, 270) -->   281\n",
            "Iteration   27 | No. of tokens: 735960 | Merged pair: (261, 103) -->   282\n",
            "Iteration   28 | No. of tokens: 731486 | Merged pair: (97, 108)  -->   283\n",
            "Iteration   29 | No. of tokens: 727438 | Merged pair: (104, 105) -->   284\n",
            "Iteration   30 | No. of tokens: 723473 | Merged pair: (115, 116) -->   285\n",
            "Iteration   31 | No. of tokens: 719545 | Merged pair: (97, 32)   -->   286\n",
            "Iteration   32 | No. of tokens: 715630 | Merged pair: (104, 97)  -->   287\n",
            "Iteration   33 | No. of tokens: 712077 | Merged pair: (10, 10)   -->   288\n",
            "Iteration   34 | No. of tokens: 708424 | Merged pair: (32, 265)  -->   289\n",
            "Iteration   35 | No. of tokens: 705024 | Merged pair: (97, 259)  -->   290\n",
            "Iteration   36 | No. of tokens: 701696 | Merged pair: (97, 260)  -->   291\n",
            "Iteration   37 | No. of tokens: 698458 | Merged pair: (262, 32)  -->   292\n",
            "Iteration   38 | No. of tokens: 695244 | Merged pair: (101, 115) -->   293\n",
            "Iteration   39 | No. of tokens: 692037 | Merged pair: (111, 109) -->   294\n",
            "Iteration   40 | No. of tokens: 688967 | Merged pair: (282, 32)  -->   295\n",
            "Iteration   41 | No. of tokens: 685988 | Merged pair: (73, 32)   -->   296\n",
            "Iteration   42 | No. of tokens: 683140 | Merged pair: (99, 104)  -->   297\n",
            "Iteration   43 | No. of tokens: 680393 | Merged pair: (111, 108) -->   298\n",
            "Iteration   44 | No. of tokens: 677664 | Merged pair: (261, 32)  -->   299\n",
            "Compression : 1.48 X\n"
          ]
        }
      ],
      "source": [
        "# now lets train for some small iterations\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "basic_tokenizer.train(text=text, vocab_size=300, verbose=True, verbose_iters=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i0zHjwhrB1H",
        "outputId": "d63220c9-3fd5-4576-e127-58aa885a06ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (116, 104): 257,\n",
              " (100, 32): 258,\n",
              " (115, 32): 259,\n",
              " (116, 32): 260,\n",
              " (105, 110): 261,\n",
              " (101, 114): 262,\n",
              " (97, 110): 263,\n",
              " (44, 32): 264,\n",
              " (257, 256): 265,\n",
              " (111, 110): 266,\n",
              " (121, 32): 267,\n",
              " (101, 110): 268,\n",
              " (111, 117): 269,\n",
              " (111, 32): 270,\n",
              " (102, 32): 271,\n",
              " (111, 114): 272,\n",
              " (46, 32): 273,\n",
              " (101, 258): 274,\n",
              " (111, 271): 275,\n",
              " (97, 114): 276,\n",
              " (32, 32): 277,\n",
              " (114, 101): 278,\n",
              " (263, 258): 279,\n",
              " (116, 105): 280,\n",
              " (116, 270): 281,\n",
              " (261, 103): 282,\n",
              " (97, 108): 283,\n",
              " (104, 105): 284,\n",
              " (115, 116): 285,\n",
              " (97, 32): 286,\n",
              " (104, 97): 287,\n",
              " (10, 10): 288,\n",
              " (32, 265): 289,\n",
              " (97, 259): 290,\n",
              " (97, 260): 291,\n",
              " (262, 32): 292,\n",
              " (101, 115): 293,\n",
              " (111, 109): 294,\n",
              " (282, 32): 295,\n",
              " (73, 32): 296,\n",
              " (99, 104): 297,\n",
              " (111, 108): 298,\n",
              " (261, 32): 299}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "basic_tokenizer.merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5qYvf8rB1H",
        "outputId": "a2e64132-8b94-48a6-d73d-608a49401da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration  100 | No. of tokens: 576783 | Merged pair: (117, 112) -->   355\n",
            "Iteration  200 | No. of tokens: 495579 | Merged pair: (266, 103) -->   455\n",
            "Iteration  300 | No. of tokens: 452955 | Merged pair: (109, 105) -->   555\n",
            "Iteration  400 | No. of tokens: 424971 | Merged pair: (119, 423) -->   655\n",
            "Iteration  500 | No. of tokens: 403227 | Merged pair: (99, 326)  -->   755\n",
            "Iteration  600 | No. of tokens: 386082 | Merged pair: (282, 264) -->   855\n",
            "Iteration  700 | No. of tokens: 372312 | Merged pair: (668, 405) -->   955\n",
            "Iteration  800 | No. of tokens: 360795 | Merged pair: (573, 265) -->  1055\n",
            "Iteration  900 | No. of tokens: 350883 | Merged pair: (84, 73)   -->  1155\n",
            "Iteration 1000 | No. of tokens: 342119 | Merged pair: (103, 346) -->  1255\n",
            "Compression : 2.92 X\n"
          ]
        }
      ],
      "source": [
        "# now lets get 1k compressions just like GPT-2\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "basic_tokenizer.train(text=text, vocab_size=1256, verbose=True, verbose_iters=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IJT5JwwKrB1H"
      },
      "outputs": [],
      "source": [
        "# lets visualise the merges\n",
        "with open(\"basic_merges.txt\", \"w\") as file:\n",
        "    sorted_merges = sorted(list(basic_tokenizer.merges.items()), key=lambda x: x[1])\n",
        "    for k, v in sorted_merges:\n",
        "        file.write(f\"[{(basic_tokenizer.vocab[k[0]]).decode('utf-8', errors='replace')}][{(basic_tokenizer.vocab[k[1]]).decode('utf-8', errors='replace')}]  ---->   {v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UvX-VrHtrB1H"
      },
      "outputs": [],
      "source": [
        "for test_string in test_strings:\n",
        "    test_text = unpack(test_string)\n",
        "    assert test_text == basic_tokenizer.decode(basic_tokenizer.encode(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexTokenizer"
      ],
      "metadata": {
        "id": "hngDL7xN3YjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yuLWEPZfrB1I"
      },
      "outputs": [],
      "source": [
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_PATTERN_REGEX = re.compile(GPT4_SPLIT_PATTERN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegexTokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.vocab_size = 256\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(self.vocab_size)}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False, verbose_iters=None):\n",
        "        ## Train the BPE tokenizer.\n",
        "        assert vocab_size > self.vocab_size # to ensure we have a larger vocab size\n",
        "        num_merges = vocab_size - self.vocab_size\n",
        "\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "        old_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "\n",
        "        if verbose:\n",
        "            start = \"Start\"\n",
        "            print(f\"{start:.20s} | No. of tokens: {old_token_length:6d} | Vocab size: {self.vocab_size:5d}\")\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            # itrate over each token list\n",
        "            pair_stats_full = {}\n",
        "            for tokens in tokens_list:\n",
        "                # first get the stats of the bigrams\n",
        "                pair_stats = get_stats(tokens)\n",
        "                # combine\n",
        "                for k, v in pair_stats.items():\n",
        "                    pair_stats_full[k] = pair_stats_full.get(k, 0) + v\n",
        "\n",
        "            # get the max freq bigram\n",
        "            max_freq_pair = max(pair_stats_full, key=lambda k: pair_stats_full.get(k, -float(\"inf\")))\n",
        "            # now lets create new tokens by replacing this pair with this\n",
        "            new_tokens_list = []\n",
        "            for tokens in tokens_list:\n",
        "                new_tokens_list.append(replace_pair(tokens, max_freq_pair, self.vocab_size))\n",
        "            # update the running variables\n",
        "            self.merges[max_freq_pair] = self.vocab_size\n",
        "            self.vocab[self.vocab_size] = self.vocab[max_freq_pair[0]] + self.vocab[max_freq_pair[1]]\n",
        "            self.vocab_size += 1\n",
        "            tokens_list = list(new_tokens_list)\n",
        "\n",
        "            if verbose:\n",
        "                if ((i + 1) % verbose_iters == 0) or (i == num_merges - 1):\n",
        "                    new_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "                    print((f\"Iteration {(i + 1):4d} | No. of tokens: {new_token_length:6d} | Merged pair: {str(max_freq_pair):10s} --> {self.merges[max_freq_pair]:5d}\"))\n",
        "\n",
        "        if verbose:\n",
        "            new_token_length = sum(len(tokens) for tokens in tokens_list)\n",
        "            compression = old_token_length / new_token_length\n",
        "            print(f\"Compression : {compression:.2f} X\")\n",
        "\n",
        "    def _encode_chunk(self, tokens):\n",
        "        while len(tokens) >= 2:\n",
        "            # first get the stats of the bigrams\n",
        "            pair_stats = get_stats(text)\n",
        "            # now check if there is a pair which is merged as per our tokenizer\n",
        "            merge_pair = min(pair_stats, key=lambda k: self.merges.get(k, float(\"inf\"))) # it will check if we get a merge pair candidate, else returns the first element\n",
        "            # check if there is actually a match\n",
        "            if self.merges.get(merge_pair) is None:\n",
        "                break\n",
        "\n",
        "            # now replace with the merges token\n",
        "            tokens = replace_pair(tokens, merge_pair, self.merges[merge_pair])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def _encode_parallel(self, tokens_list, num_processes=None):\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(self._encode_chunk, tokens_list)\n",
        "        return results\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        split_text = GPT4_PATTERN_REGEX.findall(text)\n",
        "\n",
        "        tokens_list = [list(t.encode(\"utf-8\")) for t in split_text]\n",
        "\n",
        "        encoded_tokens_list = self._encode_parallel(tokens_list)\n",
        "\n",
        "        final_tokens = [item for sublist in encoded_tokens_list for item in sublist]\n",
        "\n",
        "        return final_tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        enc_text = b\"\".join(self.vocab[id] for id in ids)\n",
        "        text = enc_text.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ],
      "metadata": {
        "id": "dxvJB8Df3WFU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets train for some small iterations\n",
        "regex_tokenizer = RegexTokenizer()\n",
        "regex_tokenizer.train(text=text, vocab_size=266, verbose=True, verbose_iters=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OsC10xS3WC2",
        "outputId": "09fb08b0-6cfa-4da2-9554-eeefe70c6064"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration    1 | No. of tokens: 975654 | Merged pair: (32, 116)  -->   256\n",
            "Iteration    2 | No. of tokens: 954925 | Merged pair: (104, 101) -->   257\n",
            "Iteration    3 | No. of tokens: 938057 | Merged pair: (32, 97)   -->   258\n",
            "Iteration    4 | No. of tokens: 924441 | Merged pair: (105, 110) -->   259\n",
            "Iteration    5 | No. of tokens: 911626 | Merged pair: (256, 257) -->   260\n",
            "Iteration    6 | No. of tokens: 900809 | Merged pair: (32, 111)  -->   261\n",
            "Iteration    7 | No. of tokens: 890173 | Merged pair: (114, 101) -->   262\n",
            "Iteration    8 | No. of tokens: 879576 | Merged pair: (32, 119)  -->   263\n",
            "Iteration    9 | No. of tokens: 869899 | Merged pair: (32, 115)  -->   264\n",
            "Iteration   10 | No. of tokens: 861074 | Merged pair: (101, 114) -->   265\n",
            "Compression : 1.16 X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets train for 1k iterations\n",
        "regex_tokenizer = RegexTokenizer()\n",
        "regex_tokenizer.train(text=text, vocab_size=1256, verbose=True, verbose_iters=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Yc6RdLDawF",
        "outputId": "6c09a70b-f7b7-4d22-857b-740bebc8c93b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start | No. of tokens: 1000000 | Vocab size:   256\n",
            "Iteration  100 | No. of tokens: 580314 | Merged pair: (97, 109)  -->   355\n",
            "Iteration  200 | No. of tokens: 501016 | Merged pair: (32, 118)  -->   455\n",
            "Iteration  300 | No. of tokens: 458709 | Merged pair: (453, 110) -->   555\n",
            "Iteration  400 | No. of tokens: 430368 | Merged pair: (459, 110) -->   655\n",
            "Iteration  500 | No. of tokens: 410025 | Merged pair: (300, 408) -->   755\n",
            "Iteration  600 | No. of tokens: 394498 | Merged pair: (103, 103) -->   855\n",
            "Iteration  700 | No. of tokens: 382035 | Merged pair: (290, 107) -->   955\n",
            "Iteration  800 | No. of tokens: 371558 | Merged pair: (341, 285) -->  1055\n",
            "Iteration  900 | No. of tokens: 362520 | Merged pair: (735, 1080) -->  1155\n",
            "Iteration 1000 | No. of tokens: 354597 | Merged pair: (109, 98)  -->  1255\n",
            "Compression : 2.82 X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets visualise the merges\n",
        "with open(\"regex_merges.txt\", \"w\") as file:\n",
        "    sorted_merges = sorted(list(regex_tokenizer.merges.items()), key=lambda x: x[1])\n",
        "    for k, v in sorted_merges:\n",
        "        file.write(f\"[{(regex_tokenizer.vocab[k[0]]).decode('utf-8', errors='replace')}][{(regex_tokenizer.vocab[k[1]]).decode('utf-8', errors='replace')}]  ---->   {v}\\n\")"
      ],
      "metadata": {
        "id": "1aGXlDT5DmlM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, test_string in enumerate(test_strings):\n",
        "    test_text = unpack(test_string)\n",
        "    try:\n",
        "        assert test_text == regex_tokenizer.decode(regex_tokenizer.encode(test_text))\n",
        "        print(f\"Test string: {i} Passed! :)\")\n",
        "    except AssertionError:\n",
        "        print(f\"Test string: {i} Failed! :(\")\n",
        "    assert test_text == regex_tokenizer.decode(regex_tokenizer.encode(test_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "6HcQBfdJ3WAB",
        "outputId": "e0d713ea-18f3-43f5-a885-6af5ebef3aec"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test string: 0 Passed! :)\n",
            "Test string: 1 Passed! :)\n",
            "Test string: 2 Passed! :)\n",
            "Test string: 3 Passed! :)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkPoolWorker-19:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-1ddf881a2043>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mregex_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test string: {i} Passed! :)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-ef7ed674ea81>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mencoded_tokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mfinal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_tokens_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-ef7ed674ea81>\u001b[0m in \u001b[0;36m_encode_parallel\u001b[0;34m(self, tokens_list, num_processes)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def your_function(x):\n",
        "    # Your processing logic here\n",
        "    return x * x  # example function\n",
        "\n",
        "def process_list_parallel(input_list, num_processes=None):\n",
        "    with Pool(processes=num_processes) as pool:\n",
        "        results = pool.map(your_function, input_list)\n",
        "    return results\n",
        "\n",
        "# Usage\n",
        "if __name__ == '__main__':\n",
        "    my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    results = process_list_parallel(my_list)\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OWoVoID3V9e",
        "outputId": "6abfc0c7-1187-4f67-fb73-a5ee962d214f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.cpu_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyh2Ed_x3V6W",
        "outputId": "ff3c45c8-5cf6-4657-c19d-c140cacc755c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwmKWKT03V3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ogWro143V0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}