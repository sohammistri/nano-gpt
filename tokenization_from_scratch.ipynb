{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCPkwBDXhz2D"
      },
      "source": [
        "Designed as a walkthrough with this [YouTube video](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkAPaUCXOhvW"
      },
      "source": [
        "# Tokenization :(\n",
        "\n",
        "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
        "\n",
        "- Why can't LLM spell words? **Tokenization**.\n",
        "- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n",
        "- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.\n",
        "- Why is LLM bad at simple arithmetic? **Tokenization**.\n",
        "- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.\n",
        "- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? **Tokenization**.\n",
        "- What is this weird warning I get about a \"trailing whitespace\"? **Tokenization**.\n",
        "- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? **Tokenization**.\n",
        "- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.\n",
        "- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n",
        "- What is the real root of suffering? **Tokenization**.\n",
        "\n",
        "---\n",
        "\n",
        "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
        "\n",
        "Example string:\n",
        "\n",
        "```\n",
        "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
        "\n",
        "127 + 677 = 804\n",
        "1275 + 6773 = 8041\n",
        "\n",
        "Egg.\n",
        "I have an Egg.\n",
        "egg.\n",
        "EGG.\n",
        "\n",
        "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì €ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì¸ ChatGPTì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.\n",
        "\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Much glory awaits someone who can delete the need for tokenization. But meanwhile, let's learn about it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PeU63eDYOhve",
        "outputId": "f36a4831-8b9b-4507-c439-2eefe1262d3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_paQxu7EOhvg",
        "outputId": "c9b75e03-c66b-4353-c36f-49035c85ab1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "len([ord(x)for x in \"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDQBEVPTOhvg",
        "outputId": "d0fb6ee9-b831-4d7d-e9ea-fe8cfcd0d81e",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(list(\"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\".encode(\"utf-8\"))) # utf-8 decreases vocab size but expands the seq lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wm3T6JYOhvg",
        "outputId": "d7f0ef20-ff1a-4493-aa1f-093e4ca42fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
            "length: 533\n",
            "---\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n"
          ]
        }
      ],
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MguvDj7CMptM"
      },
      "source": [
        "## Implement BPE\n",
        "\n",
        "We do Byte Pair Encoding for this text. [BPE article](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
        "\n",
        "The idea is to find the most commonly ocurring bigram in this list and replace that with a new token from our side recursively. This decreases the sequence length but increases vocab size (which will inturn affect our embeddings table and softmax layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qKf0-BTuMptO"
      },
      "outputs": [],
      "source": [
        "def get_stats(byte_list):\n",
        "    freq_dict = {}\n",
        "\n",
        "    for b1, b2 in zip(byte_list, byte_list[1:]):\n",
        "        freq_dict[(b1, b2)] = freq_dict.get((b1, b2), 0) + 1\n",
        "\n",
        "    freq_list = sorted([(v, k) for k, v in freq_dict.items()], reverse=True)\n",
        "    return freq_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO4qZySIMptO",
        "outputId": "7350affe-d26d-42a6-db50-b9b55563ca5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(20, (101, 32)),\n",
              " (15, (240, 159)),\n",
              " (12, (226, 128)),\n",
              " (12, (105, 110)),\n",
              " (10, (115, 32)),\n",
              " (10, (97, 110)),\n",
              " (10, (32, 97)),\n",
              " (9, (32, 116)),\n",
              " (8, (116, 104)),\n",
              " (7, (159, 135)),\n",
              " (7, (159, 133)),\n",
              " (7, (97, 114)),\n",
              " (6, (239, 189)),\n",
              " (6, (140, 240)),\n",
              " (6, (128, 140)),\n",
              " (6, (116, 32)),\n",
              " (6, (114, 32)),\n",
              " (6, (111, 114)),\n",
              " (6, (110, 103)),\n",
              " (6, (110, 100))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "get_stats(tokens)[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfKAi3_IMptP",
        "outputId": "7f2fa4a4-415e-40ba-aed4-1853259ad366"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', ' ')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chr(101), chr(32) # visualise the most common pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HQT1t5HSMptQ"
      },
      "outputs": [],
      "source": [
        "def create_new_token_list(byte_list, new_token_idx_start):\n",
        "    freq_list = get_stats(byte_list)\n",
        "\n",
        "    # most common is always at the beginning\n",
        "    max_freq_pair = freq_list[0][0]\n",
        "\n",
        "    # nothing to do if this does not exist (meaning all pairs have count 1)\n",
        "    if max_freq_pair == 1:\n",
        "        return byte_list, new_token_idx_start, {}\n",
        "\n",
        "    new_token_mapping = {}\n",
        "    for v, k in freq_list:\n",
        "        if v == max_freq_pair:\n",
        "            new_token_mapping[k] = new_token_idx_start\n",
        "            new_token_idx_start += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    new_byte_list = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(byte_list):\n",
        "        if (i < len(byte_list) - 1) and (new_token_mapping.get((byte_list[i], byte_list[i + 1])) is not None):\n",
        "            new_byte_list.append(new_token_mapping[byte_list[i], byte_list[i + 1]])\n",
        "            i += 2\n",
        "        else:\n",
        "            new_byte_list.append(byte_list[i])\n",
        "            i += 1\n",
        "\n",
        "    return new_byte_list, new_token_idx_start, new_token_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5-G8ABaMptQ",
        "outputId": "528c0948-a9b6-4e02-a959-95c48eee4c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration   1 | Old Tokens size:  616 | Old vocab size:  256 | New Tokens size:  596 | | New vocab size:  257\n",
            "Iteration   2 | Old Tokens size:  596 | Old vocab size:  257 | New Tokens size:  581 | | New vocab size:  258\n",
            "Iteration   3 | Old Tokens size:  581 | Old vocab size:  258 | New Tokens size:  557 | | New vocab size:  260\n",
            "Iteration   4 | Old Tokens size:  557 | Old vocab size:  260 | New Tokens size:  537 | | New vocab size:  262\n",
            "Iteration   5 | Old Tokens size:  537 | Old vocab size:  262 | New Tokens size:  529 | | New vocab size:  263\n",
            "Iteration   6 | Old Tokens size:  529 | Old vocab size:  263 | New Tokens size:  508 | | New vocab size:  266\n",
            "Iteration   7 | Old Tokens size:  508 | Old vocab size:  266 | New Tokens size:  472 | | New vocab size:  273\n",
            "Iteration   8 | Old Tokens size:  472 | Old vocab size:  273 | New Tokens size:  466 | | New vocab size:  274\n",
            "Iteration   9 | Old Tokens size:  466 | Old vocab size:  274 | New Tokens size:  446 | | New vocab size:  278\n",
            "Iteration  10 | Old Tokens size:  446 | Old vocab size:  278 | New Tokens size:  416 | | New vocab size:  288\n",
            "Iteration  11 | Old Tokens size:  416 | Old vocab size:  288 | New Tokens size:  412 | | New vocab size:  290\n",
            "Iteration  12 | Old Tokens size:  412 | Old vocab size:  290 | New Tokens size:  408 | | New vocab size:  291\n",
            "Iteration  13 | Old Tokens size:  408 | Old vocab size:  291 | New Tokens size:  376 | | New vocab size:  303\n",
            "Iteration  14 | Old Tokens size:  376 | Old vocab size:  303 | New Tokens size:  303 | | New vocab size:  358\n",
            "Iteration  15 | Old Tokens size:  303 | Old vocab size:  358 | New Tokens size:  295 | | New vocab size:  363\n",
            "Iteration  16 | Old Tokens size:  295 | Old vocab size:  363 | New Tokens size:  293 | | New vocab size:  364\n",
            "Iteration  17 | Old Tokens size:  293 | Old vocab size:  364 | New Tokens size:  293 | | New vocab size:  364\n"
          ]
        }
      ],
      "source": [
        "## now loop this to get the BPE sized token lisr\n",
        "\n",
        "new_token_idx_start = 256\n",
        "bpe_tokens = tokens\n",
        "i = 1\n",
        "token_mapping = {i: i for i in range(256)}\n",
        "while True:\n",
        "    old_vocab_len = len(token_mapping)\n",
        "    bpe_tokens_new, new_token_idx_start, new_token_mapping = create_new_token_list(bpe_tokens, new_token_idx_start)\n",
        "    token_mapping = token_mapping | new_token_mapping\n",
        "    print(f\"Iteration {i:3d} | Old Tokens size: {len(bpe_tokens):4d} | Old vocab size: {old_vocab_len:4d} | New Tokens size: {len(bpe_tokens_new):4d} | | New vocab size: {len(token_mapping):4d}\")\n",
        "\n",
        "    if len(bpe_tokens) == len(bpe_tokens_new):\n",
        "        break\n",
        "    else:\n",
        "        bpe_tokens = bpe_tokens_new\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP0hUTQ6MptR"
      },
      "source": [
        "### Bring everything together\n",
        "\n",
        "Like Karpathy's implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_oB3_YgXMptS"
      },
      "outputs": [],
      "source": [
        "def get_stats(byte_list):\n",
        "    freq_dict = {}\n",
        "\n",
        "    for b1, b2 in zip(byte_list, byte_list[1:]):\n",
        "        freq_dict[(b1, b2)] = freq_dict.get((b1, b2), 0) + 1\n",
        "\n",
        "    return freq_dict\n",
        "\n",
        "def create_new_token_list(byte_list, new_token_idx):\n",
        "    freq_dict = get_stats(byte_list)\n",
        "\n",
        "    max_freq_pair = max(freq_dict, key=freq_dict.get)\n",
        "\n",
        "    new_byte_list = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(byte_list):\n",
        "        if (i < len(byte_list) - 1) and ((byte_list[i], byte_list[i + 1]) == max_freq_pair):\n",
        "            new_byte_list.append(new_token_idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_byte_list.append(byte_list[i])\n",
        "            i += 1\n",
        "\n",
        "    return new_byte_list, {max_freq_pair: new_token_idx}\n",
        "\n",
        "def bpe(tokens_list, orig_vocab_size, desired_vocab_size):\n",
        "    num_compressions = desired_vocab_size - orig_vocab_size\n",
        "    new_tokens_dict = {}\n",
        "    print(f\"Iteration {0:4d} | Sequence length = {len(tokens_list):10d}\")\n",
        "\n",
        "\n",
        "    for i in range(num_compressions):\n",
        "        tokens_list, new_pair_dict = create_new_token_list(tokens_list, orig_vocab_size + i)\n",
        "        new_tokens_dict = new_tokens_dict | new_pair_dict\n",
        "        print(f\"Iteration {(i + 1):4d} | Sequence length = {len(tokens_list):10d} | Pair compressed: {str(new_pair_dict):15s}\")\n",
        "\n",
        "    return tokens_list, new_tokens_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKrp8aOJMptS",
        "outputId": "f718e332-7445-413a-8f09-ddb1b66b565c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24597"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# making the training text longer to have more representative token statistics\n",
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"\"\"A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000â€“U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ğŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍuÌ®Í™mÌºÌ­ÌŸÌ—ÍeÌÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌÌÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌÌœÍ”Ì©ÌªÍœÄ¼ÍÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍÍ pÌ±ÍÌºÄ™Ì²ÍÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍÌ£ÌÌcÌ¨Ì¦Ì±Í”ÍÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌÌ«ÌºÌÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œ\\u200bà¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points\"\"\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MqyQnR4MptT",
        "outputId": "2208b1e1-695e-41f2-b0bd-373ab16f1f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24597"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "old_tokens = list(tokens)\n",
        "len(old_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5C1GhTqMptT",
        "outputId": "6151bcdd-4724-4a84-9670-487bd8c8737f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0 | Sequence length =      24597\n",
            "Iteration    1 | Sequence length =      23951 | Pair compressed: {(101, 32): 256}\n",
            "Iteration    2 | Sequence length =      23505 | Pair compressed: {(105, 110): 257}\n",
            "Iteration    3 | Sequence length =      23081 | Pair compressed: {(115, 32): 258}\n",
            "Iteration    4 | Sequence length =      22744 | Pair compressed: {(116, 104): 259}\n",
            "Iteration    5 | Sequence length =      22450 | Pair compressed: {(101, 114): 260}\n",
            "Iteration    6 | Sequence length =      22160 | Pair compressed: {(99, 111): 261}\n",
            "Iteration    7 | Sequence length =      21875 | Pair compressed: {(116, 32): 262}\n",
            "Iteration    8 | Sequence length =      21621 | Pair compressed: {(226, 128): 263}\n",
            "Iteration    9 | Sequence length =      21378 | Pair compressed: {(44, 32): 264}\n",
            "Iteration   10 | Sequence length =      21149 | Pair compressed: {(97, 110): 265}\n",
            "Iteration   11 | Sequence length =      20935 | Pair compressed: {(111, 114): 266}\n",
            "Iteration   12 | Sequence length =      20722 | Pair compressed: {(100, 32): 267}\n",
            "Iteration   13 | Sequence length =      20541 | Pair compressed: {(97, 114): 268}\n",
            "Iteration   14 | Sequence length =      20367 | Pair compressed: {(101, 110): 269}\n",
            "Iteration   15 | Sequence length =      20201 | Pair compressed: {(257, 103): 270}\n",
            "Iteration   16 | Sequence length =      20036 | Pair compressed: {(261, 100): 271}\n",
            "Iteration   17 | Sequence length =      19882 | Pair compressed: {(121, 32): 272}\n",
            "Iteration   18 | Sequence length =      19728 | Pair compressed: {(46, 32): 273}\n",
            "Iteration   19 | Sequence length =      19582 | Pair compressed: {(97, 108): 274}\n",
            "Iteration   20 | Sequence length =      19438 | Pair compressed: {(259, 256): 275}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19438"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "desired_vocab_size = 300\n",
        "bpe_tokens, new_tokens_dict = bpe(tokens, orig_vocab_size=256, desired_vocab_size=276)\n",
        "len(bpe_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wRQYMQOMptU",
        "outputId": "cf762674-a686-46c6-8848-93019ed24656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compression ratio: 1.27X\n"
          ]
        }
      ],
      "source": [
        "print(f\"Compression ratio: {(len(old_tokens) / len(bpe_tokens)):.2f}X\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gvlcaGfMptU",
        "outputId": "49496a03-f22e-4e6a-b5c6-062a71fdebd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (105, 110): 257,\n",
              " (115, 32): 258,\n",
              " (116, 104): 259,\n",
              " (101, 114): 260,\n",
              " (99, 111): 261,\n",
              " (116, 32): 262,\n",
              " (226, 128): 263,\n",
              " (44, 32): 264,\n",
              " (97, 110): 265,\n",
              " (111, 114): 266,\n",
              " (100, 32): 267,\n",
              " (97, 114): 268,\n",
              " (101, 110): 269,\n",
              " (257, 103): 270,\n",
              " (261, 100): 271,\n",
              " (121, 32): 272,\n",
              " (46, 32): 273,\n",
              " (97, 108): 274,\n",
              " (259, 256): 275}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "new_tokens_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbvB0dxPMptU"
      },
      "source": [
        "### decoding and encoding\n",
        "\n",
        "Decode a list given the `ids` as a list of integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "atElJOGQMptU",
        "outputId": "c9241d95-b497-4599-8a48-094ebca51f17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: b'\\x00',\n",
              " 1: b'\\x01',\n",
              " 2: b'\\x02',\n",
              " 3: b'\\x03',\n",
              " 4: b'\\x04',\n",
              " 5: b'\\x05',\n",
              " 6: b'\\x06',\n",
              " 7: b'\\x07',\n",
              " 8: b'\\x08',\n",
              " 9: b'\\t',\n",
              " 10: b'\\n',\n",
              " 11: b'\\x0b',\n",
              " 12: b'\\x0c',\n",
              " 13: b'\\r',\n",
              " 14: b'\\x0e',\n",
              " 15: b'\\x0f',\n",
              " 16: b'\\x10',\n",
              " 17: b'\\x11',\n",
              " 18: b'\\x12',\n",
              " 19: b'\\x13',\n",
              " 20: b'\\x14',\n",
              " 21: b'\\x15',\n",
              " 22: b'\\x16',\n",
              " 23: b'\\x17',\n",
              " 24: b'\\x18',\n",
              " 25: b'\\x19',\n",
              " 26: b'\\x1a',\n",
              " 27: b'\\x1b',\n",
              " 28: b'\\x1c',\n",
              " 29: b'\\x1d',\n",
              " 30: b'\\x1e',\n",
              " 31: b'\\x1f',\n",
              " 32: b' ',\n",
              " 33: b'!',\n",
              " 34: b'\"',\n",
              " 35: b'#',\n",
              " 36: b'$',\n",
              " 37: b'%',\n",
              " 38: b'&',\n",
              " 39: b\"'\",\n",
              " 40: b'(',\n",
              " 41: b')',\n",
              " 42: b'*',\n",
              " 43: b'+',\n",
              " 44: b',',\n",
              " 45: b'-',\n",
              " 46: b'.',\n",
              " 47: b'/',\n",
              " 48: b'0',\n",
              " 49: b'1',\n",
              " 50: b'2',\n",
              " 51: b'3',\n",
              " 52: b'4',\n",
              " 53: b'5',\n",
              " 54: b'6',\n",
              " 55: b'7',\n",
              " 56: b'8',\n",
              " 57: b'9',\n",
              " 58: b':',\n",
              " 59: b';',\n",
              " 60: b'<',\n",
              " 61: b'=',\n",
              " 62: b'>',\n",
              " 63: b'?',\n",
              " 64: b'@',\n",
              " 65: b'A',\n",
              " 66: b'B',\n",
              " 67: b'C',\n",
              " 68: b'D',\n",
              " 69: b'E',\n",
              " 70: b'F',\n",
              " 71: b'G',\n",
              " 72: b'H',\n",
              " 73: b'I',\n",
              " 74: b'J',\n",
              " 75: b'K',\n",
              " 76: b'L',\n",
              " 77: b'M',\n",
              " 78: b'N',\n",
              " 79: b'O',\n",
              " 80: b'P',\n",
              " 81: b'Q',\n",
              " 82: b'R',\n",
              " 83: b'S',\n",
              " 84: b'T',\n",
              " 85: b'U',\n",
              " 86: b'V',\n",
              " 87: b'W',\n",
              " 88: b'X',\n",
              " 89: b'Y',\n",
              " 90: b'Z',\n",
              " 91: b'[',\n",
              " 92: b'\\\\',\n",
              " 93: b']',\n",
              " 94: b'^',\n",
              " 95: b'_',\n",
              " 96: b'`',\n",
              " 97: b'a',\n",
              " 98: b'b',\n",
              " 99: b'c',\n",
              " 100: b'd',\n",
              " 101: b'e',\n",
              " 102: b'f',\n",
              " 103: b'g',\n",
              " 104: b'h',\n",
              " 105: b'i',\n",
              " 106: b'j',\n",
              " 107: b'k',\n",
              " 108: b'l',\n",
              " 109: b'm',\n",
              " 110: b'n',\n",
              " 111: b'o',\n",
              " 112: b'p',\n",
              " 113: b'q',\n",
              " 114: b'r',\n",
              " 115: b's',\n",
              " 116: b't',\n",
              " 117: b'u',\n",
              " 118: b'v',\n",
              " 119: b'w',\n",
              " 120: b'x',\n",
              " 121: b'y',\n",
              " 122: b'z',\n",
              " 123: b'{',\n",
              " 124: b'|',\n",
              " 125: b'}',\n",
              " 126: b'~',\n",
              " 127: b'\\x7f',\n",
              " 128: b'\\x80',\n",
              " 129: b'\\x81',\n",
              " 130: b'\\x82',\n",
              " 131: b'\\x83',\n",
              " 132: b'\\x84',\n",
              " 133: b'\\x85',\n",
              " 134: b'\\x86',\n",
              " 135: b'\\x87',\n",
              " 136: b'\\x88',\n",
              " 137: b'\\x89',\n",
              " 138: b'\\x8a',\n",
              " 139: b'\\x8b',\n",
              " 140: b'\\x8c',\n",
              " 141: b'\\x8d',\n",
              " 142: b'\\x8e',\n",
              " 143: b'\\x8f',\n",
              " 144: b'\\x90',\n",
              " 145: b'\\x91',\n",
              " 146: b'\\x92',\n",
              " 147: b'\\x93',\n",
              " 148: b'\\x94',\n",
              " 149: b'\\x95',\n",
              " 150: b'\\x96',\n",
              " 151: b'\\x97',\n",
              " 152: b'\\x98',\n",
              " 153: b'\\x99',\n",
              " 154: b'\\x9a',\n",
              " 155: b'\\x9b',\n",
              " 156: b'\\x9c',\n",
              " 157: b'\\x9d',\n",
              " 158: b'\\x9e',\n",
              " 159: b'\\x9f',\n",
              " 160: b'\\xa0',\n",
              " 161: b'\\xa1',\n",
              " 162: b'\\xa2',\n",
              " 163: b'\\xa3',\n",
              " 164: b'\\xa4',\n",
              " 165: b'\\xa5',\n",
              " 166: b'\\xa6',\n",
              " 167: b'\\xa7',\n",
              " 168: b'\\xa8',\n",
              " 169: b'\\xa9',\n",
              " 170: b'\\xaa',\n",
              " 171: b'\\xab',\n",
              " 172: b'\\xac',\n",
              " 173: b'\\xad',\n",
              " 174: b'\\xae',\n",
              " 175: b'\\xaf',\n",
              " 176: b'\\xb0',\n",
              " 177: b'\\xb1',\n",
              " 178: b'\\xb2',\n",
              " 179: b'\\xb3',\n",
              " 180: b'\\xb4',\n",
              " 181: b'\\xb5',\n",
              " 182: b'\\xb6',\n",
              " 183: b'\\xb7',\n",
              " 184: b'\\xb8',\n",
              " 185: b'\\xb9',\n",
              " 186: b'\\xba',\n",
              " 187: b'\\xbb',\n",
              " 188: b'\\xbc',\n",
              " 189: b'\\xbd',\n",
              " 190: b'\\xbe',\n",
              " 191: b'\\xbf',\n",
              " 192: b'\\xc0',\n",
              " 193: b'\\xc1',\n",
              " 194: b'\\xc2',\n",
              " 195: b'\\xc3',\n",
              " 196: b'\\xc4',\n",
              " 197: b'\\xc5',\n",
              " 198: b'\\xc6',\n",
              " 199: b'\\xc7',\n",
              " 200: b'\\xc8',\n",
              " 201: b'\\xc9',\n",
              " 202: b'\\xca',\n",
              " 203: b'\\xcb',\n",
              " 204: b'\\xcc',\n",
              " 205: b'\\xcd',\n",
              " 206: b'\\xce',\n",
              " 207: b'\\xcf',\n",
              " 208: b'\\xd0',\n",
              " 209: b'\\xd1',\n",
              " 210: b'\\xd2',\n",
              " 211: b'\\xd3',\n",
              " 212: b'\\xd4',\n",
              " 213: b'\\xd5',\n",
              " 214: b'\\xd6',\n",
              " 215: b'\\xd7',\n",
              " 216: b'\\xd8',\n",
              " 217: b'\\xd9',\n",
              " 218: b'\\xda',\n",
              " 219: b'\\xdb',\n",
              " 220: b'\\xdc',\n",
              " 221: b'\\xdd',\n",
              " 222: b'\\xde',\n",
              " 223: b'\\xdf',\n",
              " 224: b'\\xe0',\n",
              " 225: b'\\xe1',\n",
              " 226: b'\\xe2',\n",
              " 227: b'\\xe3',\n",
              " 228: b'\\xe4',\n",
              " 229: b'\\xe5',\n",
              " 230: b'\\xe6',\n",
              " 231: b'\\xe7',\n",
              " 232: b'\\xe8',\n",
              " 233: b'\\xe9',\n",
              " 234: b'\\xea',\n",
              " 235: b'\\xeb',\n",
              " 236: b'\\xec',\n",
              " 237: b'\\xed',\n",
              " 238: b'\\xee',\n",
              " 239: b'\\xef',\n",
              " 240: b'\\xf0',\n",
              " 241: b'\\xf1',\n",
              " 242: b'\\xf2',\n",
              " 243: b'\\xf3',\n",
              " 244: b'\\xf4',\n",
              " 245: b'\\xf5',\n",
              " 246: b'\\xf6',\n",
              " 247: b'\\xf7',\n",
              " 248: b'\\xf8',\n",
              " 249: b'\\xf9',\n",
              " 250: b'\\xfa',\n",
              " 251: b'\\xfb',\n",
              " 252: b'\\xfc',\n",
              " 253: b'\\xfd',\n",
              " 254: b'\\xfe',\n",
              " 255: b'\\xff',\n",
              " 256: b'e ',\n",
              " 257: b'in',\n",
              " 258: b's ',\n",
              " 259: b'th',\n",
              " 260: b'er',\n",
              " 261: b'co',\n",
              " 262: b't ',\n",
              " 263: b'\\xe2\\x80',\n",
              " 264: b', ',\n",
              " 265: b'an',\n",
              " 266: b'or',\n",
              " 267: b'd ',\n",
              " 268: b'ar',\n",
              " 269: b'en',\n",
              " 270: b'ing',\n",
              " 271: b'cod',\n",
              " 272: b'y ',\n",
              " 273: b'. ',\n",
              " 274: b'al',\n",
              " 275: b'the '}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "vocab_dict = {idx: bytes([idx]) for idx in range(256)}\n",
        "# now the merges\n",
        "for k, v in new_tokens_dict.items():\n",
        "    vocab_dict[v] = vocab_dict[k[0]] + vocab_dict[k[1]]\n",
        "\n",
        "vocab_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OKmCDGXSMptU"
      },
      "outputs": [],
      "source": [
        "def decode(ids):\n",
        "    byte_array = b''.join(vocab_dict[id] for id in ids)\n",
        "    decoded_str = byte_array.decode('utf-8', errors='replace')\n",
        "    return decoded_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICYWyHRlMptV",
        "outputId": "1cd221e4-e7e3-42d7-991d-6ff56e5dd18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'\\x80'\n"
          ]
        }
      ],
      "source": [
        "print(vocab_dict[128])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwLxYjhGMptV",
        "outputId": "a4922417-2a0c-4f7c-8c03-36272bc459a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode([97])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "txrTwMvoMptW"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "    # given the string words convert to a list of ids\n",
        "    tokens = text.encode(\"utf-8\")\n",
        "    tokens = list(map(int, tokens))\n",
        "\n",
        "    while True:\n",
        "        old_tokens = list(tokens)\n",
        "        i = 0\n",
        "        new_tokens = []\n",
        "        while i < len(old_tokens):\n",
        "            if (i < len(old_tokens) - 1) and (new_tokens_dict.get((old_tokens[i], old_tokens[i+1])) is not None):\n",
        "                new_tokens.append(new_tokens_dict.get((old_tokens[i], old_tokens[i+1])))\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(old_tokens[i])\n",
        "                i += 1\n",
        "\n",
        "        tokens = list(new_tokens)\n",
        "        if len(old_tokens) == len(tokens):\n",
        "            return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2CSLKYAMptX",
        "outputId": "6d54e2d9-f8e0-4bf2-f354-1b3ca49a27ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3429\n",
            "2698\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"The swine flu pandemic sweeping the world might not have happened without a laboratory accident in the 1970s , a new study claims .\n",
        "China 's exports and imports declined in May , but this was offset by better-than-expected growth in urban fixed-asset investment - indicating the world 's third-largest economy is well on the road to recovery .\n",
        "This season Coach Ben Howland has waved that white flag and gone zone .\n",
        "\" After Simon , I can handle Jason , \" Sinitta says .\n",
        "He met Burlington in Italy , and their passion for classical forms can be seen at Chiswick in a grand processional avenue lined with stone urns and sphinxes .\n",
        "In New York , critics referenced a pair of big Johns , Cheever and Updike , in their rave reviews , while here the play seemed a more jagged and spelt-out version of Pinter 's dreamy Old Times , with the triangle rearranged against the distaff side .\n",
        "But the overall business climate is starting to change , as investors eye Africa 's growth potential , and that could lead to more options for expatriates wanting to return home , Patel from HSBC said .\n",
        "During that time , David Cameron and William Hague have repeatedly said that the undertakings were being met .\n",
        "\" Unpredictable \" seems to be the word used most often by experts to describe the outbreak of swine flu , writes Clive Cookson .\n",
        "Mercado has dodged more eliminations than any finalist in \" Idol \" history , so she 's no stranger to danger .\n",
        "And now you know how silly , plastic-surgery addicted Jacko came to his end .\n",
        "\" Golf is all about performing in the majors but I 'm not going to this major and thinking it has to happen , \" Harrington said of his bid for a rare third major win in a row .\n",
        "If you walk down Karl Johans Gate , the main drag of central Oslo , a tree-lined promenade bordered by restaurants , cafes and upscale stores , you 'll eventually find yourself face-to-face with the Royal Palace , the mammoth , cream-colored home of the Norwegian royal family .\n",
        "The title search did not find the lien .\n",
        "As for Ucas points , these are used mainly as a general guide to applicants .\n",
        "The 12-time All-Star was appearing in his first game since admitting he took performance-enhancing drugs while playing for the Texas Rangers from 2001 to 2003 .\n",
        "The company added that it was slashing staff in its North American development division from 70 people to 10 .\n",
        "Fox , 8 p.m.\n",
        "Schools are legally required to arrange full-time , suitable education for pupils excluded for six days or more .\n",
        "The MNF said the three were thought to be involved in the construction and distribution of IEDs in the Baghdad region .\n",
        "Stay in school .\n",
        "He said there was no case to answer following defence submissions at the end of two months of prosecution evidence .\n",
        "The island is home to the Chincoteague National Wildlife Refuge , the Assateague Island National Seashore and Assateague State Park .\n",
        "He would love to get his hands on Sea The Stars .\n",
        "Casa Ferreirinha 's Fernando Nicolau de Almeida created Barca Velha in 1952 , using the port grapes to make the region 's first high-quality table wine and in five decades it has only been made in 15 vintages , the most recent being the 2000 .\n",
        "Foxx , who is best-known for his performances in \" Ray \" and \" Dreamgirls , \" will play Prentice Earl Sanders , one of two black detectives determined to crack a series of racially motivated serial killings in 1973-74 San Francisco , the trade paper reported .\n",
        "\"\"\"\n",
        "print(len(text))\n",
        "bpe_text = encode(text)\n",
        "print(len(bpe_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cTtbzQGaMptX"
      },
      "outputs": [],
      "source": [
        "text2 = decode(bpe_text)\n",
        "assert text == text2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yr1zNTY0MptY"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"If you are a fan of Test cricket, one of those days as an Indian Cricket fan, when waking up at 5 in the morning felt so so rewarding!!!\n",
        "Alarm's set for 4.55 tomorrow with the hope that this core can collectively perform as a team, one final time!\n",
        " 'Do it for Jassi Bhai'\"\"\"\n",
        "assert text == decode(encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PJ8PDdSvMptY",
        "outputId": "3bc7927b-608b-42b8-b7fc-ea8177e3d605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "decode(encode(\"h\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k_3-JBqMptY"
      },
      "source": [
        "### Extra pre-processing done in GPT-2\n",
        "\n",
        "GPT-2 does some extra preprocessing of the text. Essentially, they want to seperate semantics from punctuations, spaces and any such other cases. Eg: `dogs.`, `dogs!` and `dogs?` should not be single tokens. Instead, we want `dogs` ad punctuation seperate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhYqZVaPMptZ",
        "outputId": "4f8c456f-a61f-447b-ec4e-732862958b95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ' world',\n",
              " '1234',\n",
              " ' how',\n",
              " \"'S\",\n",
              " ' the',\n",
              " \"'ll\",\n",
              " ' josh',\n",
              " \"'ve\",\n",
              " '      ',\n",
              " ' been',\n",
              " '???',\n",
              " '     ',\n",
              " ' !',\n",
              " '   ']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import regex as re\n",
        "\n",
        "## Explaination:\n",
        "## - First few are to have a seperate token from '<char> type cases across languages\n",
        "## - Next ' ?\\p{L}+' means optional white space and then characters. Thsi ensures our tokens look like ' you', ' hi' (space followed by letters)\n",
        "## - Same for ' ?\\p{N}+' except for numbers\n",
        "## - Next ' ?[^\\s\\p{L}\\p{N}]+' means any punctuations to be captures (means apart from letters and numbers)\n",
        "## - \\s+(?!\\S) to handle extra white spaces in between\n",
        "## - \\s+ for trailing white spaces\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", flags=re.IGNORECASE)\n",
        "text = \"Hello world1234 how'S the'll josh've       been???      !   \"\n",
        "gpt2pat.findall(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore GPT-2 and GPT-4o tokenizer\n",
        "\n",
        "We will see a difference in white spaces"
      ],
      "metadata": {
        "id": "UzFHcIl3PmV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Q6gbtFMptZ",
        "outputId": "6b24ec1b-1ab5-44fa-c361-4bb4b80033d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1nCUFJayMpta"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "text = \"Hello world1234 how'S the'll josh've       been???      !   \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ-Y1RSUMpta",
        "outputId": "ffbaeb69-c707-4c42-fabd-2fd4f3aed79b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([15496,\n",
              "  995,\n",
              "  1065,\n",
              "  2682,\n",
              "  703,\n",
              "  6,\n",
              "  50,\n",
              "  262,\n",
              "  1183,\n",
              "  474,\n",
              "  3768,\n",
              "  1053,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  587,\n",
              "  28358,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  220,\n",
              "  5145,\n",
              "  220,\n",
              "  220,\n",
              "  220],\n",
              " 29)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc_gpt2 = tiktoken.encoding_for_model(\"gpt-2\")\n",
        "gpt2_enc_text = enc_gpt2.encode(text)\n",
        "gpt2_enc_text, len(gpt2_enc_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZP1iqLlMpta",
        "outputId": "7397090f-609c-4f4b-e582-4d0aea26786d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([9906,\n",
              "  1917,\n",
              "  4513,\n",
              "  19,\n",
              "  1268,\n",
              "  13575,\n",
              "  279,\n",
              "  3358,\n",
              "  503,\n",
              "  9451,\n",
              "  3077,\n",
              "  996,\n",
              "  1027,\n",
              "  34115,\n",
              "  415,\n",
              "  758,\n",
              "  262],\n",
              " 17)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc_gpt4 = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "gpt4_enc_text = enc_gpt4.encode(text)\n",
        "gpt4_enc_text, len(gpt4_enc_text) # will merge whitsepaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cnG79WHMpta",
        "outputId": "8a70eaee-470e-4b80-9a51-c71e31659655"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([13225,\n",
              "  2375,\n",
              "  7633,\n",
              "  19,\n",
              "  1495,\n",
              "  31233,\n",
              "  290,\n",
              "  6090,\n",
              "  441,\n",
              "  12601,\n",
              "  7341,\n",
              "  1699,\n",
              "  1339,\n",
              "  33110,\n",
              "  530,\n",
              "  1073,\n",
              "  271],\n",
              " 17)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc_gpt4o = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "gpt4o_enc_text = enc_gpt4o.encode(text)\n",
        "gpt4o_enc_text, len(gpt4o_enc_text) # will merge whitsepaces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\"\n",
        "gpt2_enc_example = enc_gpt2.encode(example)\n",
        "gpt4_enc_example = enc_gpt4.encode(example)\n",
        "gpt4o_enc_example = enc_gpt4o.encode(example)\n",
        "\n",
        "len(gpt2_enc_example), len(gpt4_enc_example), len(gpt4o_enc_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-_MKGUshZ4B",
        "outputId": "bce9de88-15cf-4033-f222-facca9a16dcf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 72, 72)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the vocab for these tokens\n",
        "import tiktoken\n",
        "\n",
        "# Load the tokenizer for GPT-2\n",
        "gpt2_tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
        "\n",
        "# Load the tokenizer for GPT-4\n",
        "gpt4_tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "# Load the tokenizer for GPT-4o (if supported by the library)\n",
        "gpt4o_tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # Replace with the correct name if different"
      ],
      "metadata": {
        "id": "TIyZlVZRTF4q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access GPT-2 vocabulary\n",
        "gpt2_vocab_size = gpt2_tokenizer.n_vocab\n",
        "print(\"GPT-2 Vocabulary Size:\", gpt2_vocab_size)\n",
        "\n",
        "# Access GPT-4 vocabulary\n",
        "gpt4_vocab_size = gpt4_tokenizer.n_vocab\n",
        "print(\"GPT-4 Vocabulary Size:\", gpt4_vocab_size)\n",
        "\n",
        "# Access GPT-4o vocabulary (if available)\n",
        "gpt4o_vocab_size = gpt4o_tokenizer.n_vocab\n",
        "print(\"GPT-4o Vocabulary Size:\", gpt4o_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHfu7Hfea6O6",
        "outputId": "043ec16f-4b07-4c8d-a4d7-61fbd7598ae2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Vocabulary Size: 50257\n",
            "GPT-4 Vocabulary Size: 100277\n",
            "GPT-4o Vocabulary Size: 200019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_vocab = {token_id: gpt2_tokenizer.decode([token_id]) for token_id in range(gpt2_vocab_size)}\n",
        "\n",
        "import random\n",
        "random.seed(42 + 1)\n",
        "# Print the first 10 tokens and their IDs\n",
        "print(\"Random 10 tokens in GPT-2 vocabulary:\")\n",
        "for _ in range(20):\n",
        "    idx = random.randint(0, gpt2_vocab_size)\n",
        "    print(f\"Token ID: {idx}, Token: {repr(gpt2_vocab[idx])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkm9Bi4NbV1s",
        "outputId": "a11ae2b6-8e2c-47cc-8290-2e726267ca8a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random 10 tokens in GPT-2 vocabulary:\n",
            "Token ID: 2526, Token: ' risk'\n",
            "Token ID: 18748, Token: 'ilitary'\n",
            "Token ID: 45627, Token: ' rationality'\n",
            "Token ID: 49983, Token: ' Fundamental'\n",
            "Token ID: 9432, Token: ' objective'\n",
            "Token ID: 30312, Token: 'ermanent'\n",
            "Token ID: 24240, Token: ' tomato'\n",
            "Token ID: 44017, Token: ' pend'\n",
            "Token ID: 45772, Token: ' Byzantine'\n",
            "Token ID: 6310, Token: 'Inst'\n",
            "Token ID: 29700, Token: 'inders'\n",
            "Token ID: 39243, Token: ' sensed'\n",
            "Token ID: 32654, Token: ' secretive'\n",
            "Token ID: 39910, Token: ' horrend'\n",
            "Token ID: 1255, Token: ' result'\n",
            "Token ID: 33690, Token: '510'\n",
            "Token ID: 28337, Token: 'ortality'\n",
            "Token ID: 37789, Token: ' mailed'\n",
            "Token ID: 24403, Token: '227'\n",
            "Token ID: 40927, Token: 'tile'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o_vocab = {}\n",
        "\n",
        "for token_id in range(gpt4o_vocab_size):\n",
        "  try:\n",
        "    gpt4o_vocab[token_id] = gpt4o_tokenizer.decode([token_id])\n",
        "  except:\n",
        "    print(token_id)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "# Print the first 10 tokens and their IDs\n",
        "print(\"Random 10 tokens in GPT-4o vocabulary:\")\n",
        "for _ in range(20):\n",
        "    idx = random.randint(0, gpt4o_vocab_size)\n",
        "    print(f\"Token ID: {idx}, Token: {repr(gpt4o_vocab[idx])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKSipF1KcIPp",
        "outputId": "a847286e-9fb6-4dcd-8ebf-09493b7d5da5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199998\n",
            "200000\n",
            "200001\n",
            "200002\n",
            "200003\n",
            "200004\n",
            "200005\n",
            "200006\n",
            "200007\n",
            "200008\n",
            "200009\n",
            "200010\n",
            "200011\n",
            "200012\n",
            "200013\n",
            "200014\n",
            "200015\n",
            "200016\n",
            "200017\n",
            "Random 10 tokens in GPT-4o vocabulary:\n",
            "Token ID: 167621, Token: ' extracurricular'\n",
            "Token ID: 29184, Token: '(boolean'\n",
            "Token ID: 6556, Token: ' ger'\n",
            "Token ID: 194393, Token: ' Ğ·Ğ°ÑÑ‚Ğ¾Ñ'\n",
            "Token ID: 72097, Token: ' autism'\n",
            "Token ID: 64196, Token: ' relevance'\n",
            "Token ID: 58513, Token: ' directamente'\n",
            "Token ID: 36579, Token: 'Ğ´Ğ²ÑÑ€'\n",
            "Token ID: 193061, Token: ' QUICK'\n",
            "Token ID: 26868, Token: 'chem'\n",
            "Token ID: 177392, Token: ' deram'\n",
            "Token ID: 194161, Token: ' ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚'\n",
            "Token ID: 142964, Token: ' Bahnhof'\n",
            "Token ID: 22790, Token: 'falls'\n",
            "Token ID: 154794, Token: '-Cal'\n",
            "Token ID: 110604, Token: 'Shipment'\n",
            "Token ID: 8331, Token: '(res'\n",
            "Token ID: 7811, Token: 'ï¿½'\n",
            "Token ID: 24561, Token: 'eed'\n",
            "Token ID: 57314, Token: 'Kon'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some long tokens from here\n",
        "sorted(list(gpt4o_vocab.values()), key=len, reverse=True)[200:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjAmNsaK5L_Y",
        "outputId": "c9209db6-b68a-423e-ec10-36c8ea8e0ccd"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' à¸ªà¸³à¸™à¸±à¸à¹€à¸¥à¸‚à¸²à¸™à¸¸à¸à¸²à¸£à¸­à¸‡à¸„à¹Œà¸à¸£',\n",
              " '\\t\\t                   ',\n",
              " '                    ',\n",
              " '\\t                   ',\n",
              " '                   \\n',\n",
              " '        \\r\\n        \\r\\n',\n",
              " '    \\n    \\n    \\n    \\n',\n",
              " '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              " '                   ',\n",
              " '                  \\n',\n",
              " ' telecommunications',\n",
              " ' //////////////////',\n",
              " ' __________________',\n",
              " ' selbstverstÃ¤ndlich',\n",
              " ' à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸šà¸­à¸¥à¸§à¸±à¸™à¸™à¸µà¹‰',\n",
              " ' //----------------',\n",
              " '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              " '.onreadystatechange',\n",
              " '__________________\\n',\n",
              " ' significativamente',\n",
              " ' Telecommunications',\n",
              " ' Wahrscheinlichkeit',\n",
              " ' disproportionately',\n",
              " '                  ',\n",
              " '        \\n        \\n',\n",
              " ' =================',\n",
              " '                \\r\\n',\n",
              " '\\n                \\n',\n",
              " ' unterschiedlichen',\n",
              " '                 \\n',\n",
              " ' interdisciplinary',\n",
              " '(\"----------------',\n",
              " '.githubusercontent',\n",
              " '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              " ' responsabilidades',\n",
              " ' Herausforderungen',\n",
              " ' multidisciplinary',\n",
              " ' STDMETHODCALLTYPE',\n",
              " ' _________________',\n",
              " '                \\n\\n',\n",
              " '(\"================',\n",
              " ' commercialization',\n",
              " '\\t\\t\\t               ',\n",
              " '                 ',\n",
              " '                \\n',\n",
              " ' ----------------',\n",
              " ' responsibilities',\n",
              " ' particuliÃ¨rement',\n",
              " ' cryptocurrencies',\n",
              " ' responsabilidade',\n",
              " ' verantwoordelijk',\n",
              " ' htmlspecialchars',\n",
              " ' ****************',\n",
              " ' ################',\n",
              " ' characterization',\n",
              " ' caractÃ©ristiques',\n",
              " ' entrepreneurship',\n",
              " ' unterschiedliche',\n",
              " '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              " ' Responsibilities',\n",
              " ' gastrointestinal',\n",
              " '\\t\\t               ',\n",
              " ' Dienstleistungen',\n",
              " ' correspondientes',\n",
              " '                \\t',\n",
              " ' professionnelles',\n",
              " ' misunderstanding',\n",
              " ' à¹à¸‚à¸§à¸‡à¸„à¸¥à¸­à¸‡à¹€à¸•à¸¢à¹€à¸«à¸™à¸·à¸­',\n",
              " ' persoonsgegevens',\n",
              " ' à¸™à¸±à¸à¸¥à¸‡à¸—à¸¸à¸™à¸ªà¸±à¸¡à¸à¸±à¸™à¸˜à¹Œ',\n",
              " ' maatschappelijke',\n",
              " ' unconstitutional',\n",
              " ' Entrepreneurship',\n",
              " ' establecimientos',\n",
              " 'à¹€à¸›à¸´à¸”à¸­à¸ à¸´à¸›à¸£à¸²à¸¢à¸—à¸±à¹ˆà¸§à¹„à¸›',\n",
              " ' Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾',\n",
              " '\\t                ',\n",
              " ' sustentabilidade',\n",
              " ' transformational',\n",
              " ' VerÃ¶ffentlichung',\n",
              " ' Namminersorlutik',\n",
              " ' à®¤à¯†à®°à®¿à®µà®¿à®¤à¯à®¤à¯à®³à¯à®³à®¾à®°à¯',\n",
              " ' ................',\n",
              " ' interoperability',\n",
              " ' Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°',\n",
              " '                ',\n",
              " '----------------',\n",
              " '****************',\n",
              " '================',\n",
              " '////////////////',\n",
              " '.springframework',\n",
              " '################',\n",
              " '________________',\n",
              " '................',\n",
              " ' MERCHANTABILITY',\n",
              " ' recommendations',\n",
              " ' characteristics',\n",
              " ' straightforward',\n",
              " ' representatives',\n",
              " '%%%%%%%%%%%%%%%%']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(gpt4o_vocab.values()).index(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YEs38UX-sat",
        "outputId": "122784a5-583b-4220-89dd-2dcf4a83a7c1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the `encoder.json` and `vocab.bpe` files"
      ],
      "metadata": {
        "id": "iX64-2k1gkuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYHCB5IOgkNf",
        "outputId": "18113b67-785a-43d1-8770-c990ee2e1417"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 08:55:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: â€˜vocab.bpeâ€™\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K  1.23MB/s    in 0.4s    \n",
            "\n",
            "2025-01-04 08:55:46 (1.23 MB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
            "\n",
            "--2025-01-04 08:55:46--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: â€˜encoder.jsonâ€™\n",
            "\n",
            "encoder.json        100%[===================>]   1018K  1.89MB/s    in 0.5s    \n",
            "\n",
            "2025-01-04 08:55:47 (1.89 MB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "with open('encoder.json', 'r') as f:\n",
        "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
        "\n",
        "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "    bpe_data = f.read()"
      ],
      "metadata": {
        "id": "jXUCHxWYgkKx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder # maps string to token id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0qo0hBY9h5ms",
        "outputId": "3d25b899-3b83-4f8b-8e61-c60c43fabee6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '\"': 1,\n",
              " '#': 2,\n",
              " '$': 3,\n",
              " '%': 4,\n",
              " '&': 5,\n",
              " \"'\": 6,\n",
              " '(': 7,\n",
              " ')': 8,\n",
              " '*': 9,\n",
              " '+': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '1': 16,\n",
              " '2': 17,\n",
              " '3': 18,\n",
              " '4': 19,\n",
              " '5': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " ':': 25,\n",
              " ';': 26,\n",
              " '<': 27,\n",
              " '=': 28,\n",
              " '>': 29,\n",
              " '?': 30,\n",
              " '@': 31,\n",
              " 'A': 32,\n",
              " 'B': 33,\n",
              " 'C': 34,\n",
              " 'D': 35,\n",
              " 'E': 36,\n",
              " 'F': 37,\n",
              " 'G': 38,\n",
              " 'H': 39,\n",
              " 'I': 40,\n",
              " 'J': 41,\n",
              " 'K': 42,\n",
              " 'L': 43,\n",
              " 'M': 44,\n",
              " 'N': 45,\n",
              " 'O': 46,\n",
              " 'P': 47,\n",
              " 'Q': 48,\n",
              " 'R': 49,\n",
              " 'S': 50,\n",
              " 'T': 51,\n",
              " 'U': 52,\n",
              " 'V': 53,\n",
              " 'W': 54,\n",
              " 'X': 55,\n",
              " 'Y': 56,\n",
              " 'Z': 57,\n",
              " '[': 58,\n",
              " '\\\\': 59,\n",
              " ']': 60,\n",
              " '^': 61,\n",
              " '_': 62,\n",
              " '`': 63,\n",
              " 'a': 64,\n",
              " 'b': 65,\n",
              " 'c': 66,\n",
              " 'd': 67,\n",
              " 'e': 68,\n",
              " 'f': 69,\n",
              " 'g': 70,\n",
              " 'h': 71,\n",
              " 'i': 72,\n",
              " 'j': 73,\n",
              " 'k': 74,\n",
              " 'l': 75,\n",
              " 'm': 76,\n",
              " 'n': 77,\n",
              " 'o': 78,\n",
              " 'p': 79,\n",
              " 'q': 80,\n",
              " 'r': 81,\n",
              " 's': 82,\n",
              " 't': 83,\n",
              " 'u': 84,\n",
              " 'v': 85,\n",
              " 'w': 86,\n",
              " 'x': 87,\n",
              " 'y': 88,\n",
              " 'z': 89,\n",
              " '{': 90,\n",
              " '|': 91,\n",
              " '}': 92,\n",
              " '~': 93,\n",
              " 'Â¡': 94,\n",
              " 'Â¢': 95,\n",
              " 'Â£': 96,\n",
              " 'Â¤': 97,\n",
              " 'Â¥': 98,\n",
              " 'Â¦': 99,\n",
              " 'Â§': 100,\n",
              " 'Â¨': 101,\n",
              " 'Â©': 102,\n",
              " 'Âª': 103,\n",
              " 'Â«': 104,\n",
              " 'Â¬': 105,\n",
              " 'Â®': 106,\n",
              " 'Â¯': 107,\n",
              " 'Â°': 108,\n",
              " 'Â±': 109,\n",
              " 'Â²': 110,\n",
              " 'Â³': 111,\n",
              " 'Â´': 112,\n",
              " 'Âµ': 113,\n",
              " 'Â¶': 114,\n",
              " 'Â·': 115,\n",
              " 'Â¸': 116,\n",
              " 'Â¹': 117,\n",
              " 'Âº': 118,\n",
              " 'Â»': 119,\n",
              " 'Â¼': 120,\n",
              " 'Â½': 121,\n",
              " 'Â¾': 122,\n",
              " 'Â¿': 123,\n",
              " 'Ã€': 124,\n",
              " 'Ã': 125,\n",
              " 'Ã‚': 126,\n",
              " 'Ãƒ': 127,\n",
              " 'Ã„': 128,\n",
              " 'Ã…': 129,\n",
              " 'Ã†': 130,\n",
              " 'Ã‡': 131,\n",
              " 'Ãˆ': 132,\n",
              " 'Ã‰': 133,\n",
              " 'ÃŠ': 134,\n",
              " 'Ã‹': 135,\n",
              " 'ÃŒ': 136,\n",
              " 'Ã': 137,\n",
              " 'Ã': 138,\n",
              " 'Ã': 139,\n",
              " 'Ã': 140,\n",
              " 'Ã‘': 141,\n",
              " 'Ã’': 142,\n",
              " 'Ã“': 143,\n",
              " 'Ã”': 144,\n",
              " 'Ã•': 145,\n",
              " 'Ã–': 146,\n",
              " 'Ã—': 147,\n",
              " 'Ã˜': 148,\n",
              " 'Ã™': 149,\n",
              " 'Ãš': 150,\n",
              " 'Ã›': 151,\n",
              " 'Ãœ': 152,\n",
              " 'Ã': 153,\n",
              " 'Ã': 154,\n",
              " 'ÃŸ': 155,\n",
              " 'Ã ': 156,\n",
              " 'Ã¡': 157,\n",
              " 'Ã¢': 158,\n",
              " 'Ã£': 159,\n",
              " 'Ã¤': 160,\n",
              " 'Ã¥': 161,\n",
              " 'Ã¦': 162,\n",
              " 'Ã§': 163,\n",
              " 'Ã¨': 164,\n",
              " 'Ã©': 165,\n",
              " 'Ãª': 166,\n",
              " 'Ã«': 167,\n",
              " 'Ã¬': 168,\n",
              " 'Ã­': 169,\n",
              " 'Ã®': 170,\n",
              " 'Ã¯': 171,\n",
              " 'Ã°': 172,\n",
              " 'Ã±': 173,\n",
              " 'Ã²': 174,\n",
              " 'Ã³': 175,\n",
              " 'Ã´': 176,\n",
              " 'Ãµ': 177,\n",
              " 'Ã¶': 178,\n",
              " 'Ã·': 179,\n",
              " 'Ã¸': 180,\n",
              " 'Ã¹': 181,\n",
              " 'Ãº': 182,\n",
              " 'Ã»': 183,\n",
              " 'Ã¼': 184,\n",
              " 'Ã½': 185,\n",
              " 'Ã¾': 186,\n",
              " 'Ã¿': 187,\n",
              " 'Ä€': 188,\n",
              " 'Ä': 189,\n",
              " 'Ä‚': 190,\n",
              " 'Äƒ': 191,\n",
              " 'Ä„': 192,\n",
              " 'Ä…': 193,\n",
              " 'Ä†': 194,\n",
              " 'Ä‡': 195,\n",
              " 'Äˆ': 196,\n",
              " 'Ä‰': 197,\n",
              " 'ÄŠ': 198,\n",
              " 'Ä‹': 199,\n",
              " 'ÄŒ': 200,\n",
              " 'Ä': 201,\n",
              " 'Ä': 202,\n",
              " 'Ä': 203,\n",
              " 'Ä': 204,\n",
              " 'Ä‘': 205,\n",
              " 'Ä’': 206,\n",
              " 'Ä“': 207,\n",
              " 'Ä”': 208,\n",
              " 'Ä•': 209,\n",
              " 'Ä–': 210,\n",
              " 'Ä—': 211,\n",
              " 'Ä˜': 212,\n",
              " 'Ä™': 213,\n",
              " 'Äš': 214,\n",
              " 'Ä›': 215,\n",
              " 'Äœ': 216,\n",
              " 'Ä': 217,\n",
              " 'Ä': 218,\n",
              " 'ÄŸ': 219,\n",
              " 'Ä ': 220,\n",
              " 'Ä¡': 221,\n",
              " 'Ä¢': 222,\n",
              " 'Ä£': 223,\n",
              " 'Ä¤': 224,\n",
              " 'Ä¥': 225,\n",
              " 'Ä¦': 226,\n",
              " 'Ä§': 227,\n",
              " 'Ä¨': 228,\n",
              " 'Ä©': 229,\n",
              " 'Äª': 230,\n",
              " 'Ä«': 231,\n",
              " 'Ä¬': 232,\n",
              " 'Ä­': 233,\n",
              " 'Ä®': 234,\n",
              " 'Ä¯': 235,\n",
              " 'Ä°': 236,\n",
              " 'Ä±': 237,\n",
              " 'Ä²': 238,\n",
              " 'Ä³': 239,\n",
              " 'Ä´': 240,\n",
              " 'Äµ': 241,\n",
              " 'Ä¶': 242,\n",
              " 'Ä·': 243,\n",
              " 'Ä¸': 244,\n",
              " 'Ä¹': 245,\n",
              " 'Äº': 246,\n",
              " 'Ä»': 247,\n",
              " 'Ä¼': 248,\n",
              " 'Ä½': 249,\n",
              " 'Ä¾': 250,\n",
              " 'Ä¿': 251,\n",
              " 'Å€': 252,\n",
              " 'Å': 253,\n",
              " 'Å‚': 254,\n",
              " 'Åƒ': 255,\n",
              " 'Ä t': 256,\n",
              " 'Ä a': 257,\n",
              " 'he': 258,\n",
              " 'in': 259,\n",
              " 're': 260,\n",
              " 'on': 261,\n",
              " 'Ä the': 262,\n",
              " 'er': 263,\n",
              " 'Ä s': 264,\n",
              " 'at': 265,\n",
              " 'Ä w': 266,\n",
              " 'Ä o': 267,\n",
              " 'en': 268,\n",
              " 'Ä c': 269,\n",
              " 'it': 270,\n",
              " 'is': 271,\n",
              " 'an': 272,\n",
              " 'or': 273,\n",
              " 'es': 274,\n",
              " 'Ä b': 275,\n",
              " 'ed': 276,\n",
              " 'Ä f': 277,\n",
              " 'ing': 278,\n",
              " 'Ä p': 279,\n",
              " 'ou': 280,\n",
              " 'Ä an': 281,\n",
              " 'al': 282,\n",
              " 'ar': 283,\n",
              " 'Ä to': 284,\n",
              " 'Ä m': 285,\n",
              " 'Ä of': 286,\n",
              " 'Ä in': 287,\n",
              " 'Ä d': 288,\n",
              " 'Ä h': 289,\n",
              " 'Ä and': 290,\n",
              " 'ic': 291,\n",
              " 'as': 292,\n",
              " 'le': 293,\n",
              " 'Ä th': 294,\n",
              " 'ion': 295,\n",
              " 'om': 296,\n",
              " 'll': 297,\n",
              " 'ent': 298,\n",
              " 'Ä n': 299,\n",
              " 'Ä l': 300,\n",
              " 'st': 301,\n",
              " 'Ä re': 302,\n",
              " 've': 303,\n",
              " 'Ä e': 304,\n",
              " 'ro': 305,\n",
              " 'ly': 306,\n",
              " 'Ä be': 307,\n",
              " 'Ä g': 308,\n",
              " 'Ä T': 309,\n",
              " 'ct': 310,\n",
              " 'Ä S': 311,\n",
              " 'id': 312,\n",
              " 'ot': 313,\n",
              " 'Ä I': 314,\n",
              " 'ut': 315,\n",
              " 'et': 316,\n",
              " 'Ä A': 317,\n",
              " 'Ä is': 318,\n",
              " 'Ä on': 319,\n",
              " 'im': 320,\n",
              " 'am': 321,\n",
              " 'ow': 322,\n",
              " 'ay': 323,\n",
              " 'ad': 324,\n",
              " 'se': 325,\n",
              " 'Ä that': 326,\n",
              " 'Ä C': 327,\n",
              " 'ig': 328,\n",
              " 'Ä for': 329,\n",
              " 'ac': 330,\n",
              " 'Ä y': 331,\n",
              " 'ver': 332,\n",
              " 'ur': 333,\n",
              " 'Ä u': 334,\n",
              " 'ld': 335,\n",
              " 'Ä st': 336,\n",
              " 'Ä M': 337,\n",
              " \"'s\": 338,\n",
              " 'Ä he': 339,\n",
              " 'Ä it': 340,\n",
              " 'ation': 341,\n",
              " 'ith': 342,\n",
              " 'ir': 343,\n",
              " 'ce': 344,\n",
              " 'Ä you': 345,\n",
              " 'il': 346,\n",
              " 'Ä B': 347,\n",
              " 'Ä wh': 348,\n",
              " 'ol': 349,\n",
              " 'Ä P': 350,\n",
              " 'Ä with': 351,\n",
              " 'Ä 1': 352,\n",
              " 'ter': 353,\n",
              " 'ch': 354,\n",
              " 'Ä as': 355,\n",
              " 'Ä we': 356,\n",
              " 'Ä (': 357,\n",
              " 'nd': 358,\n",
              " 'ill': 359,\n",
              " 'Ä D': 360,\n",
              " 'if': 361,\n",
              " 'Ä 2': 362,\n",
              " 'ag': 363,\n",
              " 'ers': 364,\n",
              " 'ke': 365,\n",
              " 'Ä \"': 366,\n",
              " 'Ä H': 367,\n",
              " 'em': 368,\n",
              " 'Ä con': 369,\n",
              " 'Ä W': 370,\n",
              " 'Ä R': 371,\n",
              " 'her': 372,\n",
              " 'Ä was': 373,\n",
              " 'Ä r': 374,\n",
              " 'od': 375,\n",
              " 'Ä F': 376,\n",
              " 'ul': 377,\n",
              " 'ate': 378,\n",
              " 'Ä at': 379,\n",
              " 'ri': 380,\n",
              " 'pp': 381,\n",
              " 'ore': 382,\n",
              " 'Ä The': 383,\n",
              " 'Ä se': 384,\n",
              " 'us': 385,\n",
              " 'Ä pro': 386,\n",
              " 'Ä ha': 387,\n",
              " 'um': 388,\n",
              " 'Ä are': 389,\n",
              " 'Ä de': 390,\n",
              " 'ain': 391,\n",
              " 'and': 392,\n",
              " 'Ä or': 393,\n",
              " 'igh': 394,\n",
              " 'est': 395,\n",
              " 'ist': 396,\n",
              " 'ab': 397,\n",
              " 'rom': 398,\n",
              " 'Ä N': 399,\n",
              " 'th': 400,\n",
              " 'Ä com': 401,\n",
              " 'Ä G': 402,\n",
              " 'un': 403,\n",
              " 'op': 404,\n",
              " '00': 405,\n",
              " 'Ä L': 406,\n",
              " 'Ä not': 407,\n",
              " 'ess': 408,\n",
              " 'Ä ex': 409,\n",
              " 'Ä v': 410,\n",
              " 'res': 411,\n",
              " 'Ä E': 412,\n",
              " 'ew': 413,\n",
              " 'ity': 414,\n",
              " 'ant': 415,\n",
              " 'Ä by': 416,\n",
              " 'el': 417,\n",
              " 'os': 418,\n",
              " 'ort': 419,\n",
              " 'oc': 420,\n",
              " 'qu': 421,\n",
              " 'Ä from': 422,\n",
              " 'Ä have': 423,\n",
              " 'Ä su': 424,\n",
              " 'ive': 425,\n",
              " 'ould': 426,\n",
              " 'Ä sh': 427,\n",
              " 'Ä this': 428,\n",
              " 'nt': 429,\n",
              " 'ra': 430,\n",
              " 'pe': 431,\n",
              " 'ight': 432,\n",
              " 'art': 433,\n",
              " 'ment': 434,\n",
              " 'Ä al': 435,\n",
              " 'ust': 436,\n",
              " 'end': 437,\n",
              " '--': 438,\n",
              " 'all': 439,\n",
              " 'Ä O': 440,\n",
              " 'ack': 441,\n",
              " 'Ä ch': 442,\n",
              " 'Ä le': 443,\n",
              " 'ies': 444,\n",
              " 'red': 445,\n",
              " 'ard': 446,\n",
              " 'Ã¢Ä¢': 447,\n",
              " 'out': 448,\n",
              " 'Ä J': 449,\n",
              " 'Ä ab': 450,\n",
              " 'ear': 451,\n",
              " 'iv': 452,\n",
              " 'ally': 453,\n",
              " 'our': 454,\n",
              " 'ost': 455,\n",
              " 'gh': 456,\n",
              " 'pt': 457,\n",
              " 'Ä pl': 458,\n",
              " 'ast': 459,\n",
              " 'Ä can': 460,\n",
              " 'ak': 461,\n",
              " 'ome': 462,\n",
              " 'ud': 463,\n",
              " 'The': 464,\n",
              " 'Ä his': 465,\n",
              " 'Ä do': 466,\n",
              " 'Ä go': 467,\n",
              " 'Ä has': 468,\n",
              " 'ge': 469,\n",
              " \"'t\": 470,\n",
              " 'Ä U': 471,\n",
              " 'rou': 472,\n",
              " 'Ä sa': 473,\n",
              " 'Ä j': 474,\n",
              " 'Ä but': 475,\n",
              " 'Ä wor': 476,\n",
              " 'Ä all': 477,\n",
              " 'ect': 478,\n",
              " 'Ä k': 479,\n",
              " 'ame': 480,\n",
              " 'Ä will': 481,\n",
              " 'ok': 482,\n",
              " 'Ä whe': 483,\n",
              " 'Ä they': 484,\n",
              " 'ide': 485,\n",
              " '01': 486,\n",
              " 'ff': 487,\n",
              " 'ich': 488,\n",
              " 'pl': 489,\n",
              " 'ther': 490,\n",
              " 'Ä tr': 491,\n",
              " '..': 492,\n",
              " 'Ä int': 493,\n",
              " 'ie': 494,\n",
              " 'ure': 495,\n",
              " 'age': 496,\n",
              " 'Ä ne': 497,\n",
              " 'ial': 498,\n",
              " 'ap': 499,\n",
              " 'ine': 500,\n",
              " 'ice': 501,\n",
              " 'Ä me': 502,\n",
              " 'Ä out': 503,\n",
              " 'ans': 504,\n",
              " 'one': 505,\n",
              " 'ong': 506,\n",
              " 'ions': 507,\n",
              " 'Ä who': 508,\n",
              " 'Ä K': 509,\n",
              " 'Ä up': 510,\n",
              " 'Ä their': 511,\n",
              " 'Ä ad': 512,\n",
              " 'Ä 3': 513,\n",
              " 'Ä us': 514,\n",
              " 'ated': 515,\n",
              " 'ous': 516,\n",
              " 'Ä more': 517,\n",
              " 'ue': 518,\n",
              " 'og': 519,\n",
              " 'Ä St': 520,\n",
              " 'ind': 521,\n",
              " 'ike': 522,\n",
              " 'Ä so': 523,\n",
              " 'ime': 524,\n",
              " 'per': 525,\n",
              " '.\"': 526,\n",
              " 'ber': 527,\n",
              " 'iz': 528,\n",
              " 'act': 529,\n",
              " 'Ä one': 530,\n",
              " 'Ä said': 531,\n",
              " 'Ä -': 532,\n",
              " 'are': 533,\n",
              " 'Ä your': 534,\n",
              " 'cc': 535,\n",
              " 'Ä Th': 536,\n",
              " 'Ä cl': 537,\n",
              " 'ep': 538,\n",
              " 'ake': 539,\n",
              " 'able': 540,\n",
              " 'ip': 541,\n",
              " 'Ä cont': 542,\n",
              " 'Ä which': 543,\n",
              " 'ia': 544,\n",
              " 'Ä im': 545,\n",
              " 'Ä about': 546,\n",
              " 'Ä were': 547,\n",
              " 'very': 548,\n",
              " 'ub': 549,\n",
              " 'Ä had': 550,\n",
              " 'Ä en': 551,\n",
              " 'Ä comp': 552,\n",
              " ',\"': 553,\n",
              " 'Ä In': 554,\n",
              " 'Ä un': 555,\n",
              " 'Ä ag': 556,\n",
              " 'ire': 557,\n",
              " 'ace': 558,\n",
              " 'au': 559,\n",
              " 'ary': 560,\n",
              " 'Ä would': 561,\n",
              " 'ass': 562,\n",
              " 'ry': 563,\n",
              " 'Ä Ã¢Ä¢': 564,\n",
              " 'cl': 565,\n",
              " 'ook': 566,\n",
              " 'ere': 567,\n",
              " 'so': 568,\n",
              " 'Ä V': 569,\n",
              " 'ign': 570,\n",
              " 'ib': 571,\n",
              " 'Ä off': 572,\n",
              " 'Ä te': 573,\n",
              " 'ven': 574,\n",
              " 'Ä Y': 575,\n",
              " 'ile': 576,\n",
              " 'ose': 577,\n",
              " 'ite': 578,\n",
              " 'orm': 579,\n",
              " 'Ä 201': 580,\n",
              " 'Ä res': 581,\n",
              " 'Ä man': 582,\n",
              " 'Ä per': 583,\n",
              " 'Ä other': 584,\n",
              " 'ord': 585,\n",
              " 'ult': 586,\n",
              " 'Ä been': 587,\n",
              " 'Ä like': 588,\n",
              " 'ase': 589,\n",
              " 'ance': 590,\n",
              " 'ks': 591,\n",
              " 'ays': 592,\n",
              " 'own': 593,\n",
              " 'ence': 594,\n",
              " 'Ä dis': 595,\n",
              " 'ction': 596,\n",
              " 'Ä any': 597,\n",
              " 'Ä app': 598,\n",
              " 'Ä sp': 599,\n",
              " 'int': 600,\n",
              " 'ress': 601,\n",
              " 'ations': 602,\n",
              " 'ail': 603,\n",
              " 'Ä 4': 604,\n",
              " 'ical': 605,\n",
              " 'Ä them': 606,\n",
              " 'Ä her': 607,\n",
              " 'ount': 608,\n",
              " 'Ä Ch': 609,\n",
              " 'Ä ar': 610,\n",
              " 'Ä if': 611,\n",
              " 'Ä there': 612,\n",
              " 'Ä pe': 613,\n",
              " 'Ä year': 614,\n",
              " 'av': 615,\n",
              " 'Ä my': 616,\n",
              " 'Ä some': 617,\n",
              " 'Ä when': 618,\n",
              " 'ough': 619,\n",
              " 'ach': 620,\n",
              " 'Ä than': 621,\n",
              " 'ru': 622,\n",
              " 'ond': 623,\n",
              " 'ick': 624,\n",
              " 'Ä over': 625,\n",
              " 'vel': 626,\n",
              " 'Ä qu': 627,\n",
              " 'ÄŠÄŠ': 628,\n",
              " 'Ä sc': 629,\n",
              " 'reat': 630,\n",
              " 'ree': 631,\n",
              " 'Ä It': 632,\n",
              " 'ound': 633,\n",
              " 'port': 634,\n",
              " 'Ä also': 635,\n",
              " 'Ä part': 636,\n",
              " 'fter': 637,\n",
              " 'Ä kn': 638,\n",
              " 'Ä bec': 639,\n",
              " 'Ä time': 640,\n",
              " 'ens': 641,\n",
              " 'Ä 5': 642,\n",
              " 'ople': 643,\n",
              " 'Ä what': 644,\n",
              " 'Ä no': 645,\n",
              " 'du': 646,\n",
              " 'mer': 647,\n",
              " 'ang': 648,\n",
              " 'Ä new': 649,\n",
              " '----': 650,\n",
              " 'Ä get': 651,\n",
              " 'ory': 652,\n",
              " 'ition': 653,\n",
              " 'ings': 654,\n",
              " 'Ä just': 655,\n",
              " 'Ä into': 656,\n",
              " 'Ä 0': 657,\n",
              " 'ents': 658,\n",
              " 'ove': 659,\n",
              " 'te': 660,\n",
              " 'Ä people': 661,\n",
              " 'Ä pre': 662,\n",
              " 'Ä its': 663,\n",
              " 'Ä rec': 664,\n",
              " 'Ä tw': 665,\n",
              " 'ian': 666,\n",
              " 'irst': 667,\n",
              " 'ark': 668,\n",
              " 'ors': 669,\n",
              " 'Ä work': 670,\n",
              " 'ade': 671,\n",
              " 'ob': 672,\n",
              " 'Ä she': 673,\n",
              " 'Ä our': 674,\n",
              " 'wn': 675,\n",
              " 'ink': 676,\n",
              " 'lic': 677,\n",
              " 'Ä 19': 678,\n",
              " 'Ä He': 679,\n",
              " 'ish': 680,\n",
              " 'nder': 681,\n",
              " 'ause': 682,\n",
              " 'Ä him': 683,\n",
              " 'ons': 684,\n",
              " 'Ä [': 685,\n",
              " 'Ä ro': 686,\n",
              " 'form': 687,\n",
              " 'ild': 688,\n",
              " 'ates': 689,\n",
              " 'vers': 690,\n",
              " 'Ä only': 691,\n",
              " 'oll': 692,\n",
              " 'Ä spe': 693,\n",
              " 'ck': 694,\n",
              " 'ell': 695,\n",
              " 'amp': 696,\n",
              " 'Ä acc': 697,\n",
              " 'Ä bl': 698,\n",
              " 'ious': 699,\n",
              " 'urn': 700,\n",
              " 'ft': 701,\n",
              " 'ood': 702,\n",
              " 'Ä how': 703,\n",
              " 'hed': 704,\n",
              " \"Ä '\": 705,\n",
              " 'Ä after': 706,\n",
              " 'aw': 707,\n",
              " 'Ä att': 708,\n",
              " 'ov': 709,\n",
              " 'ne': 710,\n",
              " 'Ä play': 711,\n",
              " 'erv': 712,\n",
              " 'ict': 713,\n",
              " 'Ä could': 714,\n",
              " 'itt': 715,\n",
              " 'Ä am': 716,\n",
              " 'Ä first': 717,\n",
              " 'Ä 6': 718,\n",
              " 'Ä act': 719,\n",
              " 'Ä $': 720,\n",
              " 'ec': 721,\n",
              " 'hing': 722,\n",
              " 'ual': 723,\n",
              " 'ull': 724,\n",
              " 'Ä comm': 725,\n",
              " 'oy': 726,\n",
              " 'old': 727,\n",
              " 'ces': 728,\n",
              " 'ater': 729,\n",
              " 'Ä fe': 730,\n",
              " 'Ä bet': 731,\n",
              " 'we': 732,\n",
              " 'iff': 733,\n",
              " 'Ä two': 734,\n",
              " 'ock': 735,\n",
              " 'Ä back': 736,\n",
              " ').': 737,\n",
              " 'ident': 738,\n",
              " 'Ä under': 739,\n",
              " 'rough': 740,\n",
              " 'sel': 741,\n",
              " 'xt': 742,\n",
              " 'Ä may': 743,\n",
              " 'round': 744,\n",
              " 'Ä po': 745,\n",
              " 'ph': 746,\n",
              " 'iss': 747,\n",
              " 'Ä des': 748,\n",
              " 'Ä most': 749,\n",
              " 'Ä did': 750,\n",
              " 'Ä add': 751,\n",
              " 'ject': 752,\n",
              " 'Ä inc': 753,\n",
              " 'fore': 754,\n",
              " 'Ä pol': 755,\n",
              " 'ont': 756,\n",
              " 'Ä again': 757,\n",
              " 'clud': 758,\n",
              " 'tern': 759,\n",
              " 'Ä know': 760,\n",
              " 'Ä need': 761,\n",
              " 'Ä cons': 762,\n",
              " 'Ä co': 763,\n",
              " 'Ä .': 764,\n",
              " 'Ä want': 765,\n",
              " 'Ä see': 766,\n",
              " 'Ä 7': 767,\n",
              " 'ning': 768,\n",
              " 'iew': 769,\n",
              " 'Ä This': 770,\n",
              " 'ced': 771,\n",
              " 'Ä even': 772,\n",
              " 'Ä ind': 773,\n",
              " 'ty': 774,\n",
              " 'Ä We': 775,\n",
              " 'ath': 776,\n",
              " 'Ä these': 777,\n",
              " 'Ä pr': 778,\n",
              " 'Ä use': 779,\n",
              " 'Ä because': 780,\n",
              " 'Ä fl': 781,\n",
              " 'ng': 782,\n",
              " 'Ä now': 783,\n",
              " 'Ä Ã¢Ä¢Äµ': 784,\n",
              " 'com': 785,\n",
              " 'ise': 786,\n",
              " 'Ä make': 787,\n",
              " 'Ä then': 788,\n",
              " 'ower': 789,\n",
              " 'Ä every': 790,\n",
              " 'Ä Un': 791,\n",
              " 'Ä sec': 792,\n",
              " 'oss': 793,\n",
              " 'uch': 794,\n",
              " 'Ä em': 795,\n",
              " 'Ä =': 796,\n",
              " 'Ä Re': 797,\n",
              " 'ied': 798,\n",
              " 'rit': 799,\n",
              " 'Ä inv': 800,\n",
              " 'lect': 801,\n",
              " 'Ä supp': 802,\n",
              " 'ating': 803,\n",
              " 'Ä look': 804,\n",
              " 'man': 805,\n",
              " 'pect': 806,\n",
              " 'Ä 8': 807,\n",
              " 'row': 808,\n",
              " 'Ä bu': 809,\n",
              " 'Ä where': 810,\n",
              " 'ific': 811,\n",
              " 'Ä years': 812,\n",
              " 'ily': 813,\n",
              " 'Ä diff': 814,\n",
              " 'Ä should': 815,\n",
              " 'Ä rem': 816,\n",
              " 'Th': 817,\n",
              " 'In': 818,\n",
              " 'Ä ev': 819,\n",
              " 'day': 820,\n",
              " \"'re\": 821,\n",
              " 'rib': 822,\n",
              " 'Ä rel': 823,\n",
              " 'ss': 824,\n",
              " 'Ä def': 825,\n",
              " 'Ä right': 826,\n",
              " 'Ä sy': 827,\n",
              " '),': 828,\n",
              " 'les': 829,\n",
              " '000': 830,\n",
              " 'hen': 831,\n",
              " 'Ä through': 832,\n",
              " 'Ä Tr': 833,\n",
              " '__': 834,\n",
              " 'Ä way': 835,\n",
              " 'Ä don': 836,\n",
              " 'Ä ,': 837,\n",
              " 'Ä 10': 838,\n",
              " 'ased': 839,\n",
              " 'Ä ass': 840,\n",
              " 'ublic': 841,\n",
              " 'Ä reg': 842,\n",
              " 'Ä And': 843,\n",
              " 'ix': 844,\n",
              " 'Ä very': 845,\n",
              " 'Ä includ': 846,\n",
              " 'other': 847,\n",
              " 'Ä imp': 848,\n",
              " 'oth': 849,\n",
              " 'Ä sub': 850,\n",
              " 'Ä Ã¢Ä¢Ä¶': 851,\n",
              " 'Ä being': 852,\n",
              " 'arg': 853,\n",
              " 'Ä Wh': 854,\n",
              " '==': 855,\n",
              " 'ible': 856,\n",
              " 'Ä does': 857,\n",
              " 'ange': 858,\n",
              " 'ram': 859,\n",
              " 'Ä 9': 860,\n",
              " 'ert': 861,\n",
              " 'ps': 862,\n",
              " 'ited': 863,\n",
              " 'ational': 864,\n",
              " 'Ä br': 865,\n",
              " 'Ä down': 866,\n",
              " 'Ä many': 867,\n",
              " 'aking': 868,\n",
              " 'Ä call': 869,\n",
              " 'uring': 870,\n",
              " 'ities': 871,\n",
              " 'Ä ph': 872,\n",
              " 'ics': 873,\n",
              " 'als': 874,\n",
              " 'Ä dec': 875,\n",
              " 'ative': 876,\n",
              " 'ener': 877,\n",
              " 'Ä before': 878,\n",
              " 'ility': 879,\n",
              " 'Ä well': 880,\n",
              " 'Ä much': 881,\n",
              " 'erson': 882,\n",
              " 'Ä those': 883,\n",
              " 'Ä such': 884,\n",
              " 'Ä ke': 885,\n",
              " 'Ä end': 886,\n",
              " 'Ä But': 887,\n",
              " 'ason': 888,\n",
              " 'ting': 889,\n",
              " 'Ä long': 890,\n",
              " 'ef': 891,\n",
              " 'Ä think': 892,\n",
              " 'ys': 893,\n",
              " 'Ä bel': 894,\n",
              " 'Ä sm': 895,\n",
              " 'its': 896,\n",
              " 'ax': 897,\n",
              " 'Ä own': 898,\n",
              " 'Ä prov': 899,\n",
              " 'Ä set': 900,\n",
              " 'ife': 901,\n",
              " 'ments': 902,\n",
              " 'ble': 903,\n",
              " 'ward': 904,\n",
              " 'Ä show': 905,\n",
              " 'Ä pres': 906,\n",
              " 'ms': 907,\n",
              " 'omet': 908,\n",
              " 'Ä ob': 909,\n",
              " 'Ä say': 910,\n",
              " 'Ä Sh': 911,\n",
              " 'ts': 912,\n",
              " 'ful': 913,\n",
              " 'Ä eff': 914,\n",
              " 'Ä gu': 915,\n",
              " 'Ä inst': 916,\n",
              " 'und': 917,\n",
              " 'ren': 918,\n",
              " 'cess': 919,\n",
              " 'Ä ent': 920,\n",
              " 'Ä You': 921,\n",
              " 'Ä good': 922,\n",
              " 'Ä start': 923,\n",
              " 'ince': 924,\n",
              " 'Ä made': 925,\n",
              " 'tt': 926,\n",
              " 'stem': 927,\n",
              " 'olog': 928,\n",
              " 'up': 929,\n",
              " 'Ä |': 930,\n",
              " 'ump': 931,\n",
              " 'Ä hel': 932,\n",
              " 'vern': 933,\n",
              " 'ular': 934,\n",
              " 'ually': 935,\n",
              " 'Ä ac': 936,\n",
              " 'Ä mon': 937,\n",
              " 'Ä last': 938,\n",
              " 'Ä 200': 939,\n",
              " '10': 940,\n",
              " 'Ä stud': 941,\n",
              " 'ures': 942,\n",
              " 'Ä Ar': 943,\n",
              " 'self': 944,\n",
              " 'ars': 945,\n",
              " 'meric': 946,\n",
              " 'ues': 947,\n",
              " 'cy': 948,\n",
              " 'Ä min': 949,\n",
              " 'ollow': 950,\n",
              " 'Ä col': 951,\n",
              " 'io': 952,\n",
              " 'Ä mod': 953,\n",
              " 'Ä count': 954,\n",
              " 'Ä Com': 955,\n",
              " 'hes': 956,\n",
              " 'Ä fin': 957,\n",
              " 'air': 958,\n",
              " 'ier': 959,\n",
              " 'Ã¢Ä¢Ä¶': 960,\n",
              " 'read': 961,\n",
              " 'ank': 962,\n",
              " 'atch': 963,\n",
              " 'ever': 964,\n",
              " 'Ä str': 965,\n",
              " 'Ä point': 966,\n",
              " 'ork': 967,\n",
              " 'Ä New': 968,\n",
              " 'Ä sur': 969,\n",
              " 'ool': 970,\n",
              " 'alk': 971,\n",
              " 'ement': 972,\n",
              " 'Ä used': 973,\n",
              " 'ract': 974,\n",
              " 'ween': 975,\n",
              " 'Ä same': 976,\n",
              " 'oun': 977,\n",
              " 'Ä Al': 978,\n",
              " 'ci': 979,\n",
              " 'Ä differe': 980,\n",
              " 'Ä while': 981,\n",
              " '--------': 982,\n",
              " 'Ä game': 983,\n",
              " 'cept': 984,\n",
              " 'Ä sim': 985,\n",
              " '...': 986,\n",
              " 'Ä inter': 987,\n",
              " 'ek': 988,\n",
              " 'Ä report': 989,\n",
              " 'Ä produ': 990,\n",
              " 'Ä still': 991,\n",
              " 'led': 992,\n",
              " 'ah': 993,\n",
              " 'Ä here': 994,\n",
              " 'Ä world': 995,\n",
              " 'Ä though': 996,\n",
              " 'Ä num': 997,\n",
              " 'arch': 998,\n",
              " 'imes': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyOHcg5siFQy",
        "outputId": "438afb9d-d3b8-412a-fc5c-6bd92ac63a72"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(bpe_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lC-T0U6XiHUz",
        "outputId": "36f15b1d-3629-4432-80c0-22088967327b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "420572"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "# ^---- ~equivalent to our \"merges\""
      ],
      "metadata": {
        "id": "YpjaQcNhgkIU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_merges # cases where we merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFVwP-9BgkF-",
        "outputId": "1f4a667d-975e-49bf-c676-a73d5da56cf9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ä ', 't'),\n",
              " ('Ä ', 'a'),\n",
              " ('h', 'e'),\n",
              " ('i', 'n'),\n",
              " ('r', 'e'),\n",
              " ('o', 'n'),\n",
              " ('Ä t', 'he'),\n",
              " ('e', 'r'),\n",
              " ('Ä ', 's'),\n",
              " ('a', 't'),\n",
              " ('Ä ', 'w'),\n",
              " ('Ä ', 'o'),\n",
              " ('e', 'n'),\n",
              " ('Ä ', 'c'),\n",
              " ('i', 't'),\n",
              " ('i', 's'),\n",
              " ('a', 'n'),\n",
              " ('o', 'r'),\n",
              " ('e', 's'),\n",
              " ('Ä ', 'b'),\n",
              " ('e', 'd'),\n",
              " ('Ä ', 'f'),\n",
              " ('in', 'g'),\n",
              " ('Ä ', 'p'),\n",
              " ('o', 'u'),\n",
              " ('Ä a', 'n'),\n",
              " ('a', 'l'),\n",
              " ('a', 'r'),\n",
              " ('Ä t', 'o'),\n",
              " ('Ä ', 'm'),\n",
              " ('Ä o', 'f'),\n",
              " ('Ä ', 'in'),\n",
              " ('Ä ', 'd'),\n",
              " ('Ä ', 'h'),\n",
              " ('Ä an', 'd'),\n",
              " ('i', 'c'),\n",
              " ('a', 's'),\n",
              " ('l', 'e'),\n",
              " ('Ä t', 'h'),\n",
              " ('i', 'on'),\n",
              " ('o', 'm'),\n",
              " ('l', 'l'),\n",
              " ('en', 't'),\n",
              " ('Ä ', 'n'),\n",
              " ('Ä ', 'l'),\n",
              " ('s', 't'),\n",
              " ('Ä ', 're'),\n",
              " ('v', 'e'),\n",
              " ('Ä ', 'e'),\n",
              " ('r', 'o'),\n",
              " ('l', 'y'),\n",
              " ('Ä b', 'e'),\n",
              " ('Ä ', 'g'),\n",
              " ('Ä ', 'T'),\n",
              " ('c', 't'),\n",
              " ('Ä ', 'S'),\n",
              " ('i', 'd'),\n",
              " ('o', 't'),\n",
              " ('Ä ', 'I'),\n",
              " ('u', 't'),\n",
              " ('e', 't'),\n",
              " ('Ä ', 'A'),\n",
              " ('Ä ', 'is'),\n",
              " ('Ä ', 'on'),\n",
              " ('i', 'm'),\n",
              " ('a', 'm'),\n",
              " ('o', 'w'),\n",
              " ('a', 'y'),\n",
              " ('a', 'd'),\n",
              " ('s', 'e'),\n",
              " ('Ä th', 'at'),\n",
              " ('Ä ', 'C'),\n",
              " ('i', 'g'),\n",
              " ('Ä f', 'or'),\n",
              " ('a', 'c'),\n",
              " ('Ä ', 'y'),\n",
              " ('v', 'er'),\n",
              " ('u', 'r'),\n",
              " ('Ä ', 'u'),\n",
              " ('l', 'd'),\n",
              " ('Ä s', 't'),\n",
              " ('Ä ', 'M'),\n",
              " (\"'\", 's'),\n",
              " ('Ä ', 'he'),\n",
              " ('Ä ', 'it'),\n",
              " ('at', 'ion'),\n",
              " ('it', 'h'),\n",
              " ('i', 'r'),\n",
              " ('c', 'e'),\n",
              " ('Ä y', 'ou'),\n",
              " ('i', 'l'),\n",
              " ('Ä ', 'B'),\n",
              " ('Ä w', 'h'),\n",
              " ('o', 'l'),\n",
              " ('Ä ', 'P'),\n",
              " ('Ä w', 'ith'),\n",
              " ('Ä ', '1'),\n",
              " ('t', 'er'),\n",
              " ('c', 'h'),\n",
              " ('Ä a', 's'),\n",
              " ('Ä w', 'e'),\n",
              " ('Ä ', '('),\n",
              " ('n', 'd'),\n",
              " ('i', 'll'),\n",
              " ('Ä ', 'D'),\n",
              " ('i', 'f'),\n",
              " ('Ä ', '2'),\n",
              " ('a', 'g'),\n",
              " ('er', 's'),\n",
              " ('k', 'e'),\n",
              " ('Ä ', '\"'),\n",
              " ('Ä ', 'H'),\n",
              " ('e', 'm'),\n",
              " ('Ä c', 'on'),\n",
              " ('Ä ', 'W'),\n",
              " ('Ä ', 'R'),\n",
              " ('he', 'r'),\n",
              " ('Ä w', 'as'),\n",
              " ('Ä ', 'r'),\n",
              " ('o', 'd'),\n",
              " ('Ä ', 'F'),\n",
              " ('u', 'l'),\n",
              " ('at', 'e'),\n",
              " ('Ä a', 't'),\n",
              " ('r', 'i'),\n",
              " ('p', 'p'),\n",
              " ('o', 're'),\n",
              " ('Ä T', 'he'),\n",
              " ('Ä s', 'e'),\n",
              " ('u', 's'),\n",
              " ('Ä p', 'ro'),\n",
              " ('Ä h', 'a'),\n",
              " ('u', 'm'),\n",
              " ('Ä a', 're'),\n",
              " ('Ä d', 'e'),\n",
              " ('a', 'in'),\n",
              " ('an', 'd'),\n",
              " ('Ä o', 'r'),\n",
              " ('ig', 'h'),\n",
              " ('es', 't'),\n",
              " ('is', 't'),\n",
              " ('a', 'b'),\n",
              " ('r', 'om'),\n",
              " ('Ä ', 'N'),\n",
              " ('t', 'h'),\n",
              " ('Ä c', 'om'),\n",
              " ('Ä ', 'G'),\n",
              " ('u', 'n'),\n",
              " ('o', 'p'),\n",
              " ('0', '0'),\n",
              " ('Ä ', 'L'),\n",
              " ('Ä n', 'ot'),\n",
              " ('es', 's'),\n",
              " ('Ä e', 'x'),\n",
              " ('Ä ', 'v'),\n",
              " ('re', 's'),\n",
              " ('Ä ', 'E'),\n",
              " ('e', 'w'),\n",
              " ('it', 'y'),\n",
              " ('an', 't'),\n",
              " ('Ä b', 'y'),\n",
              " ('e', 'l'),\n",
              " ('o', 's'),\n",
              " ('or', 't'),\n",
              " ('o', 'c'),\n",
              " ('q', 'u'),\n",
              " ('Ä f', 'rom'),\n",
              " ('Ä ha', 've'),\n",
              " ('Ä s', 'u'),\n",
              " ('i', 've'),\n",
              " ('ou', 'ld'),\n",
              " ('Ä s', 'h'),\n",
              " ('Ä th', 'is'),\n",
              " ('n', 't'),\n",
              " ('r', 'a'),\n",
              " ('p', 'e'),\n",
              " ('igh', 't'),\n",
              " ('ar', 't'),\n",
              " ('m', 'ent'),\n",
              " ('Ä a', 'l'),\n",
              " ('u', 'st'),\n",
              " ('en', 'd'),\n",
              " ('-', '-'),\n",
              " ('al', 'l'),\n",
              " ('Ä ', 'O'),\n",
              " ('ac', 'k'),\n",
              " ('Ä c', 'h'),\n",
              " ('Ä ', 'le'),\n",
              " ('i', 'es'),\n",
              " ('re', 'd'),\n",
              " ('ar', 'd'),\n",
              " ('Ã¢', 'Ä¢'),\n",
              " ('ou', 't'),\n",
              " ('Ä ', 'J'),\n",
              " ('Ä a', 'b'),\n",
              " ('e', 'ar'),\n",
              " ('i', 'v'),\n",
              " ('al', 'ly'),\n",
              " ('ou', 'r'),\n",
              " ('o', 'st'),\n",
              " ('g', 'h'),\n",
              " ('p', 't'),\n",
              " ('Ä p', 'l'),\n",
              " ('as', 't'),\n",
              " ('Ä c', 'an'),\n",
              " ('a', 'k'),\n",
              " ('om', 'e'),\n",
              " ('u', 'd'),\n",
              " ('T', 'he'),\n",
              " ('Ä h', 'is'),\n",
              " ('Ä d', 'o'),\n",
              " ('Ä g', 'o'),\n",
              " ('Ä h', 'as'),\n",
              " ('g', 'e'),\n",
              " (\"'\", 't'),\n",
              " ('Ä ', 'U'),\n",
              " ('r', 'ou'),\n",
              " ('Ä s', 'a'),\n",
              " ('Ä ', 'j'),\n",
              " ('Ä b', 'ut'),\n",
              " ('Ä w', 'or'),\n",
              " ('Ä a', 'll'),\n",
              " ('e', 'ct'),\n",
              " ('Ä ', 'k'),\n",
              " ('am', 'e'),\n",
              " ('Ä w', 'ill'),\n",
              " ('o', 'k'),\n",
              " ('Ä w', 'he'),\n",
              " ('Ä the', 'y'),\n",
              " ('id', 'e'),\n",
              " ('0', '1'),\n",
              " ('f', 'f'),\n",
              " ('ic', 'h'),\n",
              " ('p', 'l'),\n",
              " ('t', 'her'),\n",
              " ('Ä t', 'r'),\n",
              " ('.', '.'),\n",
              " ('Ä in', 't'),\n",
              " ('i', 'e'),\n",
              " ('u', 're'),\n",
              " ('ag', 'e'),\n",
              " ('Ä n', 'e'),\n",
              " ('i', 'al'),\n",
              " ('a', 'p'),\n",
              " ('in', 'e'),\n",
              " ('ic', 'e'),\n",
              " ('Ä m', 'e'),\n",
              " ('Ä o', 'ut'),\n",
              " ('an', 's'),\n",
              " ('on', 'e'),\n",
              " ('on', 'g'),\n",
              " ('ion', 's'),\n",
              " ('Ä wh', 'o'),\n",
              " ('Ä ', 'K'),\n",
              " ('Ä u', 'p'),\n",
              " ('Ä the', 'ir'),\n",
              " ('Ä a', 'd'),\n",
              " ('Ä ', '3'),\n",
              " ('Ä u', 's'),\n",
              " ('at', 'ed'),\n",
              " ('ou', 's'),\n",
              " ('Ä m', 'ore'),\n",
              " ('u', 'e'),\n",
              " ('o', 'g'),\n",
              " ('Ä S', 't'),\n",
              " ('in', 'd'),\n",
              " ('i', 'ke'),\n",
              " ('Ä s', 'o'),\n",
              " ('im', 'e'),\n",
              " ('p', 'er'),\n",
              " ('.', '\"'),\n",
              " ('b', 'er'),\n",
              " ('i', 'z'),\n",
              " ('a', 'ct'),\n",
              " ('Ä on', 'e'),\n",
              " ('Ä sa', 'id'),\n",
              " ('Ä ', '-'),\n",
              " ('a', 're'),\n",
              " ('Ä you', 'r'),\n",
              " ('c', 'c'),\n",
              " ('Ä T', 'h'),\n",
              " ('Ä c', 'l'),\n",
              " ('e', 'p'),\n",
              " ('a', 'ke'),\n",
              " ('ab', 'le'),\n",
              " ('i', 'p'),\n",
              " ('Ä con', 't'),\n",
              " ('Ä wh', 'ich'),\n",
              " ('i', 'a'),\n",
              " ('Ä ', 'im'),\n",
              " ('Ä ab', 'out'),\n",
              " ('Ä we', 're'),\n",
              " ('ver', 'y'),\n",
              " ('u', 'b'),\n",
              " ('Ä h', 'ad'),\n",
              " ('Ä ', 'en'),\n",
              " ('Ä com', 'p'),\n",
              " (',', '\"'),\n",
              " ('Ä I', 'n'),\n",
              " ('Ä u', 'n'),\n",
              " ('Ä a', 'g'),\n",
              " ('i', 're'),\n",
              " ('ac', 'e'),\n",
              " ('a', 'u'),\n",
              " ('ar', 'y'),\n",
              " ('Ä w', 'ould'),\n",
              " ('as', 's'),\n",
              " ('r', 'y'),\n",
              " ('Ä ', 'Ã¢Ä¢'),\n",
              " ('c', 'l'),\n",
              " ('o', 'ok'),\n",
              " ('e', 're'),\n",
              " ('s', 'o'),\n",
              " ('Ä ', 'V'),\n",
              " ('ig', 'n'),\n",
              " ('i', 'b'),\n",
              " ('Ä of', 'f'),\n",
              " ('Ä t', 'e'),\n",
              " ('v', 'en'),\n",
              " ('Ä ', 'Y'),\n",
              " ('i', 'le'),\n",
              " ('o', 'se'),\n",
              " ('it', 'e'),\n",
              " ('or', 'm'),\n",
              " ('Ä 2', '01'),\n",
              " ('Ä re', 's'),\n",
              " ('Ä m', 'an'),\n",
              " ('Ä p', 'er'),\n",
              " ('Ä o', 'ther'),\n",
              " ('or', 'd'),\n",
              " ('ul', 't'),\n",
              " ('Ä be', 'en'),\n",
              " ('Ä l', 'ike'),\n",
              " ('as', 'e'),\n",
              " ('an', 'ce'),\n",
              " ('k', 's'),\n",
              " ('ay', 's'),\n",
              " ('ow', 'n'),\n",
              " ('en', 'ce'),\n",
              " ('Ä d', 'is'),\n",
              " ('ct', 'ion'),\n",
              " ('Ä an', 'y'),\n",
              " ('Ä a', 'pp'),\n",
              " ('Ä s', 'p'),\n",
              " ('in', 't'),\n",
              " ('res', 's'),\n",
              " ('ation', 's'),\n",
              " ('a', 'il'),\n",
              " ('Ä ', '4'),\n",
              " ('ic', 'al'),\n",
              " ('Ä the', 'm'),\n",
              " ('Ä he', 'r'),\n",
              " ('ou', 'nt'),\n",
              " ('Ä C', 'h'),\n",
              " ('Ä a', 'r'),\n",
              " ('Ä ', 'if'),\n",
              " ('Ä the', 're'),\n",
              " ('Ä p', 'e'),\n",
              " ('Ä y', 'ear'),\n",
              " ('a', 'v'),\n",
              " ('Ä m', 'y'),\n",
              " ('Ä s', 'ome'),\n",
              " ('Ä whe', 'n'),\n",
              " ('ou', 'gh'),\n",
              " ('ac', 'h'),\n",
              " ('Ä th', 'an'),\n",
              " ('r', 'u'),\n",
              " ('on', 'd'),\n",
              " ('ic', 'k'),\n",
              " ('Ä o', 'ver'),\n",
              " ('ve', 'l'),\n",
              " ('Ä ', 'qu'),\n",
              " ('ÄŠ', 'ÄŠ'),\n",
              " ('Ä s', 'c'),\n",
              " ('re', 'at'),\n",
              " ('re', 'e'),\n",
              " ('Ä I', 't'),\n",
              " ('ou', 'nd'),\n",
              " ('p', 'ort'),\n",
              " ('Ä al', 'so'),\n",
              " ('Ä p', 'art'),\n",
              " ('f', 'ter'),\n",
              " ('Ä k', 'n'),\n",
              " ('Ä be', 'c'),\n",
              " ('Ä t', 'ime'),\n",
              " ('en', 's'),\n",
              " ('Ä ', '5'),\n",
              " ('op', 'le'),\n",
              " ('Ä wh', 'at'),\n",
              " ('Ä n', 'o'),\n",
              " ('d', 'u'),\n",
              " ('m', 'er'),\n",
              " ('an', 'g'),\n",
              " ('Ä n', 'ew'),\n",
              " ('--', '--'),\n",
              " ('Ä g', 'et'),\n",
              " ('or', 'y'),\n",
              " ('it', 'ion'),\n",
              " ('ing', 's'),\n",
              " ('Ä j', 'ust'),\n",
              " ('Ä int', 'o'),\n",
              " ('Ä ', '0'),\n",
              " ('ent', 's'),\n",
              " ('o', 've'),\n",
              " ('t', 'e'),\n",
              " ('Ä pe', 'ople'),\n",
              " ('Ä p', 're'),\n",
              " ('Ä it', 's'),\n",
              " ('Ä re', 'c'),\n",
              " ('Ä t', 'w'),\n",
              " ('i', 'an'),\n",
              " ('ir', 'st'),\n",
              " ('ar', 'k'),\n",
              " ('or', 's'),\n",
              " ('Ä wor', 'k'),\n",
              " ('ad', 'e'),\n",
              " ('o', 'b'),\n",
              " ('Ä s', 'he'),\n",
              " ('Ä o', 'ur'),\n",
              " ('w', 'n'),\n",
              " ('in', 'k'),\n",
              " ('l', 'ic'),\n",
              " ('Ä 1', '9'),\n",
              " ('Ä H', 'e'),\n",
              " ('is', 'h'),\n",
              " ('nd', 'er'),\n",
              " ('au', 'se'),\n",
              " ('Ä h', 'im'),\n",
              " ('on', 's'),\n",
              " ('Ä ', '['),\n",
              " ('Ä ', 'ro'),\n",
              " ('f', 'orm'),\n",
              " ('i', 'ld'),\n",
              " ('at', 'es'),\n",
              " ('ver', 's'),\n",
              " ('Ä on', 'ly'),\n",
              " ('o', 'll'),\n",
              " ('Ä s', 'pe'),\n",
              " ('c', 'k'),\n",
              " ('e', 'll'),\n",
              " ('am', 'p'),\n",
              " ('Ä a', 'cc'),\n",
              " ('Ä b', 'l'),\n",
              " ('i', 'ous'),\n",
              " ('ur', 'n'),\n",
              " ('f', 't'),\n",
              " ('o', 'od'),\n",
              " ('Ä h', 'ow'),\n",
              " ('he', 'd'),\n",
              " ('Ä ', \"'\"),\n",
              " ('Ä a', 'fter'),\n",
              " ('a', 'w'),\n",
              " ('Ä at', 't'),\n",
              " ('o', 'v'),\n",
              " ('n', 'e'),\n",
              " ('Ä pl', 'ay'),\n",
              " ('er', 'v'),\n",
              " ('ic', 't'),\n",
              " ('Ä c', 'ould'),\n",
              " ('it', 't'),\n",
              " ('Ä a', 'm'),\n",
              " ('Ä f', 'irst'),\n",
              " ('Ä ', '6'),\n",
              " ('Ä a', 'ct'),\n",
              " ('Ä ', '$'),\n",
              " ('e', 'c'),\n",
              " ('h', 'ing'),\n",
              " ('u', 'al'),\n",
              " ('u', 'll'),\n",
              " ('Ä com', 'm'),\n",
              " ('o', 'y'),\n",
              " ('o', 'ld'),\n",
              " ('c', 'es'),\n",
              " ('at', 'er'),\n",
              " ('Ä f', 'e'),\n",
              " ('Ä be', 't'),\n",
              " ('w', 'e'),\n",
              " ('if', 'f'),\n",
              " ('Ä tw', 'o'),\n",
              " ('oc', 'k'),\n",
              " ('Ä b', 'ack'),\n",
              " (')', '.'),\n",
              " ('id', 'ent'),\n",
              " ('Ä u', 'nder'),\n",
              " ('rou', 'gh'),\n",
              " ('se', 'l'),\n",
              " ('x', 't'),\n",
              " ('Ä m', 'ay'),\n",
              " ('rou', 'nd'),\n",
              " ('Ä p', 'o'),\n",
              " ('p', 'h'),\n",
              " ('is', 's'),\n",
              " ('Ä d', 'es'),\n",
              " ('Ä m', 'ost'),\n",
              " ('Ä d', 'id'),\n",
              " ('Ä ad', 'd'),\n",
              " ('j', 'ect'),\n",
              " ('Ä in', 'c'),\n",
              " ('f', 'ore'),\n",
              " ('Ä p', 'ol'),\n",
              " ('on', 't'),\n",
              " ('Ä ag', 'ain'),\n",
              " ('cl', 'ud'),\n",
              " ('ter', 'n'),\n",
              " ('Ä kn', 'ow'),\n",
              " ('Ä ne', 'ed'),\n",
              " ('Ä con', 's'),\n",
              " ('Ä c', 'o'),\n",
              " ('Ä ', '.'),\n",
              " ('Ä w', 'ant'),\n",
              " ('Ä se', 'e'),\n",
              " ('Ä ', '7'),\n",
              " ('n', 'ing'),\n",
              " ('i', 'ew'),\n",
              " ('Ä Th', 'is'),\n",
              " ('c', 'ed'),\n",
              " ('Ä e', 'ven'),\n",
              " ('Ä in', 'd'),\n",
              " ('t', 'y'),\n",
              " ('Ä W', 'e'),\n",
              " ('at', 'h'),\n",
              " ('Ä the', 'se'),\n",
              " ('Ä p', 'r'),\n",
              " ('Ä u', 'se'),\n",
              " ('Ä bec', 'ause'),\n",
              " ('Ä f', 'l'),\n",
              " ('n', 'g'),\n",
              " ('Ä n', 'ow'),\n",
              " ('Ä Ã¢Ä¢', 'Äµ'),\n",
              " ('c', 'om'),\n",
              " ('is', 'e'),\n",
              " ('Ä m', 'ake'),\n",
              " ('Ä the', 'n'),\n",
              " ('ow', 'er'),\n",
              " ('Ä e', 'very'),\n",
              " ('Ä U', 'n'),\n",
              " ('Ä se', 'c'),\n",
              " ('os', 's'),\n",
              " ('u', 'ch'),\n",
              " ('Ä e', 'm'),\n",
              " ('Ä ', '='),\n",
              " ('Ä R', 'e'),\n",
              " ('i', 'ed'),\n",
              " ('r', 'it'),\n",
              " ('Ä in', 'v'),\n",
              " ('le', 'ct'),\n",
              " ('Ä su', 'pp'),\n",
              " ('at', 'ing'),\n",
              " ('Ä l', 'ook'),\n",
              " ('m', 'an'),\n",
              " ('pe', 'ct'),\n",
              " ('Ä ', '8'),\n",
              " ('ro', 'w'),\n",
              " ('Ä b', 'u'),\n",
              " ('Ä whe', 're'),\n",
              " ('if', 'ic'),\n",
              " ('Ä year', 's'),\n",
              " ('i', 'ly'),\n",
              " ('Ä d', 'iff'),\n",
              " ('Ä sh', 'ould'),\n",
              " ('Ä re', 'm'),\n",
              " ('T', 'h'),\n",
              " ('I', 'n'),\n",
              " ('Ä e', 'v'),\n",
              " ('d', 'ay'),\n",
              " (\"'\", 're'),\n",
              " ('ri', 'b'),\n",
              " ('Ä re', 'l'),\n",
              " ('s', 's'),\n",
              " ('Ä de', 'f'),\n",
              " ('Ä r', 'ight'),\n",
              " ('Ä s', 'y'),\n",
              " (')', ','),\n",
              " ('l', 'es'),\n",
              " ('00', '0'),\n",
              " ('he', 'n'),\n",
              " ('Ä th', 'rough'),\n",
              " ('Ä T', 'r'),\n",
              " ('_', '_'),\n",
              " ('Ä w', 'ay'),\n",
              " ('Ä d', 'on'),\n",
              " ('Ä ', ','),\n",
              " ('Ä 1', '0'),\n",
              " ('as', 'ed'),\n",
              " ('Ä as', 's'),\n",
              " ('ub', 'lic'),\n",
              " ('Ä re', 'g'),\n",
              " ('Ä A', 'nd'),\n",
              " ('i', 'x'),\n",
              " ('Ä ', 'very'),\n",
              " ('Ä in', 'clud'),\n",
              " ('ot', 'her'),\n",
              " ('Ä im', 'p'),\n",
              " ('ot', 'h'),\n",
              " ('Ä su', 'b'),\n",
              " ('Ä Ã¢Ä¢', 'Ä¶'),\n",
              " ('Ä be', 'ing'),\n",
              " ('ar', 'g'),\n",
              " ('Ä W', 'h'),\n",
              " ('=', '='),\n",
              " ('ib', 'le'),\n",
              " ('Ä do', 'es'),\n",
              " ('an', 'ge'),\n",
              " ('r', 'am'),\n",
              " ('Ä ', '9'),\n",
              " ('er', 't'),\n",
              " ('p', 's'),\n",
              " ('it', 'ed'),\n",
              " ('ation', 'al'),\n",
              " ('Ä b', 'r'),\n",
              " ('Ä d', 'own'),\n",
              " ('Ä man', 'y'),\n",
              " ('ak', 'ing'),\n",
              " ('Ä c', 'all'),\n",
              " ('ur', 'ing'),\n",
              " ('it', 'ies'),\n",
              " ('Ä p', 'h'),\n",
              " ('ic', 's'),\n",
              " ('al', 's'),\n",
              " ('Ä de', 'c'),\n",
              " ('at', 'ive'),\n",
              " ('en', 'er'),\n",
              " ('Ä be', 'fore'),\n",
              " ('il', 'ity'),\n",
              " ('Ä we', 'll'),\n",
              " ('Ä m', 'uch'),\n",
              " ('ers', 'on'),\n",
              " ('Ä th', 'ose'),\n",
              " ('Ä su', 'ch'),\n",
              " ('Ä ', 'ke'),\n",
              " ('Ä ', 'end'),\n",
              " ('Ä B', 'ut'),\n",
              " ('as', 'on'),\n",
              " ('t', 'ing'),\n",
              " ('Ä l', 'ong'),\n",
              " ('e', 'f'),\n",
              " ('Ä th', 'ink'),\n",
              " ('y', 's'),\n",
              " ('Ä be', 'l'),\n",
              " ('Ä s', 'm'),\n",
              " ('it', 's'),\n",
              " ('a', 'x'),\n",
              " ('Ä o', 'wn'),\n",
              " ('Ä pro', 'v'),\n",
              " ('Ä s', 'et'),\n",
              " ('if', 'e'),\n",
              " ('ment', 's'),\n",
              " ('b', 'le'),\n",
              " ('w', 'ard'),\n",
              " ('Ä sh', 'ow'),\n",
              " ('Ä p', 'res'),\n",
              " ('m', 's'),\n",
              " ('om', 'et'),\n",
              " ('Ä o', 'b'),\n",
              " ('Ä s', 'ay'),\n",
              " ('Ä S', 'h'),\n",
              " ('t', 's'),\n",
              " ('f', 'ul'),\n",
              " ('Ä e', 'ff'),\n",
              " ('Ä g', 'u'),\n",
              " ('Ä in', 'st'),\n",
              " ('u', 'nd'),\n",
              " ('re', 'n'),\n",
              " ('c', 'ess'),\n",
              " ('Ä ', 'ent'),\n",
              " ('Ä Y', 'ou'),\n",
              " ('Ä go', 'od'),\n",
              " ('Ä st', 'art'),\n",
              " ('in', 'ce'),\n",
              " ('Ä m', 'ade'),\n",
              " ('t', 't'),\n",
              " ('st', 'em'),\n",
              " ('ol', 'og'),\n",
              " ('u', 'p'),\n",
              " ('Ä ', '|'),\n",
              " ('um', 'p'),\n",
              " ('Ä he', 'l'),\n",
              " ('ver', 'n'),\n",
              " ('ul', 'ar'),\n",
              " ('u', 'ally'),\n",
              " ('Ä a', 'c'),\n",
              " ('Ä m', 'on'),\n",
              " ('Ä l', 'ast'),\n",
              " ('Ä 2', '00'),\n",
              " ('1', '0'),\n",
              " ('Ä st', 'ud'),\n",
              " ('u', 'res'),\n",
              " ('Ä A', 'r'),\n",
              " ('sel', 'f'),\n",
              " ('ar', 's'),\n",
              " ('mer', 'ic'),\n",
              " ('u', 'es'),\n",
              " ('c', 'y'),\n",
              " ('Ä m', 'in'),\n",
              " ('oll', 'ow'),\n",
              " ('Ä c', 'ol'),\n",
              " ('i', 'o'),\n",
              " ('Ä m', 'od'),\n",
              " ('Ä c', 'ount'),\n",
              " ('Ä C', 'om'),\n",
              " ('he', 's'),\n",
              " ('Ä f', 'in'),\n",
              " ('a', 'ir'),\n",
              " ('i', 'er'),\n",
              " ('Ã¢Ä¢', 'Ä¶'),\n",
              " ('re', 'ad'),\n",
              " ('an', 'k'),\n",
              " ('at', 'ch'),\n",
              " ('e', 'ver'),\n",
              " ('Ä st', 'r'),\n",
              " ('Ä po', 'int'),\n",
              " ('or', 'k'),\n",
              " ('Ä N', 'ew'),\n",
              " ('Ä s', 'ur'),\n",
              " ('o', 'ol'),\n",
              " ('al', 'k'),\n",
              " ('em', 'ent'),\n",
              " ('Ä us', 'ed'),\n",
              " ('ra', 'ct'),\n",
              " ('we', 'en'),\n",
              " ('Ä s', 'ame'),\n",
              " ('ou', 'n'),\n",
              " ('Ä A', 'l'),\n",
              " ('c', 'i'),\n",
              " ('Ä diff', 'ere'),\n",
              " ('Ä wh', 'ile'),\n",
              " ('----', '----'),\n",
              " ('Ä g', 'ame'),\n",
              " ('ce', 'pt'),\n",
              " ('Ä s', 'im'),\n",
              " ('..', '.'),\n",
              " ('Ä in', 'ter'),\n",
              " ('e', 'k'),\n",
              " ('Ä re', 'port'),\n",
              " ('Ä pro', 'du'),\n",
              " ('Ä st', 'ill'),\n",
              " ('l', 'ed'),\n",
              " ('a', 'h'),\n",
              " ('Ä he', 're'),\n",
              " ('Ä wor', 'ld'),\n",
              " ('Ä th', 'ough'),\n",
              " ('Ä n', 'um'),\n",
              " ('ar', 'ch'),\n",
              " ('im', 'es'),\n",
              " ('al', 'e'),\n",
              " ('Ä S', 'e'),\n",
              " ('Ä I', 'f'),\n",
              " ('/', '/'),\n",
              " ('Ä L', 'e'),\n",
              " ('Ä re', 't'),\n",
              " ('Ä re', 'f'),\n",
              " ('Ä tr', 'ans'),\n",
              " ('n', 'er'),\n",
              " ('ut', 'ion'),\n",
              " ('ter', 's'),\n",
              " ('Ä t', 'ake'),\n",
              " ('Ä C', 'l'),\n",
              " ('Ä con', 'f'),\n",
              " ('w', 'ay'),\n",
              " ('a', 've'),\n",
              " ('Ä go', 'ing'),\n",
              " ('Ä s', 'l'),\n",
              " ('u', 'g'),\n",
              " ('Ä A', 'meric'),\n",
              " ('Ä spe', 'c'),\n",
              " ('Ä h', 'and'),\n",
              " ('Ä bet', 'ween'),\n",
              " ('ist', 's'),\n",
              " ('Ä D', 'e'),\n",
              " ('o', 'ot'),\n",
              " ('I', 't'),\n",
              " ('Ä e', 'ar'),\n",
              " ('Ä again', 'st'),\n",
              " ('Ä h', 'igh'),\n",
              " ('g', 'an'),\n",
              " ('a', 'z'),\n",
              " ('at', 'her'),\n",
              " ('Ä ex', 'p'),\n",
              " ('Ä o', 'p'),\n",
              " ('Ä in', 's'),\n",
              " ('Ä g', 'r'),\n",
              " ('Ä hel', 'p'),\n",
              " ('Ä re', 'qu'),\n",
              " ('et', 's'),\n",
              " ('in', 's'),\n",
              " ('Ä P', 'ro'),\n",
              " ('is', 'm'),\n",
              " ('Ä f', 'ound'),\n",
              " ('l', 'and'),\n",
              " ('at', 'a'),\n",
              " ('us', 's'),\n",
              " ('am', 'es'),\n",
              " ('Ä p', 'erson'),\n",
              " ('Ä g', 'reat'),\n",
              " ('p', 'r'),\n",
              " ('Ä s', 'ign'),\n",
              " ('Ä A', 'n'),\n",
              " (\"'\", 've'),\n",
              " ('Ä s', 'omet'),\n",
              " ('Ä s', 'er'),\n",
              " ('h', 'ip'),\n",
              " ('Ä r', 'un'),\n",
              " ('Ä ', ':'),\n",
              " ('Ä t', 'er'),\n",
              " ('ire', 'ct'),\n",
              " ('Ä f', 'ollow'),\n",
              " ('Ä d', 'et'),\n",
              " ('ic', 'es'),\n",
              " ('Ä f', 'ind'),\n",
              " ('1', '2'),\n",
              " ('Ä m', 'em'),\n",
              " ('Ä c', 'r'),\n",
              " ('e', 'red'),\n",
              " ('e', 'x'),\n",
              " ('Ä ex', 't'),\n",
              " ('ut', 'h'),\n",
              " ('en', 'se'),\n",
              " ('c', 'o'),\n",
              " ('Ä te', 'am'),\n",
              " ('v', 'ing'),\n",
              " ('ou', 'se'),\n",
              " ('as', 'h'),\n",
              " ('at', 't'),\n",
              " ('v', 'ed'),\n",
              " ('Ä sy', 'stem'),\n",
              " ('Ä A', 's'),\n",
              " ('d', 'er'),\n",
              " ('iv', 'es'),\n",
              " ('m', 'in'),\n",
              " ('Ä le', 'ad'),\n",
              " ('Ä B', 'l'),\n",
              " ('c', 'ent'),\n",
              " ('Ä a', 'round'),\n",
              " ('Ä go', 'vern'),\n",
              " ('Ä c', 'ur'),\n",
              " ('vel', 'op'),\n",
              " ('an', 'y'),\n",
              " ('Ä c', 'our'),\n",
              " ('al', 'th'),\n",
              " ('ag', 'es'),\n",
              " ('iz', 'e'),\n",
              " ('Ä c', 'ar'),\n",
              " ('od', 'e'),\n",
              " ('Ä l', 'aw'),\n",
              " ('Ä re', 'ad'),\n",
              " (\"'\", 'm'),\n",
              " ('c', 'on'),\n",
              " ('Ä re', 'al'),\n",
              " ('Ä supp', 'ort'),\n",
              " ('Ä 1', '2'),\n",
              " ('..', '..'),\n",
              " ('Ä re', 'ally'),\n",
              " ('n', 'ess'),\n",
              " ('Ä f', 'act'),\n",
              " ('Ä d', 'ay'),\n",
              " ('Ä b', 'oth'),\n",
              " ('y', 'ing'),\n",
              " ('Ä s', 'erv'),\n",
              " ('Ä F', 'or'),\n",
              " ('Ä th', 'ree'),\n",
              " ('Ä w', 'om'),\n",
              " ('Ä m', 'ed'),\n",
              " ('od', 'y'),\n",
              " ('Ä The', 'y'),\n",
              " ('5', '0'),\n",
              " ('Ä ex', 'per'),\n",
              " ('t', 'on'),\n",
              " ('Ä e', 'ach'),\n",
              " ('ak', 'es'),\n",
              " ('Ä c', 'he'),\n",
              " ('Ä c', 're'),\n",
              " ('in', 'es'),\n",
              " ('Ä re', 'p'),\n",
              " ('1', '9'),\n",
              " ('g', 'g'),\n",
              " ('ill', 'ion'),\n",
              " ('Ä g', 'rou'),\n",
              " ('ut', 'e'),\n",
              " ('i', 'k'),\n",
              " ('W', 'e'),\n",
              " ('g', 'et'),\n",
              " ('E', 'R'),\n",
              " ('Ä m', 'et'),\n",
              " ('Ä s', 'ays'),\n",
              " ('o', 'x'),\n",
              " ('Ä d', 'uring'),\n",
              " ('er', 'n'),\n",
              " ('iz', 'ed'),\n",
              " ('a', 'red'),\n",
              " ('Ä f', 'am'),\n",
              " ('ic', 'ally'),\n",
              " ('Ä ha', 'pp'),\n",
              " ('Ä I', 's'),\n",
              " ('Ä ch', 'ar'),\n",
              " ('m', 'ed'),\n",
              " ('v', 'ent'),\n",
              " ('Ä g', 'ener'),\n",
              " ('i', 'ent'),\n",
              " ('p', 'le'),\n",
              " ('i', 'et'),\n",
              " ('re', 'nt'),\n",
              " ('1', '1'),\n",
              " ('v', 'es'),\n",
              " ('pt', 'ion'),\n",
              " ('Ä 2', '0'),\n",
              " ('form', 'ation'),\n",
              " ('Ä c', 'or'),\n",
              " ('Ä off', 'ic'),\n",
              " ('ie', 'ld'),\n",
              " ('Ä to', 'o'),\n",
              " ('is', 'ion'),\n",
              " ('Ä in', 'f'),\n",
              " ('Ä ', 'Z'),\n",
              " ('t', 'he'),\n",
              " ('o', 'ad'),\n",
              " ('Ä p', 'ublic'),\n",
              " ('Ä pro', 'g'),\n",
              " ('r', 'ic'),\n",
              " ('*', '*'),\n",
              " ('Ä w', 'ar'),\n",
              " ('Ä p', 'ower'),\n",
              " ('v', 'iew'),\n",
              " ('Ä f', 'ew'),\n",
              " ('Ä l', 'oc'),\n",
              " ('Ä differe', 'nt'),\n",
              " ('Ä st', 'ate'),\n",
              " ('Ä he', 'ad'),\n",
              " (\"'\", 'll'),\n",
              " ('Ä p', 'oss'),\n",
              " ('Ä st', 'at'),\n",
              " ('re', 't'),\n",
              " ('ant', 's'),\n",
              " ('Ä v', 'al'),\n",
              " ('Ä is', 's'),\n",
              " ('Ä c', 'le'),\n",
              " ('i', 'vers'),\n",
              " ('an', 'c'),\n",
              " ('Ä ex', 'pl'),\n",
              " ('Ä an', 'other'),\n",
              " ('Ä ', 'Q'),\n",
              " ('Ä a', 'v'),\n",
              " ('th', 'ing'),\n",
              " ('n', 'ce'),\n",
              " ('W', 'h'),\n",
              " ('Ä ch', 'ild'),\n",
              " ('Ä s', 'ince'),\n",
              " ('i', 'red'),\n",
              " ('l', 'ess'),\n",
              " ('Ä l', 'ife'),\n",
              " ('Ä de', 'velop'),\n",
              " ('itt', 'le'),\n",
              " ('Ä de', 'p'),\n",
              " ('Ä p', 'ass'),\n",
              " ('Ã£', 'Ä¥'),\n",
              " ('Ä t', 'urn'),\n",
              " ('or', 'n'),\n",
              " ('Th', 'is'),\n",
              " ('b', 'ers'),\n",
              " ('ro', 'ss'),\n",
              " ('Ä A', 'd'),\n",
              " ('Ä f', 'r'),\n",
              " ('Ä res', 'p'),\n",
              " ('Ä sec', 'ond'),\n",
              " ('o', 'h'),\n",
              " ('Ä ', '/'),\n",
              " ('Ä dis', 'c'),\n",
              " ('Ä ', '&'),\n",
              " ('Ä somet', 'hing'),\n",
              " ('Ä comp', 'le'),\n",
              " ('Ä ', 'ed'),\n",
              " ('Ä f', 'il'),\n",
              " ('Ä mon', 'th'),\n",
              " ('a', 'j'),\n",
              " ('u', 'c'),\n",
              " ('Ä govern', 'ment'),\n",
              " ('Ä with', 'out'),\n",
              " ('Ä le', 'g'),\n",
              " ('Ä d', 'ist'),\n",
              " ('Ä p', 'ut'),\n",
              " ('Ä qu', 'est'),\n",
              " ('an', 'n'),\n",
              " ('Ä pro', 't'),\n",
              " ('2', '0'),\n",
              " ('Ä ne', 'ver'),\n",
              " ('i', 'ence'),\n",
              " ('Ä le', 'vel'),\n",
              " ('Ä ar', 't'),\n",
              " ('Ä th', 'ings'),\n",
              " ('Ä m', 'ight'),\n",
              " ('Ä eff', 'ect'),\n",
              " ('Ä cont', 'ro'),\n",
              " ('Ä c', 'ent'),\n",
              " ('Ä 1', '8'),\n",
              " ('Ä all', 'ow'),\n",
              " ('Ä bel', 'ie'),\n",
              " ('ch', 'ool'),\n",
              " ('ot', 't'),\n",
              " ('Ä inc', 're'),\n",
              " ('Ä fe', 'el'),\n",
              " ('Ä res', 'ult'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(bpe_merges) # they do 50000 merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ToR3EG_gkDo",
        "outputId": "c73f07fa-10da-4257-d769-745b671860a6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder['Ä we'] # they merge the characters in this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64W6LI_DgkBN",
        "outputId": "4d289a2b-f265-4792-f1aa-abda538c1722"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "356"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# but where does 1 extra token come from ??\n",
        "# 256 (byte token) + 50,000 merges + 1 special token :)\n",
        "encoder[\"<|endoftext|>\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_MScipmgj-m",
        "outputId": "c79a23d0-8fbc-409d-c73a-a8dcb7751526"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4o has 2 special tokens and 199998 (256 + 199,742 merges)"
      ],
      "metadata": {
        "id": "8ckPoXm2gj7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentencepiece tokenizer\n",
        "\n",
        "(From Karpathy's notebook)\n",
        "\n",
        "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
        "\n",
        "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
        "\n",
        "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
        "\n",
        "TLDR:\n",
        "\n",
        "- tiktoken encodes to utf-8 and then BPEs bytes\n",
        "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
        "\n",
        "(Personally I think the tiktoken way is a lot cleaner...)"
      ],
      "metadata": {
        "id": "Y62SP0WIq8Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL7Cecruq8F7",
        "outputId": "d513b065-72de-43a8-abe2-e4f78f518183"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"toy.txt\", 'w') as file:\n",
        "  file.write(\"\"\"It is in her mind and within her calculated realm of possibilities .\n",
        "Clicking to one of these sites can trigger ads selling fake anti-spyware or turn the visitor 's PC into a hub for clicking on Web ads , while routing the ad payment to the intruder .\n",
        "The Housing Federation 's Belinda Porich said : \" Even by London standards , these are astronomical prices and many people - especially young , first time buyers - can only dream of owning a home .\n",
        "Wachovia Corp. , the bank purchased by Wells Fargo & Co . , will pay more than $ 4.5 million to settle a brokerage regulator 's claims that it failed to ensure that clients received discounts on investment trusts and mutual funds .\n",
        "\" This is the archive 's coming of age , in a way , because it 's now so accessible , \" said Robert Browning , director of the archives .\n",
        "\" It 's got much worse in the last five years .\n",
        "That eliminates the messy middle step where Carr gets sacked six times , then benched .\n",
        "Opposing the passage of the Senate bill --Obama 's bill-- from the left means dealing with accusations like these .\n",
        "For one thing , there are significant differences between the House and Senate proposals to address the plight of the jobless , differences that would have to be worked out before a bill could be sent to President Bush .\n",
        "There can 't be any more .\n",
        "Officer Rob Gibbs , a spokesman for the Albuquerque Police Department , said he was not aware of any consultations with the department regarding security for Boyd 's clinic .\n",
        "But the department had also said that it supported many of the benefits deriving from the agreement , including the potential to make millions of books more accessible .\n",
        "The millionaire activist , who will be a candidate for the Tories in the next General Election , told BBC1 's The Andrew Marr Show : \" We can achieve massive reductions from very little investment , and from the home-owners ' point of view that does pretty quickly lead to savings on your bills .\n",
        "What gives Democrats hope is that McDonnell 's numbers haven 't moved much over several months , while Deeds is weak in voter groups where he has room to grow : among young Democrats and African Americans .\n",
        "Their rooting section was celebrating early in the rout of Germany after Mexico took a 6-0 lead after two innings .\n",
        "27 , 1995 , at the massive Army base in eastern North Carolina when he opened fire with a rifle from a concealed position .\n",
        "It was a tough crowd Tuesday night at the senior center in Dartmouth , Mass .\n",
        "While investors may be demanding higher yields for their US bonds , there is slim evidence of an impending buyers ' strike , he notes .\n",
        "Use a glass or ceramic bowl , or even a stout plastic bag .\n",
        "This is the thing - all that party infighting , who said what when or why - THAT DOESN 'T MATTER EXCEPT TO THEM AND YOU GUYS .\n",
        "Mr. Long most recently served as Manager of Acquisitions and Evaluations for Phoenix Exploration Company .\n",
        "This implicit sadness sets Cyrano apart even more than usual from the soldiers of his Gascon regiment and the bons vivants of Paris .\n",
        "Black remains incensed at Radler for \" double-crossing \" his friends .\n",
        "The Progressive Liberal Party issued a statement apologizing for Forbes 's outburst , saying his announcement of an acquittal was \" incorrect . \"\n",
        "After eight months to 10 months of gradual dose increases , most can eat the peanut-flour equivalent of 15 peanuts daily , said Burks , who two years ago began reporting these signs of desensitization as long as children took their daily medicine .\n",
        "The Pleasanton-based company said Thursday that it earned $ 194.6 million , or 44 cents per share , during the three months ended Sept .\n",
        "parties - without jeopardizing network security. the address into your Internet browser. requirements and even cause a data breach .\n",
        "The Department of Food Science and Human Nutrition has a nationally recognized food stamp education program that helps food stamp recipients learn how to maximize their food stamp benefits by getting items with the best nutritional quality for their family .\n",
        "He said the dogs were later rounded up and were doing fine .\n",
        "If you threw Kings of Leon , Scissor Sisters and Arcade Fire into a blender , this -- with an obligatory shot of wheatgrass -- is exactly what you would get in your musical glass .\n",
        "Most medical experts agree that there is no evidence to suggest that children who take multivitamins encounter any negative consequences from ingesting high levels of nutrients .\n",
        "The incident was included in the post-match report of Alan Wiley , the referee , even though he took no action at the time .\n",
        "MILLETS-owner Blacks Leisure yesterday said it was on the recovery trail after like-for-like sales nudged up 3 per cent in the 26 weeks to the start of September .\n",
        "Trading houses rose on higher crude oil prices , with crude oil futures edging up in New York to $ 81.30 a barrel overnight .\n",
        "Last month the United Nations urged Russia and ex-Soviet Central Asia to stem drug trafficking from Afghanistan to Europe , saying the proceeds from a record opium crop were funding global terrorism . ( c ) Reuters 2007 .\n",
        "Henin collected $ 95,500 for the win while Kuznetsova settled for $ 50,500 .\n",
        "\" This is a solid slate of candidates with extensive banking and financial services experience , a deep understanding of international credit and equity markets , and first-hand knowledge of the governing regulatory system , \" said Richard Parsons , chairman of Citi 's board .\n",
        "The pair had been penalised five places on the grid for allegedly blocking rival drivers during qualifying , following complaints from Renault and the improving BMW Sauber team .\n",
        "Wateridge , of St Clement , Jersey , was the first person to be charged in connection with a multimillion-pound historic abuse investigation on the Channel Island .\n",
        "Burley insisted he now had the \" full support \" of the SFA , despite ambiguous comments from their president , George Peat , in recent weeks .\n",
        "The LCCC however , boasts the Old Trafford Lodge , with hotel rooms that have balconies looking out over the ground , which could be ideal for those looking to catch a game of cricket or the Green Day or Muse concerts that the ground will host later this year .\n",
        "I mean , that is the agreement when you are a NATO ally , is if another country is attacked , you 're going to be expected to be called upon and help .\n",
        "\" Increasing access to AEDs and CPR / AED training is an essential part of our mission and we hope the Senate will embrace this issue as well , \" said Scott Conner , senior vice president for the American Red Cross 's Preparedness , Health and Safety Services .\n",
        "Middle East , Asia ( excluding Japan ) and Central / South America countries. which the company is ranked as the sixth largest in the world market .\n",
        "On the wider RPI measure , which includes mortgage repayments and is often used in wage negotiations , inflation also rose sharply , to 3.7 % in January , from 2.4 % in December .\n",
        "Maybe some of you even remember sitting in one of those giant , shaking Afterburner arcade cabinets , firing off missiles , yelling \" THERE 'S A BOGEY ON MAH TAIL ! \" while the mums on the way to the candyfloss stand gave you a wide berth .\n",
        "It 's not a problem that can be solved by TV commercials featuring GM 's government-appointed auto-industry-newbie chairman , former AT & T chief executive Ed Whitacre , telling us how much he loves GM .\n",
        "The bottle was thrown into the ocean 24 years ago as part of a contest in Ocean city that promised a prize to the person whose note went the farthest .\n",
        "Describing his daily routine , he said he gets up just after 7am , except on weekends and holidays .\n",
        "Panic ensued at a Fort Worth , Texas , Bank of America call center Wednesday after a few workers complained of headaches , dizziness and shortness of breath .\n",
        "A small 2006 study from a University of Washington researcher found that young Indians living in Bangalore used cell phones to get to know partners introduced to them by their parents .\n",
        "COLUMBUS , Ohio - For the second day in a row , rock star Bruce Springsteen sang a few songs and urged thousands of potential voters in a battleground state to register and support Democrat Barack Obama .\n",
        "At events I regularly meet young Muslims and non-Muslims who have simply never heard arguments put for why liberal democracy is , though not perfect , our only achievable , messy , hope .\n",
        "\" I 'm hanging on by my fingernails . ... You can never get ahead , \" she said .\n",
        "It was a quiet finale otherwise for Flintoff , hero of the 2005 success , but England will find it hard to cover for such an inspirational figure .\n",
        "ThatÊ¼s bad news for the party that controls the White House and Congress at a time of near 10 percent unemployment and the slow economic recovery .\n",
        "\" All our attention is focused on making sure this thing never , never , ever becomes law , \" said John Boehner , Republican House leader .\n",
        "The downturn is helping Intel Capital 's portfolio .\n",
        "Police are waiting for further forensic evidence before deciding whether to charge them .\n",
        "He wanted all three of them to live together .\n",
        "Richard Leakey , who was given a government brief to clean up the crooked national wildlife department , bemoans the lack of progress .\n",
        "Mohamed Elibiary is a counter terrorism advisor who has advised President Barack Obama 's Homeland Security Council on home grown terror .\n",
        "It pointed out that neither the CoT study nor other investigations have ever found a causal link \" between the presence of cabin air contamination and the symptoms complained of by a very small minority of cabin and flight deck crew . \"\n",
        "Apart from his ornate Western military dress , pith helmet and monocle , he also dresses in Savile Row suits , wears a sword and is driven around the island in a London taxi .\n",
        "The new study is based on an online survey of parents with children 17 and younger .\n",
        "The left foot of Diamanti was also a constant threat for the hosts and Hart had to be alive midway through the half to palm over a stinging drive .\n",
        "Scotland 's airports were warning flights may be disrupted due to the bad weather .\n",
        "Geffen plans to meet with GE Chairman Jeffrey Immelt next week , with Jeff Zucker , NBC Universal 's president and chief executive , and Universal Studios President Ron Meyer also expected to attend , the newspaper reported .\n",
        "Jimmy Kimmel has never been on the show before , or if he has I don 't remember it .\n",
        "FOR years , when the artist Steven Parrino wasn 't jamming power chords on his electric guitar or tinkering with his motorcycle in his garagelike studio in Brooklyn , he was recycling his unsold paintings : twisting them into eccentric new shapes , smashing their stretcher bars or stabbing them repeatedly with scissors .\n",
        "Many of these risks and uncertainties are beyond Oncothyreon 's control .\n",
        "Beckham received a frosty reception at the match -- his first home game since returning from a five-month spell at Milan .\n",
        "Tivoli ( 00 45 3315 1001 , www.tivoli.dk ) is open from April 8 to September 20 and then for Hallowe 'en and Christmas .\n",
        "Clinton has indicated that she might take the fight to the convention in August .\n",
        "Her three pooches also wore matching Dolce & Gabbana floral lace collars .\n",
        "This article was first published on guardian.co.uk at 13.07 BST on Monday 20 April 2009 .\n",
        "Priestland 's conversion ensured the visitors were unable to even claim a bonus point , leaving them with an uphill task against Pool 6 leaders Leinster next weekend while the Scarlets travel to Brive .\n",
        "Russert was married to Maureen Orth , a writer for Vanity Fair magazine .\n",
        "It is true , however , that having worked so hard to keep Ronaldo last summer , United 's disinclination to offer him a new contract , given his current one expires in three years and does not even put the World Player of the Year among the top five-paid players in England , never mind the world , is unusual .\n",
        "America 's gold-medal chasing swimmers say teammate Eric Shanteau 's battle with cancer has inspired them and made them realise there is more to life than the Olympic Games .\n",
        "We will do it for Christina-Taylor Green , Dorothy Morris , Phyllis Schenk ( sic / Schneck ) , Dorwan Stoddard : ordinary citizens who died participating in their democracy .\n",
        "Dallas had a 13-point lead heading into the fourth quarter , but Sacramento closed to within 102-100 and had a chance to tie it as the final minute began .\n",
        "A : That he did it all .\n",
        "And now he was faced with the fact that his new executive compensation policy , which only applied to a narrow subset of executives at a few institutions , had been powerless to stop the worst violators at AIG from getting their undeserved payday .\n",
        "With its proposed spending cuts , revealed Thursday as the White House released a more detailed picture of its $ 3.6-trillion budget , the administration is aiming to cast itself as a careful custodian of tax dollars .\n",
        "British Foreign Secretary David Miliband has urged the Afghan government to exploit the military success to reconcile with moderate Taliban guerrillas .\n",
        "If this proves insufficient , technical measures will be introduced - including the powers to disconnect pirates .\n",
        "That solved his problem in Algeria .\n",
        "The DPJ opposes the refuelling , claiming it violates Japan 's pacifist constitution .\n",
        "Wright will go to prison and Riseborough will go to a young offenders ' institution .\n",
        "X Factor : Should we switch off ? 10The ultimate herbal remedy : Can cannabis improve autism ?\n",
        "A reconciliation of net income applicable to common shareholders to FFO is contained in the table accompanying this release .\n",
        "Because the plant is an attempt to develop new technology with potentially significant public benefits , the Energy Department is supposed to pay 74 percent of the costs , but because of growing costs , the government is now seeking to have the industry pick up a larger share of expenses , about $ 1.3 billion .\n",
        "Reporting from Altar , Mexico -- On a cloudless afternoon in northern Sonora , migrants and drug runners lounge in equal numbers under scattered mesquite trees , playing cards or sipping water .\n",
        "The unions representing Hollywood 's actors have reached tentative agreement with advertisers on a new contract covering work in commercials .\n",
        "Such a buildup within a country 's own banking system makes it all the more difficult for a government to propose a debt restructuring , given that the country 's local banks and its many unionized employees will suffer , not just faceless hedge funds abroad .\n",
        "But the Beetlejuice legend had some concerns about his character .\n",
        "Nowadays that 's no longer the case .\n",
        "Despite how the season has gone so far , on and off the court , Wizards players Antawn Jamison and Brendan Haywood both spoke Thursday about holding out hope of contending for a playoff spot .\n",
        "The New York Times obituary of Maryanne Amacher .\n",
        "\" We have to make sure we 're at the races and put South Africa under pressure again , \" he said .\n",
        "Xinhua said , according to Ma , China is fully prepared to discuss human rights with other countries as long as such talks take place in a state of mutual respect among all participants .\n",
        "The agency said the approval of Kari 's pay was done after consulting with the Treasury Department .\n",
        "Smith | 14.10.08 , 08 : 43 GMT - I 'm afraid that this may well be the case .\n",
        "An ecumenical service in memory of the victims of 32 different nationalities is to be held in Notre Dame Cathedral in Paris today .\n",
        "Croatia coach Slaven Bilic has emerged as a major contender for the vacant manager 's job at Schalke .\n",
        "Based not on a show or even a song or a game , \" Kit Kittredge : An American Girl \" is based on a doll .\n",
        "\" Everyone who purchased a card received a letter giving them a temporary code , and explaining that they could still get their discount by using the code and who to contact .\n",
        "However cliquey they might be , the highest achievers can still be friends with the worst bullies .\n",
        "\" You don 't realize there 's lead in it , you eat a cookie , you eat something without washing your hands , that exposure builds up in your body over time , \" said Dr. James Menoutis , who runs the lab at Quantex .\n",
        "Google 's Chrome browser will hog your computer 's processor more than the new Internet Explorer 8 .\n",
        "Shouldn 't the father be thrown in jail ?\n",
        "Hence the fierce backlash in Ms Hussein 's favour .\n",
        "Consumer advocacy blog The Consumerist phrased Facebook 's fresh policy as \" We Can Do Anything We Want With Your Content .\n",
        "But even with the discount , mountain rescue teams and the RNLI said the increase would be significant and put added pressures on funds .\n",
        "Crowds look closely and slowly .\n",
        "The reductions would be in addition to the 44,000 jobs already cut through widespread buyouts .\n",
        "House Democrats said they would not approve any money until President Obama has presented a detailed plan for how the shutdown would be handled .\n",
        "Formal agreements will be made with El Salvador , Australia , Romania and Estonia once a long-awaited security pact with the United States , which was approved by Parliament on Thursday , becomes law .\n",
        "But those wanting to go will also have had to pre-register their details , a system introduced in 2007 to stop ticket touting .\n",
        "The mass street demonstrations , polished campaign slogans and televised debates more closely resembled Western elections than the scripted campaigns in most other Middle Eastern countries .\n",
        "A senior US official said \" human rights , of course , will be discussed , \" citing Tibet and unrest in China .\n",
        "This is hardly a surprise to those of us who have worked the steroid beat over the years .\n",
        "\" It 's an emotional thing .\n",
        "Those fees , designed as a disincentive for firms to get too large and complex , would also serve as working capital for a government mechanism to wind down troubled systemic firms .\n",
        "According to the IOC Blogging Guidelines for the 2010 Games , athletes and other accredited people must keep their posts confined to their personal experiences .\n",
        "Persistently cold feet and hands are very common , and even healthy non-smokers can experience the symptoms .\n",
        "Here in the US it seems that only now are we beginning to take it seriously .\n",
        "\" The rioters , armed with weapons from the U.S. , Israel and England , opened fire on people in a futile attempt to accuse the police and the Basiji , with the cooperation of foreign media , \" Firouzabadi said in an open letter addressed to Imam Mahdi , a venerated Shiite Muslim who disappeared hundreds of years ago and whose messianic return , it is believed , will herald a new age .\n",
        "The median household income for a family of four in New Jersey is $ 94,441 , according to the U.S. Department of Health and Human Services .\n",
        "NEW YORK ( AP ) - Ron Gardenhire threw a couple of lefties at the New York Yankees and nothing went right .\n",
        "In September 2006 , Mr Johnson went one better than irritating the people of a city by managing to annoy an entire country .\n",
        "LONDON - Reviled by the public and spurned in private , bankers have been looking for solace in adultery , according to a dating Web site for people seeking affairs .\n",
        "McChrystal and General David Petraeus , head of the U.S. Central Command , were participating in the talks .\n",
        "The Pennsylvania Senator has endured two bouts of Hodgkin 's lymphoma and the chemotherapy that goes with it , a couple of procedures for a recurrent benign brain tumor , and heart-bypass surgery that sent him into cardiac arrest .\n",
        "Homegrown signs like Johnny Cueto ( Dominican ) and Joey Votto ( Canada ) should be regulars in the Queen City for some time , and they soon could be joined by the likes of Cuban Yonder Alonso , Puerto Rican Neftali Soto , Venezuelan Yorman Rodriguez , and Dominicans , Juan Francisco and Juan Duran .\n",
        "When he maintained that Vidal 's writing was \" no more interesting than the contents of the stomach of an intellectual cow , \" they booed heartily .\n",
        "Three policemen and three assailants were killed in a gunfight in front of the consulate .\n",
        "( AP ) Police say a large explosion has killed at least three people and heavily damaged a building housing a federal investigative agency in Pakistan 's east .\n",
        "He beat them 6-3 in his season debut , then got a no-decision on May 12 .\n",
        "A This all goes back to your lease and how well you read through it !\n",
        "In some cases , shirt and skirt were conjoined , no longer separates , which looked like a speedy solution for the time-poor getting up in the morning .\n",
        "Reformist Web sites said he had been held at Kahrizak prison , where much of the alleged prisoner abuse took place , and his jaw was broken when his father received his body .\n",
        "3 ( UPI ) -- Authorities in Georgia said a man dressed as an elf was arrested for allegedly telling a mall Santa that he was carrying explosives .\n",
        "Swedish midfielder Ljungberg had already given Arsenal the lead in this FA Cup quarterfinal but , with Bolton steaming forward for an equalizer , he then astonishingly scooped the ball over the crossbar from two yards out with the goal gaping .\n",
        "\" Democrats have realized if there is any time they can steal this seat , now is the time to do it , \" Hoffman said .\n",
        "Ferrell took notes as the messaging gurus outlined their options : focus on opinion-makers inside the Beltway with high-frequency ads in Capitol Hill papers .\n",
        "The deal should become a firm order shortly .\n",
        "England will carefully watch the early game today -- between Ireland and Sri Lanka -- and consider whether to play an additional pace bowler , Ryan Sidebottom .\n",
        "COLUMBUS , Ohio ( AP ) - Penn State , unbeaten and unbowed , proved it belongs in the middle of any national championship talk .\n",
        "The field , once developed , will have 70 wells -- half production and half injection , two-thirds of which will be drilled from an artificially constructed island 2.8 miles from the coast .\n",
        "That is the first development from the release of the Bowl Championship Series standings yesterday , as Ohio State holds the top spot and South Florida is No. 2 thanks to strong numbers in the computer polls .\n",
        "Especially given that spaffing is very much frowned upon in the classroom .\n",
        "That said , the outlook remains broadly encouraging .\n",
        "LOUIS - Wicked thunderstorms with wind reaching speeds of 120 mph pushed through parts of the Midwest on Friday , leaving four people dead , collapsing a church and knocking out power to thousands , authorities said .\n",
        "A long-troubled $ 14 billion program to build a landing craft for the Marine Corps is destined for the chopping block , defense officials and analysts said Wednesday , part of $ 100 billion in savings that Defense Secretary Robert M. Gates has pledged to squeeze from the Pentagon 's budget .\n",
        "Has become disappointing after showing early promise , but he would hardly be the first to turn over a new leaf after entering the care of Tim Vaughan .\n",
        "Marina rarely leaves her two-room home in northern Israel these days .\n",
        "Lopez acknowledged that votes for Cruz were nullified , but claims they added up to only 8 ballots of about 100 cast in this largely unpaved village of about 1,500 people .\n",
        "The map remained in the Wolfegg collection for the next hundred years - until 2003 , when the US Library of Congress announced , with great fanfare , that it had acquired the map from the castle 's owner for the staggering sum of $ 10m .\n",
        "He appeared in four games before being optioned back to Triple-A Louisville .\n",
        "We support action that would lead to more places being made available in February .\n",
        "17 / PRNewswire-USNewswire / -- Quitting is hard and staying quit is even harder .\n",
        "Also present were some of Africa 's most controversial leaders .\n",
        "Why did you ever leave ?\n",
        "But that doesn 't mean they won 't have anything to say .\n",
        "He took part in a charity ' guess the weight ' contest , where organisers had to use specialist gear from a stone landscaping company usually used to weigh lorries to calculate his immense mass .\n",
        "On Thursday , employees at Quinn Insurance said a meeting with administrators in Enniskillen earlier was \" positive . \"\n",
        "The service is being upgraded to give more homes channel Five , but 460,000 households are expected to lose access to ITV3 and ITV4 instead .\n",
        "For outdoorsy dads , think beyond the standard barbecue accessories .\n",
        "The strong sequential increase in NI instrument control revenue indicates that the overall test and measurement industry may have bottomed out in Q2 and begun a sequential recovery in Q3 . Product revenue was $ 152 million , down 24 percent from Q3 2008 , and software maintenance revenue was $ 13 million , down 9 percent year-over-year .\n",
        "He said NASA 's funding plans would put NASA back on track as a \" big-picture innovator \" in technology development that could create future growth .\n",
        "Jane Seymour also recounted for WebMD her struggles conceiving twin boys at age 44 .\n",
        "Investors sold off shares , unimpressed by the economic stimulus plan President Bush announced Friday .\n",
        "There is also a certain innocence and sweetness about Moretz 's performance .\n",
        "Morgan Stanley said it had $ 198.2 billion in capital as of Feb .\n",
        "The dead man was wearing a wetsuit when his remains were found on Tuesday afternoon at Camasunary on Loch Slapin .\n",
        "The FTSE 100 index had fallen nearly 17 per cent since September 30 by Wednesday 's close , and the market opened a further 5 per cent lower on Thursday .\n",
        "Then they found and beat another white man so bad that over a month later , he is still in the hospital .\n",
        "In a March 2006 attack in Mahmoudiya , about 20 miles south of Baghdad , Green and three other soldiers went to the home of 14-year-old Abeer Qassim al-Janabi .\n",
        "If others don 't step in quickly , Washington will need to twist their arms or do even more itself .\n",
        "\" We had the old brass bell from the previous lifeboat on board , so we upended it to use as the christening font .\n",
        "Yet , the more effective the present clean-up seems , the more likely is it that central bankers will draw that lesson .\n",
        "Playing the outsourcing card ( \" It 's not our fault -- it 's this company we work with \" ) ?\n",
        "\" The substance of life doesn 't change much from one culture to another , \" she observes , \" but the human soul requires a beautiful wrapper . \"\n",
        "Alaibi and her family , meanwhile , paid a heavy price for helping Americans .\n",
        "The debate tells the story : \" Now that Hoffman has emerged as the GOP 's best bet for holding the Republican seat , the Democratic candidate , Bill Owens , used a Thursday debate to tie Hoffman to the Club for Growth , an anti-tax group which has backed Hoffman , and ignored the Republican candidate , Dede Scozzafava , \" ABC 's Teddy Davis writes .\n",
        "CAVALIERS ' WALLACE DOUBTFUL Cavaliers forward Ben Wallace is doubtful for Game 3 of Cleveland 's playoff series against the Boston Celtics because of allergies and a left inner ear infection .\n",
        "Instead , NASA would be asked to monitor climate change and develop a new rocket .\n",
        "Speaking in oratorical cadences that evoke Sean Connery by way of John Huston , Mr. Day-Lewis 's oil baron , Daniel Plainview , dripping with black slime in the early scenes , is a fearsome creature whose soul rots as his income multiplies .\n",
        "So far , AT & T , Verizon Communications and Sprint Nextel , which were embarrassed in 2005 after it was discovered they were cooperating with the National Security Agency in a warrantless surveillance scheme by the Bush administration , have not signed on .\n",
        "Lincoln Cycling Grand Prix paid Â£ 10,000 for its police costs .\n",
        "Emphasizing that it doesn 't need American firepower , a Pakistani general said an offensive along the frontier has killed more than 1,000 militants and predicted the region would be \" stabilized \" within two months .\n",
        "14 ( UPI ) -- Barack Obama license plates are now available to Illinois residents who can 't get enough of the new U.S. president , state officials say .\n",
        "Here in Vero Beach , at least there was , there is , sometimes , Koufax .\n",
        "Reject portions of writings you don 't like and compliment others ; if you can. joint efforts .\n",
        "Although Andrew Flintoff was periodically a handful , Hilfenhaus has been the best speedster in the contest .\n",
        "On Saturday , the man - who was rolled into court for a remand hearing on a stretcher with his arm in bandages and leg in a splint - denied all charges against him , although he admitted to having been in the area .\n",
        "It has also offered a $ 100,000 grant for animal habitat improvements at the Alaska Zoo if the relocation deal goes through .\n",
        "And I think it would eventually drive the Taliban out of these areas , much the way it 's been done in some of the urban areas in Iraq through the inkblot strategy .\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "4YXg2wL9gj5C"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "import sentencepiece as spm\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=True,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True, # so that we do not get <unk> for unknown tokens\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=False, # this puts a space in front every \"sentence\" so that 'world' and ' world' are same\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)\n"
      ],
      "metadata": {
        "id": "fSgD3kPegj2f"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6SU5XMI4coH9",
        "outputId": "dbf61e4b-c57b-400a-dc86-ee6f8485b905"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['â–t', 259],\n",
              " ['â–a', 260],\n",
              " ['in', 261],\n",
              " ['he', 262],\n",
              " ['er', 263],\n",
              " ['â–the', 264],\n",
              " ['re', 265],\n",
              " ['on', 266],\n",
              " ['â–,', 267],\n",
              " ['â–s', 268],\n",
              " ['en', 269],\n",
              " ['â–w', 270],\n",
              " ['an', 271],\n",
              " ['â–c', 272],\n",
              " ['â–.', 273],\n",
              " ['â–o', 274],\n",
              " ['ed', 275],\n",
              " ['at', 276],\n",
              " ['â–b', 277],\n",
              " ['ing', 278],\n",
              " ['is', 279],\n",
              " ['â–f', 280],\n",
              " ['it', 281],\n",
              " ['or', 282],\n",
              " ['es', 283],\n",
              " ['ou', 284],\n",
              " ['â–p', 285],\n",
              " ['ar', 286],\n",
              " ['â–in', 287],\n",
              " ['â–an', 288],\n",
              " ['â–h', 289],\n",
              " ['â–m', 290],\n",
              " ['al', 291],\n",
              " ['as', 292],\n",
              " ['â–to', 293],\n",
              " ['â–of', 294],\n",
              " ['ic', 295],\n",
              " ['il', 296],\n",
              " ['â–d', 297],\n",
              " ['â–and', 298],\n",
              " ['ro', 299],\n",
              " ['ent', 300],\n",
              " ['â–th', 301],\n",
              " ['le', 302],\n",
              " ['id', 303],\n",
              " ['ion', 304],\n",
              " [\"â–'\", 305],\n",
              " ['â–e', 306],\n",
              " ['â–n', 307],\n",
              " ['â–g', 308],\n",
              " ['â–be', 309],\n",
              " ['â–re', 310],\n",
              " ['st', 311],\n",
              " ['om', 312],\n",
              " ['ay', 313],\n",
              " ['ut', 314],\n",
              " ['ad', 315],\n",
              " ['ve', 316],\n",
              " ['â–S', 317],\n",
              " ['se', 318],\n",
              " ['â–is', 319],\n",
              " ['â–', 320],\n",
              " ['e', 321],\n",
              " ['t', 322],\n",
              " ['a', 323],\n",
              " ['n', 324],\n",
              " ['i', 325],\n",
              " ['o', 326],\n",
              " ['s', 327],\n",
              " ['r', 328],\n",
              " ['h', 329],\n",
              " ['l', 330],\n",
              " ['d', 331],\n",
              " ['c', 332],\n",
              " ['u', 333],\n",
              " ['m', 334],\n",
              " ['g', 335],\n",
              " ['f', 336],\n",
              " ['p', 337],\n",
              " ['w', 338],\n",
              " ['y', 339],\n",
              " ['b', 340],\n",
              " [',', 341],\n",
              " ['.', 342],\n",
              " ['v', 343],\n",
              " ['k', 344],\n",
              " ['T', 345],\n",
              " ['-', 346],\n",
              " ['S', 347],\n",
              " ['A', 348],\n",
              " [\"'\", 349],\n",
              " ['0', 350],\n",
              " ['C', 351],\n",
              " ['\"', 352],\n",
              " ['B', 353],\n",
              " ['I', 354],\n",
              " ['M', 355],\n",
              " ['P', 356],\n",
              " ['x', 357],\n",
              " ['D', 358],\n",
              " ['H', 359],\n",
              " ['W', 360],\n",
              " ['1', 361],\n",
              " ['E', 362],\n",
              " ['O', 363],\n",
              " ['R', 364],\n",
              " ['F', 365],\n",
              " ['L', 366],\n",
              " ['G', 367],\n",
              " ['N', 368],\n",
              " ['z', 369],\n",
              " ['2', 370],\n",
              " ['U', 371],\n",
              " ['q', 372],\n",
              " ['4', 373],\n",
              " ['3', 374],\n",
              " ['J', 375],\n",
              " ['j', 376],\n",
              " ['$', 377],\n",
              " ['Y', 378],\n",
              " ['5', 379],\n",
              " ['(', 380],\n",
              " [')', 381],\n",
              " [':', 382],\n",
              " ['7', 383],\n",
              " ['V', 384],\n",
              " ['6', 385],\n",
              " ['8', 386],\n",
              " ['K', 387],\n",
              " ['9', 388],\n",
              " ['Q', 389],\n",
              " ['/', 390],\n",
              " ['?', 391],\n",
              " ['&', 392],\n",
              " ['X', 393],\n",
              " ['!', 394],\n",
              " ['%', 395],\n",
              " ['Z', 396],\n",
              " [';', 397],\n",
              " ['|', 398],\n",
              " ['Â£', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello ì•ˆë…•í•˜ì„¸ìš”\")\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WsyQn5CsLAG",
        "outputId": "88a6d290-ae94-42a6-c8ed-d053922ee49d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[262, 330, 330, 326, 320, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbyBWCIssXHd",
        "outputId": "e419d4e6-7942-40f4-e1e3-fc8449a37ff8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'l', 'l', 'o', 'â–', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sRqj2hwsXEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFtoJrW1sXBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FfwG7LIsW_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA50lbUosW8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7AAjkO1sW6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAiAy66usW3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xcRs61jsW0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDCoCNf8sWL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}