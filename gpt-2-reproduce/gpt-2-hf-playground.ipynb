{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— Transformers Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some text\n",
    "\n",
    "Link: [GPT-2 Model Card](https://huggingface.co/openai-community/gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, a language model\", says a voice with a very low level of awareness. \"For example, with JavaScript and'},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, it's pretty obvious that we need more complexity to model languages. But it's interesting\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and when you say something like:\\n\\n[T]hem, the value of a variable type should be\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a computer programmer. I'll say this, there's nothing I could to hide.\\n\\nThen the\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I'm not really prepared for you to ask me questions here. I have a lot to learn. I\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dictionary Keys:\n",
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the smallest GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Access the state dictionary\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Inspect the state dictionary (e.g., print keys or specific weights)\n",
    "print(\"State Dictionary Keys:\")\n",
    "for key, val in state_dict.items():\n",
    "    print(key, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets create the GPT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layers: int = 12 # number of layers\n",
    "    n_heads: int = 12 # number of heads\n",
    "    emb_dim: int = 768 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.emb_dim % config.n_heads == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.emb_dim, 3 * config.emb_dim)\n",
    "        self.c_proj = nn.Linear(config.emb_dim, config.emb_dim)\n",
    "        self.register_buffer(\"bias\",\\\n",
    "                              torch.tril(torch.ones(config.block_size, config.block_size)).\\\n",
    "                                view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_heads = config.n_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.c_attn(x) # (B, T, 3*C)\n",
    "        q, k, v = torch.split(qkv, C, dim=2)\n",
    "        q = q.view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, C//self.n_heads).transpose(1, 2) # (B, nH, T, H)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) * ((k.size(-1)) ** (-0.50)) # (B, nH, T, T)\n",
    "        attn_mask = attn.masked_fill(self.bias[:, :, :T, :T]==0, float(\"-inf\"))\n",
    "        attn_scores = torch.softmax(attn_mask, dim=-1)\n",
    "\n",
    "        out = attn_scores @ v # (B, nH, T, H)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        out = self.c_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.emb_dim, 4 * config.emb_dim)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.emb_dim, config.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, C)\n",
    "        x = self.c_fc(x) # (B, T, 4 * C)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x) # (B, T, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.emb_dim)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.emb_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) # residual (B,T,C)\n",
    "        x = x + self.mlp(self.ln_2(x)) # residual (B,T,C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.emb_dim), #[50257, 768]\n",
    "            wpe = nn.Embedding(config.block_size, config.emb_dim), #[1024, 768]\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.emb_dim)\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x shape (B, T), targets shape (B, T)\n",
    "        B, T =  x.shape\n",
    "        token_emb = self.transformer.wte(x) # (B, T, C)\n",
    "        pos_emb = self.transformer.wpe(torch.arange(0, T,\\\n",
    "                                                     device=device)) # (T, C)\n",
    "        x = token_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        for layer in self.transformer.h:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, V)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1)).item()\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layers=12, n_heads=12, emb_dim=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layers=24, n_heads=16, emb_dim=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layers=36, n_heads=20, emb_dim=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layers=48, n_heads=25, emb_dim=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=1024, vocab_size=50257, n_layers=12, n_heads=12, emb_dim=768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(model_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets sample some text again from tis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, text ,max_length, num_return_sequences, k=50):\n",
    "    # first tokenize the text\n",
    "    tokens = tokenizer.encode(text)\n",
    "    tokens = torch.tensor(tokens, device=device).long()\n",
    "    print(tokens)\n",
    "    print(\"-\"*50)\n",
    "    # next make it (B, T)\n",
    "    tokens = tokens.unsqueeze(0).expand(num_return_sequences, -1)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        x = tokens[:, -config.block_size:] # (B, T)\n",
    "        print(x)\n",
    "        logits, _ = model(x) # (B, T, V)\n",
    "\n",
    "        logits = logits[:, -1, :] # (B, V)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(probs, k, dim=-1)\n",
    "\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
    "        sampled_indices = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "        tokens = torch.cat((tokens, sampled_indices), dim=1)\n",
    "\n",
    "    output_text = tokenizer.decode_batch(tokens.cpu().detach().numpy())\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if device == \"mps\":\n",
    "    torch.mps.manual_seed(42)\n",
    "elif device == \"cuda\":\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "       device='mps:0')\n",
      "--------------------------------------------------\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780]],\n",
      "       device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760,   703],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262,  1223],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11,   287],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257, 20280],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892,   286]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760,   703,   284],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262,  1223,  2073],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11,   287,   428],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257, 20280,   396],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892,   286,  1243]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760,   703,   284,  1234],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262,  1223,  2073,   319],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11,   287,   428,  1339],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257, 20280,   396,    13],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892,   286,  1243, 10338]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760,   703,   284,  1234,   428],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262,  1223,  2073,   319,   340],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11,   287,   428,  1339,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257, 20280,   396,    13,   314],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892,   286,  1243, 10338,   329]], device='mps:0')\n",
      "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724,\n",
      "           314,  1101,  5385,   351,   340,    11,   475,   314,  1101,   407,\n",
      "         43472,   287,   326,    13,  3894,    11,   351,   326,   531,    11,\n",
      "           314,   760,   703,   284,  1234,   428,  1978],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   262,\n",
      "         15582,    11,   284,   787,   779,   286,   340,    11,   318,  2495,\n",
      "           922,    13,  1406,  1521,   466,   345,   423,   326,   290,   407,\n",
      "          1949,  1262,  1223,  2073,   319,   340,   357],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   314,  1101,\n",
      "          1804,   428,   670,   287, 11361,    11,   290,   788,   314,  1101,\n",
      "          3597,  2438,   329, 25271,    13,   198,   198,  2396,   356,   460,\n",
      "           766,    11,   287,   428,  1339,    11,   616],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   345,\n",
      "           821,  1642, 14895,   546,   616,   779,   286,   606,    13,   314,\n",
      "          1101,   407,   257,  3288,  3303, 22454,  1008,    13,   314,  1101,\n",
      "           407,   257, 20280,   396,    13,   314,  1101],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11,   880,    11,\n",
      "           314,  1101,   422,  7349,   290,   423,   284,  3551,   257,  8300,\n",
      "          3303,   329,   340,    13,   314,   423,   616,   898, 25818,   780,\n",
      "           314,   892,   286,  1243, 10338,   329,   262]], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello, I'm a language model, which means I'm familiar with it, but I'm not fluent in that. Well, with that said, I know how to put this together.\",\n",
       " \"Hello, I'm a language model, and the syntax, to make use of it, is pretty good. So why do you have that and not try using something else on it (from\",\n",
       " \"Hello, I'm a language model, I'm doing this work in Python, and then I'm writing code for Haskell.\\n\\nSo we can see, in this case, my previous\",\n",
       " \"Hello, I'm a language model, and you're making assumptions about my use of them. I'm not a natural language learner. I'm not a linguist. I'm interested\",\n",
       " \"Hello, I'm a language model, well, I'm from Java and have to write a programming language for it. I have my own vocabulary because I think of things differently for the first\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, \"Hello, I'm a language model,\" ,max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and my project will get better with time, but I think there are a lot more things that can help you\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, so if I don't have a problem, I can fix it by creating new words\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to learn some stuff. I'll try to do some basic programming and just learn better ways\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't believe in grammar. This will work for every language model. You can define it very quickly\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, a model of how things should be, and then we look at different things as well.\" I\\'d like to'}]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "         [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "         [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
       "         [0.4294, 0.8854, 0.5739, 0.2666, 0.6274]],\n",
       "\n",
       "        [[0.2696, 0.4414, 0.2969, 0.8317, 0.1053],\n",
       "         [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],\n",
       "         [0.9516, 0.0753, 0.8860, 0.5832, 0.3376],\n",
       "         [0.8090, 0.5779, 0.9040, 0.5547, 0.3423]],\n",
       "\n",
       "        [[0.6343, 0.3644, 0.7104, 0.9464, 0.7890],\n",
       "         [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],\n",
       "         [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],\n",
       "         [0.7071, 0.6581, 0.4913, 0.8913, 0.1447]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3,4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x[:, -1, :]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn((5, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50]), torch.Size([5, 50]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_probs, topk_indices = torch.topk(probs, dim=-1, k=50) # (B, 50)\n",
    "topk_probs.shape, topk_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1580, 0.0257, 0.0231, 0.0206, 0.0193, 0.0165, 0.0159, 0.0153, 0.0148,\n",
       "        0.0141, 0.0139, 0.0126, 0.0123, 0.0120, 0.0119, 0.0118, 0.0117, 0.0112,\n",
       "        0.0108, 0.0101, 0.0100, 0.0098, 0.0090, 0.0087, 0.0082, 0.0081, 0.0080,\n",
       "        0.0077, 0.0073, 0.0073, 0.0073, 0.0070, 0.0066, 0.0066, 0.0062, 0.0061,\n",
       "        0.0060, 0.0059, 0.0059, 0.0056, 0.0055, 0.0055, 0.0054, 0.0054, 0.0053,\n",
       "        0.0053, 0.0053, 0.0051, 0.0050, 0.0050])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6419)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_idx = torch.multinomial(topk_probs, num_samples=1, replacement=True) # (B,)\n",
    "sampled_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26],\n",
       "        [19],\n",
       "        [12],\n",
       "        [ 1],\n",
       "        [37]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58,   6, 141, 161,  68, 132,  40,  19, 165,  86,  95, 159,  80, 106,\n",
       "          87,   4,  27,   2, 109, 110,  59, 188,   8,  76, 157, 144,  25,  69,\n",
       "          93, 134,  85,  79,   5, 190, 123,  38,  63,  45,  28, 163, 105,  74,\n",
       "         162,  99, 192, 184, 153,  31,  49, 194],\n",
       "        [ 57, 184, 131,  35, 110,  36,  21, 100,  65,  64,  91, 111, 101,  82,\n",
       "          85, 196, 112, 192, 175, 138, 162, 154,  98, 161,  59, 137,  20, 151,\n",
       "         173,  87,  48,  62, 114, 150,  23,  28, 178, 148,  38, 106, 130,  25,\n",
       "          76,  66,  42, 135,   8,  46, 176,  24],\n",
       "        [107, 101, 149,  92, 182, 100, 191,  95, 157,  20, 158, 192, 175,  98,\n",
       "           2, 113, 194,  68, 129, 104,   1, 105,  78,  41,  36, 110,   0,  94,\n",
       "          87,   4,  49,  89, 160,  40,  96,  48, 152, 130,  50, 115,  21,  35,\n",
       "          47, 174,  66,  72,  70, 134, 185, 126],\n",
       "        [ 31,  25,  75,  12,  63,  87, 167, 102,  69, 113, 162,  40,   0, 175,\n",
       "          46, 190,  91, 149,  38, 176, 153,  64, 130, 164, 127, 194, 128,  47,\n",
       "          79,  98, 179,  16,  10, 163,  78,  60, 140, 184, 168, 121, 191,  62,\n",
       "          90, 100, 188,  59, 123, 101,  19, 137],\n",
       "        [ 62,  72, 142,  34,  52,  64, 126,  75, 131, 186,  20,  18,  59, 185,\n",
       "          31,  15,  14, 172,  17, 182, 118,  60, 187,  32, 165,  38, 115, 109,\n",
       "          73,  54,  91,  76, 116, 169,  97,  90,  10,   3, 121,  27,  28,  79,\n",
       "          74, 152, 125,  16, 167, 156, 107, 171]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25],\n",
       "        [138],\n",
       "        [175],\n",
       "        [ 25],\n",
       "        [  3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = torch.gather(topk_indices, -1, sampled_idx) # (B, 1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "         [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "         [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
       "         [0.4294, 0.8854, 0.5739, 0.2666, 0.6274]],\n",
       "\n",
       "        [[0.2696, 0.4414, 0.2969, 0.8317, 0.1053],\n",
       "         [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],\n",
       "         [0.9516, 0.0753, 0.8860, 0.5832, 0.3376],\n",
       "         [0.8090, 0.5779, 0.9040, 0.5547, 0.3423]],\n",
       "\n",
       "        [[0.6343, 0.3644, 0.7104, 0.9464, 0.7890],\n",
       "         [0.2814, 0.7886, 0.5895, 0.7539, 0.1952],\n",
       "         [0.0050, 0.3068, 0.1165, 0.9103, 0.6440],\n",
       "         [0.7071, 0.6581, 0.4913, 0.8913, 0.1447]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3,4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
