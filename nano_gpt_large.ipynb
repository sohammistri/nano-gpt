{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFW2Uz2eAkY0"
      },
      "source": [
        "# Train NanoGPT on Karpathy's hyperparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i7u_A2wBFC6"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ehJGYQURAcnq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1337)\n",
        "from IPython.display import display, Markdown\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zX63aQMAcp8",
        "outputId": "ee51baf8-b5c8-41de-a813-54dc1e708e27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"input.txt\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_55Ph-qAAcsa",
        "outputId": "bd9c0e23-87ab-441e-c3ef-84a1428f61a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = sorted(list(set(data)))\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-wY47U-Acuo",
        "outputId": "9d3c01bf-344d-41a1-81d2-31b7c7b64bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OmJWzmUVAcw-"
      },
      "outputs": [],
      "source": [
        "stoi = {c:i for i, c in enumerate(chars)}\n",
        "itos = {i:c for i, c in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWpZ8vrRAczU",
        "outputId": "d13eca82-ce51-4b4d-b42f-107dff5117ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = encode(data)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9O_KjB3Ac1q",
        "outputId": "ed0b63b7-cab7-4487-847a-16a41d438121"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(892315, 223079)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_size = len(tokens)\n",
        "split_idx = int(0.8 * data_size)\n",
        "train_tokens = tokens[:split_idx]\n",
        "val_tokens = tokens[split_idx:]\n",
        "len(train_tokens), len(val_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oZ4Uc-NTAc3_"
      },
      "outputs": [],
      "source": [
        "def get_batch(tokens, block_size, batch_size):\n",
        "    batch = torch.randint(0, len(tokens)-block_size, (batch_size,)) # B dimension array of random indices\n",
        "    Xb = torch.stack([torch.LongTensor(tokens[i:i+block_size]) for i in batch], dim=0) # Create (B, T) dimension array\n",
        "    yb = torch.stack([torch.LongTensor(tokens[i+1:i+block_size+1]) for i in batch], dim=0) # Create (B, T) dimension array\n",
        "    return Xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7B6lzX7xAc6f"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_loss(tokens, block_size, batch_size, model, device):\n",
        "    loss_values = []\n",
        "    for _ in range(100):\n",
        "        Xb, yb = get_batch(tokens, block_size, batch_size)\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        _, loss = model(Xb, yb)\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "    mean_loss = torch.FloatTensor(loss_values).mean().item()\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-T_jNRPdAc89"
      },
      "outputs": [],
      "source": [
        "def train(train_tokens, val_tokens, model, optimizer, device, block_size, batch_size, n_iters, eval_interval):\n",
        "    train_lossi, val_lossi = [], []\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        model.train()\n",
        "        Xb, yb = get_batch(train_tokens, block_size, batch_size)\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        # forward\n",
        "        _, loss = model(Xb, yb)\n",
        "\n",
        "        # set grads to zero\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # do backward\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i % eval_interval == 0) or (i == n_iters - 1):\n",
        "            model.eval()\n",
        "            train_loss = compute_loss(train_tokens, block_size, batch_size, model, device)\n",
        "            val_loss = compute_loss(val_tokens, block_size, batch_size, model, device)\n",
        "\n",
        "            train_lossi.append(train_loss)\n",
        "            val_lossi.append(val_loss)\n",
        "\n",
        "            print(f\"Step {i}/{n_iters} --> Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "\n",
        "        # break\n",
        "\n",
        "    return train_lossi, val_lossi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFmpS5exCKhb"
      },
      "source": [
        "## Implement NanoGPT\n",
        "\n",
        "![nanogpt-architecture](nanogpt-architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5qrCKA3LAc_S"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionDecoder(nn.Module):\n",
        "    def __init__(self, channel_dim, head_dim, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.k_project = nn.Linear(channel_dim, head_dim, bias=False)\n",
        "        self.q_project = nn.Linear(channel_dim, head_dim, bias=False)\n",
        "        self.v_project = nn.Linear(channel_dim, head_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        mask = torch.triu(torch.ones(block_size, block_size), diagonal=1).bool()\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, T, C)\n",
        "        # create keys queries and values\n",
        "        k, q, v = self.k_project(x), self.q_project(x), self.v_project(x) # all shaped (B, T, H)\n",
        "\n",
        "        # get the weights\n",
        "        B, T, H = k.shape\n",
        "        wei = (q @ k.transpose(-2, -1)) * (H**-0.5) # B,T,H @ B,H,T --> B,T,T\n",
        "        wei = wei.masked_fill(self.mask[:T, :T], float('-inf')) # B,T,T\n",
        "        wei = torch.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        # get the out\n",
        "        out = wei @ v # (B,T,T) @ (B,T,H) -> (B,T,H)\n",
        "        return out\n",
        "\n",
        "class MultiHeadSelfAttentionDecoderBlock(nn.Module):\n",
        "    def __init__(self, channel_dim, num_heads, head_dim, block_size, dropout):\n",
        "        super().__init__()\n",
        "        # MHA\n",
        "        self.ln1 = nn.LayerNorm(channel_dim)\n",
        "        self.mha_decoder = nn.ModuleList([SelfAttentionDecoder(channel_dim, head_dim, block_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads*head_dim, channel_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # FFN\n",
        "        self.ln2 = nn.LayerNorm(channel_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(channel_dim, 4 * channel_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * channel_dim, channel_dim),\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.proj(torch.cat([head(self.ln1(x)) for head in self.mha_decoder],\\\n",
        "                           dim=-1)) # head (x) -> (B,T,H) * NH -> [B,T,H*NH]\n",
        "        x = self.dropout1(x)\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class NanoGPT(nn.Module):\n",
        "    def __init__(self, emb_dim, vocab_size, block_size, num_heads, n_layers, dropout, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.block_size = block_size\n",
        "        # get the embeddings matrix\n",
        "        self.tok_embs = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_embs = nn.Embedding(block_size, emb_dim)\n",
        "\n",
        "        self.mha_block = nn.Sequential(\n",
        "            *[MultiHeadSelfAttentionDecoderBlock(emb_dim, num_heads, emb_dim//num_heads, block_size, dropout) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # Final linear layer\n",
        "        self.ln = nn.LayerNorm(emb_dim)\n",
        "        self.lm_layer = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        print(f\"No. of parameters: {sum([p.numel() for p in self.parameters()])}\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # x shape (B, T), targets shape (B, T)\n",
        "        B, T = x.shape\n",
        "        token_embs = self.tok_embs(x) #(B,T,C)\n",
        "        position_embs = self.pos_embs(torch.arange(T).to(self.device)) #(T,C)\n",
        "        embs = token_embs + position_embs\n",
        "        embs = self.mha_block(embs)\n",
        "        logits = self.lm_layer(self.ln(embs))\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, V = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "JRCwD5yxAdBp"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "n_iters = 5000\n",
        "eval_interval = n_iters//10\n",
        "lr = 3e-4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "emb_dim = 384\n",
        "num_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "vocab_size = len(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMiSLj3dAdD-",
        "outputId": "d29dd72c-96d2-4d43-8e55-11a610e776b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of parameters: 10788929\n"
          ]
        }
      ],
      "source": [
        "model = NanoGPT(emb_dim=emb_dim, vocab_size=vocab_size, block_size=block_size, num_heads=num_heads,\\\n",
        "                 n_layers=n_layers, dropout=dropout, device=device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYcPzDfLAbe7",
        "outputId": "71915ce2-9204-4ea5-c03c-818591a3ace7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 788, 929)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "10,788,929"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "IAnjNbHANRxg"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o464uqiqNRvD",
        "outputId": "0acfc775-2dda-4032-bd9e-a73e46ba3666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0/5000 --> Train: 3.8190 | Val: 3.8336\n",
            "Step 500/5000 --> Train: 2.0245 | Val: 2.1289\n",
            "Step 1000/5000 --> Train: 1.6913 | Val: 1.9363\n",
            "Step 1500/5000 --> Train: 1.5734 | Val: 1.8870\n",
            "Step 2000/5000 --> Train: 1.5145 | Val: 1.8665\n"
          ]
        }
      ],
      "source": [
        "train_lossi, val_lossi = train(train_tokens=train_tokens, val_tokens=val_tokens, model=model, optimizer=optimizer,\\\n",
        "      device=device, block_size=block_size, batch_size=batch_size, n_iters=n_iters, eval_interval=eval_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McXz82U4kVE8"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, max_tokens=1000, random_seed=1337):\n",
        "  model.eval()\n",
        "\n",
        "  torch.manual_seed(random_seed)\n",
        "\n",
        "  curr_window = torch.LongTensor([[0]]).to(device)\n",
        "\n",
        "  full_window = []\n",
        "\n",
        "  for _ in range(max_tokens):\n",
        "    # truncate current window to block size\n",
        "    curr_window = curr_window[:, -block_size:]\n",
        "\n",
        "    # forward pass and get logits\n",
        "    logits, _ = model(curr_window) # (B,T,V)\n",
        "\n",
        "    # get probs and sample\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    B,T,C = probs.shape\n",
        "    next_idx = torch.multinomial(probs.view(B*T, C), num_samples=1, replacement=True)[-1].item() # (B*T,)\n",
        "\n",
        "    # collect the resulst\n",
        "    full_window.append(next_idx)\n",
        "    curr_window = torch.cat([curr_window,\\\n",
        "                             torch.LongTensor([[next_idx]]).to(device)], dim=-1)\n",
        "    \n",
        "  generated_str = decode(full_window)\n",
        "  return generated_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(generate(model)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_str = generate(model, max_tokens=10000, random_seed=42)\n",
        "with open(\"out.txt\", \"w\") as f:\n",
        "  f.write(generated_str)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
